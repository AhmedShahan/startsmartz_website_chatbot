[
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 0,
    "content": "What is an AI agent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is an AI agent?\nIntroducing a new series of musings on AI agents, called \"In the Loop\".\n\nHarrison Chase\n4 min read\nJun 28, 2024"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 1,
    "content": "‚ÄúWhat is an agent?‚ÄùI get asked this question almost daily. At LangChain, we build tools to help developers build LLM applications, especially those that act as a reasoning engines and interact with external sources of data and computation. This includes systems that are commonly referred to as ‚Äúagents‚Äù.Everyone seems to have a slightly different definition of what an AI agent is. My definition is perhaps more technical than most: üí°An AI agent is a system that uses an LLM to decide the control flow of an application.Even here, I‚Äôll admit that my definition is not perfect. People often think of agents as advanced, autonomous, and human-like ‚Äî¬†but what about a simple system where an LLM routes between two different paths? This fits my technical definition, but not the common perception of"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 2,
    "content": "‚Äî¬†but what about a simple system where an LLM routes between two different paths? This fits my technical definition, but not the common perception of what an agent should be capable of. It‚Äôs hard to define exactly what an agent is!That‚Äôs why I really liked Andrew Ng‚Äôs tweet last week. In it he suggests that ‚Äúrather than arguing over which work to include or exclude as being a true AI agent, we can acknowledge that there are different degrees to which systems can be agentic.‚Äù Just like autonomous vehicles, for example, have levels of autonomy, we can also view AI agent capabilities as a spectrum. I really agree with this viewpoint and I think Andrew expressed it nicely. In the future, when I get asked about what an agent is, I will instead turn the conversation to discuss what it means to"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 3,
    "content": "Andrew expressed it nicely. In the future, when I get asked about what an agent is, I will instead turn the conversation to discuss what it means to be ‚Äúagentic‚Äù.What does it mean to be agentic?I gave a TED talk last year about LLM systems and used the slide below to talk about the different levels of autonomy present in LLM applications.A system is more ‚Äúagentic‚Äù the more an LLM decides how the system can behave.Using an LLM to route inputs into a particular downstream workflow has some small amount of ‚Äúagentic‚Äù behavior. This would fall into the Router category in the above diagram.If you do use multiple LLMs to do multiple routing steps? This would be somewhere between Router and State Machine.If one of those steps is then determining whether to continue or finish - effectively"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 4,
    "content": "steps? This would be somewhere between Router and State Machine.If one of those steps is then determining whether to continue or finish - effectively allowing the system to run in a loop until finished? That would fall into State Machine.If the system is building tools, remembering those, and then taking those in future steps? That is similar to what the Voyager paper implemented, and is incredibly agentic, falling into the higher Autonomous Agent category.These definitions of ‚Äúagentic‚Äù are still pretty technical. I prefer the more technical definition of ‚Äúagentic‚Äù because I think it‚Äôs useful when designing and describing LLM systems.Why is ‚Äúagentic‚Äù a helpful concept?As with all concepts, it‚Äôs worth asking why we even need the concept of ‚Äúagentic‚Äù. What does it help with?Having an idea"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 5,
    "content": "‚Äúagentic‚Äù a helpful concept?As with all concepts, it‚Äôs worth asking why we even need the concept of ‚Äúagentic‚Äù. What does it help with?Having an idea of how agentic your system can guide your decision-making during the development process - including building it, running it, interacting with it, evaluating it, and even monitoring it.The more agentic your system is, the more an orchestration framework will help. If you are designing a complex agentic system, having a framework with the right abstractions for thinking about these concepts can enable faster development. This framework should have first-class support for branching logic and cycles.The more agentic your system is, the harder it is to run. It will be more and more complex, having some tasks that will take a long time to"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 6,
    "content": "cycles.The more agentic your system is, the harder it is to run. It will be more and more complex, having some tasks that will take a long time to complete. This means you will want to run jobs as background runs. This also means you want durable execution to handle any errors that occur halfway through.The more agentic your system is, the more you will want to interact with it while it‚Äôs running. You‚Äôll want the ability to observe what is going on inside, since the exact steps taken may not be known ahead of time. You‚Äôll want the ability to modify the state or instructions of the agent at a particular point in time, to nudge it back on track if it‚Äôs deviating from the intended path.The more agentic your system is, the more you will want an evaluation framework built for these types of"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 7,
    "content": "if it‚Äôs deviating from the intended path.The more agentic your system is, the more you will want an evaluation framework built for these types of applications. You‚Äôll want to run evals multiple times, since there is compounding amount of randomness. You‚Äôll want the ability to test not only the final output but also the intermediate steps to test how efficient the agent is behaving.The more agentic your system is, the more you will want a new type of monitoring framework. You‚Äôll want the ability to drill down into all the steps an agent takes. You‚Äôll also want the ability to query for runs based on steps an agent takes.Understanding and leveraging the spectrum of agentic capabilities in your system can improve the efficiency and robustness of your development process.Agentic is newOne"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 8,
    "content": "the spectrum of agentic capabilities in your system can improve the efficiency and robustness of your development process.Agentic is newOne thing that I often think about is what is actually new in all this craze. Do we need new tooling and new infrastructure for the LLM applications people are building? Or will generic tools and infrastructure from pre-LLM days suffice?To me, the more agentic your application is, the more critical it is to have new tooling and infrastructure. That‚Äôs exactly what motivated us to build LangGraph, the agent orchestrator to help with building, running, and interacting with agents, and LangSmith, the testing and observability platform for LLM apps. As we move further on the agentic spectrum, the entire ecosystem of supportive tooling needs to be reimagined."
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 9,
    "content": "Tags\nHarrison ChaseIn the Loop\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nNot Another Workflow Builder\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Agents\n\n\nIn the Loop\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nThe rise of \"context engineering\"\n\n\nIn the Loop\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow and when to build multi-agent systems\n\n\nIn the Loop\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Metric That Determines AI Product Success\n\n\nIn the Loop\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to think about agent frameworks\n\n\nIn the Loop\n20 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up"
  },
  {
    "url": "https://blog.langchain.dev/what-is-an-agent/",
    "chunk_id": 10,
    "content": "In the Loop\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to think about agent frameworks\n\n\nIn the Loop\n20 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://twitter.com/LangChainAI",
    "chunk_id": 0,
    "content": "JavaScript is not available.\nWe‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.\nHelp Center\n\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      ¬© 2025 X Corp.\n    \nSomething went wrong, but don‚Äôt fret ‚Äî let‚Äôs give it another shot.Try again Some privacy related extensions may cause issues on x.com. Please disable them and try again."
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 0,
    "content": "How Dun & Bradstreet‚Äôs ChatD&B‚Ñ¢ uses LangChain and LangSmith to deliver trusted, data-driven AI insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Dun & Bradstreet‚Äôs ChatD&B‚Ñ¢ uses LangChain and LangSmith to deliver trusted, data-driven AI insights\nLearn how Dun & Bradstreet, a leading financial data analytics company, empowers global clients with business data ‚Äî¬†from credit risk to ownership structures ‚Äî to make better decisions using their AI assistant, Chat D&B. See how LangChain and LangSmith have helped Dun & Bradstreet on their journey.\n\nCase Studies\n4 min read\nNov 18, 2024"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 1,
    "content": "Dun & Bradstreet (D&B), a global leader in business decisioning data and analytics, empowers companies to solve critical problems by providing trusted, curated data and analytics that drive informed decisions and improved outcomes. Serving over 240,000 clients in 200+ countries, Dun & Bradstreet‚Äôs Data Cloud fuels solutions that boost revenue, reduce costs, manage risk, and transform business.¬†Traditional AI and ML have long been integral to Dun & Bradstreet‚Äôs offerings, enabling clients to rapidly extract actionable insights from its global data cloud consisting of 580+ million business entities. More recently, LLMs have enabled Dun & Bradstreet to transform customer interactions with the company‚Äôs trusted insights.¬†Unlike traditional chatbots, ChatD&B‚Äî built using LangChain ‚Äîprovides"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 2,
    "content": "to transform customer interactions with the company‚Äôs trusted insights.¬†Unlike traditional chatbots, ChatD&B‚Äî built using LangChain ‚Äîprovides users with real-time access to a vast array of diverse data sources, including structured and unstructured formats. The LangChain-enabled autonomous AI agents‚Äô framework is a game changer in delivering value to customers:Customer use case complexity: Autonomous agents enable complex conversational scenarios that were previously unmanageable, for example, comparing several business entities across numerous dimensions at once and providing a holistic summary.Customer use case diversity: ChatD&B supports workflows across a wide range of domains, including credit risk, sales & marketing, supply chain management, KYC, and more‚Äîfar beyond the capabilities"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 3,
    "content": "across a wide range of domains, including credit risk, sales & marketing, supply chain management, KYC, and more‚Äîfar beyond the capabilities of single-purpose chatbots.Trusted information: The LangChain framework integrates LLMs with traditional tools like APIs, leveraging each for its strengths and thus reducing hallucination risks.Explainability: ‚ÄúShow your work‚Äù capabilities provide clear data lineage, allowing customers to see which data informed each answer, a crucial element of responsible AI.Personalized experience: Dynamic data entitlements ensure interactions are tailored to individual customer needs.Customers have reported significant time savings and ability to perform key activities that had previously not been possible.¬†Building smarter conversations¬†The challenge for ChatD&B"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 4,
    "content": "time savings and ability to perform key activities that had previously not been possible.¬†Building smarter conversations¬†The challenge for ChatD&B wasn‚Äôt just to answer questions‚Äî it needed to give answers backed by real, accurate and trusted data, with full transparency and explainability. Dun & Bradstreet understands that its customers need to trust not just the answers, but also to see the reasoning behind each response. The implications of an erroneous response could present costly risks to a decision or action taken as a result.¬†Gary Kotvets, Chief Data and Analytics Officer at Dun & Bradstreet notes that:‚ÄúOur ‚Äòshow your work‚Äô framework make data sources and lineage explainable in ChatD&B so our users have the confidence in the quality and validity of the information presented. Our"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 5,
    "content": "make data sources and lineage explainable in ChatD&B so our users have the confidence in the quality and validity of the information presented. Our goal with ChatD&B is to provide users with the ability to harness our highly valued commercial data on both public and private companies at scale anywhere, anytime right at their fingertips.‚ÄùThis is where LangChain came into the picture. LangChain allowed Dun & Bradstreet to turn its raw data into a conversational, intelligent AI assistant by accessing various data blocks dynamically and explaining results in real-time. For example, when a customer asks, ‚ÄúIs [x company] in Texas a high credit risk?‚Äù, ChatD&B doesn‚Äôt just return a credit score, it provides much more comprehensive information. It pulls from multiple data points‚Äî lawsuits, liens,"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 6,
    "content": "ChatD&B doesn‚Äôt just return a credit score, it provides much more comprehensive information. It pulls from multiple data points‚Äî lawsuits, liens, corporate structure‚Äî and provides the reasoning behind the score, explaining what factors are influencing it. This \"show your work\" capability offers explainability and is one of the most appreciated features.LangChain‚Äôs modular structure also allowed the company to inject important contextual information, making ChatD&B more than just a simple Q&A AI assistant. Each tool, from risk assessments to ownership structures, had its own context and understanding. The system could explain, for instance, why a credit score was considered ‚Äúgood‚Äù or ‚Äúbad,‚Äù based on real data and predefined scales. Now, customers no longer have to interpret complex"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 7,
    "content": "why a credit score was considered ‚Äúgood‚Äù or ‚Äúbad,‚Äù based on real data and predefined scales. Now, customers no longer have to interpret complex datasets by themselves‚Äî ChatD&B does the heavy lifting, helping to explain the insights clearly and concisely.Raising the bar with ChatD&B¬†As ChatD&B expanded in complexity and adoption, the team at Dun & Bradstreet wanted to ensure the AI assistant consistently delivered accurate, grounded results. By integrating LangSmith, Dun & Bradstreet‚Äôs data science team could analyze how the AI was performing, step-by-step. LangSmith allows the team to see and monitor every decision made, compare new queries with historical data, and assess the overall quality of the responses.For instance, when a user asks for a company‚Äôs failure risk, LangSmith allows"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 8,
    "content": "with historical data, and assess the overall quality of the responses.For instance, when a user asks for a company‚Äôs failure risk, LangSmith allows the Dun & Bradstreet team to trace every action the agent took, providing visibility into the reasoning. This observability is crucial, especially for customers using ChatD&B for high-stakes decisions.LangSmith‚Äôs testing features also empowers Dun & Bradstreet to run \"what if\" scenarios. The team can compare how the system responds to new queries against similar questions asked before, ensuring consistent, high-quality outputs. By building extensive ground-truth data and using LangSmith‚Äôs LLM-judge features, the company can continuously improve ChatD&B, refining it with every early adopter user interaction.The Impact¬†ChatD&B is enabling a"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 9,
    "content": "features, the company can continuously improve ChatD&B, refining it with every early adopter user interaction.The Impact¬†ChatD&B is enabling a broader range of customer users to access Dun & Bradstreet‚Äôs actionable insights through a streamlined, conversational ‚Äúone-stop shop‚Äù interface. Early adopters are already seeing significant time savings and greater satisfaction, as they no longer need to manually sift through multiple products, datasets, or documents. Users are also discovering new capabilities, both immediate‚Äîsuch as quickly retrieving key insights on a supplier before a meeting‚Äîand more strategic, like identifying new growth opportunities in innovative ways.The Road AheadDun & Bradstreet is dedicated to enhancing ChatD&B‚Äôs capabilities, continuously exploring new use cases to"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 10,
    "content": "in innovative ways.The Road AheadDun & Bradstreet is dedicated to enhancing ChatD&B‚Äôs capabilities, continuously exploring new use cases to make it the ultimate conversational assistant for business data. The team will continue to roll out new functionalities to empower confident, data-driven decision-making.The Dun & Bradstreet team is also excited about its ongoing collaboration with LangChain and the powerful advancements LangChain continues to introduce. For instance, LangGraph enhances the system‚Äôs ability to understand complex relationships within data, enabling deeper insights and more precise responses across diverse business contexts."
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 11,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-dun-bradstreet/",
    "chunk_id": 12,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 0,
    "content": "LangSmith - Observability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangSmith ObservabilityKnow what your agents are really doingLangSmith Observability gives you complete visibility into agent behavior with tracing, real-time monitoring, alerting, and high-level insights into usage.Sign UpGet a demoHelping top teams ship reliable agentsFind failures fast with agent tracingQuickly debug and understand non-deterministic LLM app behavior with tracing. See what your agent is doing step by step ‚Äîthen fix issues to improve latency and response quality.Get started tracing your app"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 2,
    "content": "Monitor what matters ‚Ä®to the businessTrack business-critical metrics like costs, latency, and response quality with live dashboards. Get alerts when issues happen and drill into the root cause.See how to create a custom dashboard\n\n\nDiscover usage patterns and issues automaticallySee clusters of similar conversations to understand what users actually want and quickly find all instances of similar problems to address systemic issues.Learn more about Insights"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 3,
    "content": "Ready to get visibility into your agents?LangSmith works with any framework. If you‚Äôre already using LangChain or LangGraph, just set one environment variable to get started with tracing your AI application.Sign up for freeBook a demoResources for LangSmith ObservabilitywebinarGet started with LangSmith TracingDOCSLangSmith Observability conceptsblogLangSmith OTel supportFAQs for LangSmith ObservabilityWhat frameworks and libraries does LangSmith work with?\n\nLangSmith works with any framework. You can use it with or without our open source frameworks LangChain and LangGraph. Learn more.Does LangSmith support OTel?"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 4,
    "content": "Yes, LangSmith supports with OTel to unify your observability stack across services. Your application does not need to be written in Python or Typescript. See the docs.Can I use LangSmith Observability without LangSmith Evaluation?\n\nYes. You can use LangSmith Observability with or without Evaluation. For all plan types, you'll get access to both and only pay for what you use.I can‚Äôt have data leave my environment. Can I self-host LangSmith?\n\nYes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.Where is my data stored?"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 5,
    "content": "When using LangSmith hosted at smith.langchain.com, data is stored in GCP us-central-1. If you‚Äôre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment. For more information, check out our documentation.Will LangSmith add latency to my application?\n\nNo, LangSmith does not add any latency to your application. In the LangSmith SDK, there‚Äôs a callback handler that sends traces to a LangSmith trace collector which runs as an async, distributed process. Additionally, if LangSmith experiences an incident, your application performance will not be disrupted.Will you train on the data that I send LangSmith?"
  },
  {
    "url": "https://www.langchain.com/langsmith/observability",
    "chunk_id": 6,
    "content": "We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information.How much does LangSmith cost?\n\nSee our pricing page for more information, and find a plan that works for you.ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#conclusion",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 0,
    "content": "Plans and Pricing - LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upPlans for teams of¬†any¬†sizeGet all LangSmith services -- pay for what you useDeveloperFor solo developers getting started.$0 / seat per monththen pay as you goFirst 5k traces included, starting at $0.50 per 1k base ¬†traces thereafter."
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 1,
    "content": "Start for freeGet started with:\n\nUp to 5k base traces / mo, then pay-as--you-go\n\nTracing to debug agent execution\n\nOnline and offline evals\n\nPrompt Hub, Playground, and Canvas for auto improving prompts\n\nAnnotation queues for human feedback\n\nMonitoring and alerting\n\nCommunity support\n\n1 seatPlusFor teams building and deploying agents.$39 / seat per monththen pay as you goFirst 10k traces included, starting at $0.50 per 1k base ¬†traces thereafter.\n\n\n\nSign upEverything in the Developer plan, and:\n\nUp to 10k base traces / mo, then pay-as-you-go\n\n1 dev-sized ¬†agent deployment included\n\nEmail support\n\nUp to 10 seats\n\nUp to 3 workspacesEnterpriseFor teams with advanced hosting, security, and support needs.¬†CustomContact salesEverything in the Plus plan, and:"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 2,
    "content": "Up to 3 workspacesEnterpriseFor teams with advanced hosting, security, and support needs.¬†CustomContact salesEverything in the Plus plan, and:\n\nAlternative hosting options, including hybrid and self-hosted so data doesn‚Äôt leave your VPC\n\nCustom SSO¬†and RBAC\n\nAcccess to deployed engineering team\n\nSupport SLA\n\nTeam trainings & architectural guidance\n\nCustom seats and workspacesLangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups\n\n\nEducation\n\n\nEnquire about startup pricing\n\n\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\n\n\nEducation"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 3,
    "content": "Education\n\n\nCompare plansFeaturesDeveloperPlusEnterprisePricingUsersMaximum of 1 seat(free)Up to 10 seats$39¬†per¬†seat/monthCustomTrace volume (Observability¬†&¬†Evaluation)Prices for traces vary depending on the data retention period you've set. \n\n\n\n 5k base traces / mo included Pay as you go thereafter ‚ìò 10k base traces / mo included Pay as you go thereafter ‚ìòCustomMax trace ingestion and trace size Prices for traces vary depending on the data retention period you've set."
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 4,
    "content": "See hourly trace ingestion and trace event limits here.See hourly trace ingestion and trace event limits here.CustomNode execution cost (Deployment)N/A1 free Dev deployment with unlimited node executions included.For additional deployments:¬†$0.001/node executionCustomUptime cost (Deployment)N/A$0.0007 / min per Development deployment$0.0036 / min per Production deploymentCustomLangSmith Observability &¬†Evaluation:Debug, monitor, and improve your AI¬†applications with tracing and evalsTracingMonitoringInsights (beta)\n\nOnline and offline evalsDataset collectionAnnotation queue (human feedback)Prompt Hub and PlaygroundBulk data export"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 5,
    "content": "Online and offline evalsDataset collectionAnnotation queue (human feedback)Prompt Hub and PlaygroundBulk data export\n\nLangSmith Deployment:Deploy and manage long-running agents with our purpose-built infrastructureLangSmith StudioExpose agent as MCP serverReal-time streaming of intermediary steps and final outputAgent Authorization (beta)1-Click Deploy\n\nHorizontally-scalable service for production-sized deployments\n\nAssistants API\n\n30+¬†API¬†endpoints including state and memory\n\nCron scheduling\n\nAuthentication &¬†authorization for LangGraph APIs"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 6,
    "content": "Assistants API\n\n30+¬†API¬†endpoints including state and memory\n\nCron scheduling\n\nAuthentication &¬†authorization for LangGraph APIs\n\nHosting options:Choose where to host the LangSmith platformPlatform hosting option(s)CloudCloudCloud, Hybrid, or Self-HostedInfraN/AFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane¬†Self-Hosted: Fully self-managedData locationN/ALangChain's Cloud (US or EU)Cloud:¬†LangChain's Cloud (US¬†or EU)Hybrid:¬†LangChain's Cloud (US or EU)Self-Hosted:¬†Your VPCSecurity ControlsSSO\n\nGoogle, GitHubCustom SSORole-Based Access Control\n\n\n\nOrganization Roles (User and Admin)\n\nSupportCommunity SlackEmail Support\n\nTeam trainings\n\n\n\n Architectural guidance for your applications\n\n\n\nAccess to deployed engineers\n\n\n\nSLA"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 7,
    "content": "SupportCommunity SlackEmail Support\n\nTeam trainings\n\n\n\n Architectural guidance for your applications\n\n\n\nAccess to deployed engineers\n\n\n\nSLA\n\n\n\nProcurementBillingMonthly, self-serveMonthly, self-serveAnnual invoiceCustom Terms\n\n\n\nInfosec Review\n\n\n\nFAQsGeneral QuestionsWhich plan is right for me?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 8,
    "content": "Infosec Review\n\n\n\nFAQsGeneral QuestionsWhich plan is right for me?\n\nOur Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).¬†¬†‚ÄçThe Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangSmith Deployment service, with 1 free dev-sized deployment included.¬†‚ÄçThe Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.Do you offer a plan for startups?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 9,
    "content": "Yes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. You‚Äôll get discounted rates and generous free trace allotments to build with confidence from day one.‚Äç‚ÄçApply here to get started with startup pricing. Customers can stay on the Startup Plan for 1 year before graduating to the Plus Plan.When will I be billed?\n\nFor the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.Will you train on the data that I send to LangSmith?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 10,
    "content": "We will not train on your data, and you own all rights to your data. See our Terms of Service for more information.LangSmith Observability &¬†Evaluation QuestionsWhat is a trace? Can it contain multiple events?\n\nA trace represents a single execution of your application‚Äîwhether it‚Äôs an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here's an example of a single trace.What is the difference between a base trace and an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 11,
    "content": "Base traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can \"upgrade\" base traces to extended traces at $4.50 per 1k traces.Why might I want to upgrade a base trace to an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 12,
    "content": "Base traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They‚Äôre priced for volume and short-term utility.‚ÄçExtended traces, on the other hand, are retained for 400 days and often include valuable feedback‚Äîwhether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.‚ÄçLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 13,
    "content": "Base traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They‚Äôre priced for volume and short-term utility.‚ÄçExtended traces, on the other hand, are retained for 400 days and often include valuable feedback‚Äîwhether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.‚ÄçLangSmith lets you choose the right retention for each trace, helping you balance cost and value. I‚Äôve exhausted my free trace allocation. What can I do?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 14,
    "content": "If you‚Äôve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If you‚Äôve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangSmith Deployment QuestionsDoes LangSmith Deployment include any free deployments?\n\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, you‚Äôll be charged by usage (nodes executed and uptime).For my free Dev agent deployment for LangSmith Deployment, is there a cap on the number of nodes executed?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 15,
    "content": "If you‚Äôre on the Plus plan, you get 1 free dev-sized agentdeployment ‚Äì all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed?\n\nNodes Executed is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.What does uptime mean for LangSmith Deployment usage?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 16,
    "content": "Uptime is the duration your deployment‚Äôs database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev agent deployments are typically short-lived (used during iteration, then deleted) ‚Äì whereas Production agent deployments stay live and are updated via revisions (rather than being deleted).¬†When should I use a dev-sized deployment vs. a production-sized agent deployment?"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 17,
    "content": "We recommend using the production-sized deployment for any customer-facing agent. Dev-sized agent deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.Ready to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems"
  },
  {
    "url": "https://www.langchain.com/pricing-langsmith",
    "chunk_id": 18,
    "content": "up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#introduction",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 0,
    "content": "Overview - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationAccount administrationOverviewGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APILangGraph Server APIControl Plane API for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQOn this pageResource"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 1,
    "content": "for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQOn this pageResource HierarchyOrganizationsWorkspacesResource tagsUser Management and RBACUsersAPI keysExpiration DatesPersonal Access Tokens (PATs)Service keysOrganization rolesWorkspace roles (RBAC)Best PracticesEnvironment SeparationUsage and BillingData RetentionWhy retention mattersHow it worksBilling modelRate LimitsScenariosHandling 429s responses in your applicationUsage LimitsProperties of usage limitingSide effects of extended data retention traces limitUpdating usage limitsRelated contentAdditional ResourcesAccount administrationOverviewCopy pageCopy pageThis overview covers topics related to managing users, organizations, and workspaces within"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 2,
    "content": "ResourcesAccount administrationOverviewCopy pageCopy pageThis overview covers topics related to managing users, organizations, and workspaces within LangSmith."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 3,
    "content": "‚ÄãResource Hierarchy\n‚ÄãOrganizations\nAn organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide.\nWhen you log in for the first time, a personal organization will be created for you automatically. If you‚Äôd like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:\nFeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing pageCollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available\n‚ÄãWorkspaces"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 4,
    "content": "‚ÄãWorkspaces\nWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition.\nA workspace is a logical grouping of users and resources within an organization. A workspace separates trust boundaries for resources and access control. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide.\nIt is recommended to create a separate workspace for each team within your organization. To organize resources even further, you can use Resource Tags to group resources within a workspace.\nThe following image shows a sample workspace settings page:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 5,
    "content": "The following image shows a sample workspace settings page: \nThe following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: \nSee the table below for details on which features are available in which scope (organization or workspace):"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 6,
    "content": "See the table below for details on which features are available in which scope (organization or workspace):\nResource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & ExperimentsWorkspacePromptsWorkspaceResource TagsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 7,
    "content": "* Data retention settings and usage limits will be available soon for the organization level as well ** Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag. See the self-hosted user management docs for details.\n‚ÄãResource tags\nResource tags allow you to organize resources within a workspaces. Each tag is a key-value pair that can be assigned to a resource. Tags can be used to filter workspace-scoped resources in the UI and API: Projects, Datasets, Annotation Queues, Deployments, and Experiments.\nEach new workspace comes with two default tag keys: Application and Environment; as the names suggest, these tags can be used to categorize resources based on the application and environment they belong to. More tags can be added as needed."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 8,
    "content": "LangSmith resource tags are very similar to tags in cloud services like AWS."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 9,
    "content": "‚ÄãUser Management and RBAC\n‚ÄãUsers\nA user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations.\nOrganization members are managed in organization settings:\n\nAnd workspace members are managed in workspace settings:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 10,
    "content": "‚ÄãAPI keys\nWe ended support for legacy API keys prefixed with ls__ on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with ls__ will no longer work as of October 22, 2024.\n‚ÄãExpiration Dates\nWhen you create an API key, you have the option to set an expiration date. Adding an expiration date keys enhances security and minimize the risk of unauthorized access. For example, you may set expiration dates on keys for temporary tasks that require elevated access.\nBy default, keys never expire. Once expired, an API key is no longer valid and cannot be reactivated or have its expiration modified.\n‚ÄãPersonal Access Tokens (PATs)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 11,
    "content": "‚ÄãPersonal Access Tokens (PATs)\nPersonal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. We recommend not using these to authenticate requests from your application, but rather using them for personal scripts or tools that interact with the LangSmith API. If the user associated with the PAT is removed from the organization, the PAT will no longer work.\nPATs are prefixed with lsv2_pt_\n‚ÄãService keys"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 12,
    "content": "PATs are prefixed with lsv2_pt_\n‚ÄãService keys\nService keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Only admins can create service keys. We recommend using these for applications / services that need to interact with the LangSmith API, such as LangGraph agents or other integrations. Service keys may be scoped to a single workspace, multiple workspaces, or the entire organization, and can be used to authenticate requests to the LangSmith API for whichever workspace(s) it has access to.\nService keys are prefixed with lsv2_sk_\nUse the X-Tenant-Id header to specify the target workspace.\nWhen using PATs: If this header is omitted, requests will run against the default workspace associated with the key."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 13,
    "content": "When using PATs: If this header is omitted, requests will run against the default workspace associated with the key.\nWhen using organization-scoped service keys: You must include the X-Tenant-Id header when accessing workspace-scoped resources. Without it, the request will fail with a 403 Forbidden error."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 14,
    "content": "To see how to create a service key or Personal Access Token, see the setup guide\n‚ÄãOrganization roles\nOrganization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information.\nThe organization role selected also impacts workspace membership as described here:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 15,
    "content": "Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organization\nOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 16,
    "content": "The Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins. Custom organization-scoped roles are not available yet.\nSee the table below for all organization permissions:\nOrganization UserOrganization AdminView organization configuration‚úÖ‚úÖView organization roles‚úÖ‚úÖView organization members‚úÖ‚úÖView data retention settings‚úÖ‚úÖView usage limits‚úÖ‚úÖAdmin access to all workspaces‚úÖManage billing settings‚úÖCreate workspaces‚úÖCreate, edit, and delete organization roles‚úÖInvite new users to organization‚úÖDelete user invites‚úÖRemove users from an organization‚úÖUpdate data retention settings*‚úÖUpdate usage limits*‚úÖ\n‚ÄãWorkspace roles (RBAC)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 17,
    "content": "‚ÄãWorkspace roles (RBAC)\nRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.\nRoles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 18,
    "content": "Admin - has full access to all resources within the workspace\nViewer - has read-only access to all resources within the workspace\nEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys)\n\nOrganization admins can also create/edit custom roles with specific permissions for different resources.\nRoles can be managed in organization settings under the Roles tab:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 19,
    "content": "For more details on assigning and creating roles, see the access control setup guide.\n‚ÄãBest Practices\n‚ÄãEnvironment Separation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 20,
    "content": "‚ÄãBest Practices\n‚ÄãEnvironment Separation\nUse resource tags to organize resources by environment using the default tag key Environment and different values for the environment (e.g. dev, staging, prod). This tagging structure will allow you to organize your tracing projects today and easily enforce permissions when we release attribute based access control (ABAC). ABAC on the resource tag will provide a fine-grained way to restrict access to production tracing projects, for example. We do not recommend that you use Workspaces for environment separation as you cannot share resources across Workspaces. If you would like to promote a prompt from staging to prod, we recommend you use commit tags instead. See docs for more information.\n‚ÄãUsage and Billing\n‚ÄãData Retention"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 21,
    "content": "‚ÄãUsage and Billing\n‚ÄãData Retention\nIn May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced a new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange for savings up to 10x. On this page, we‚Äôll go through how data retention works and is priced in LangSmith.\n‚ÄãWhy retention matters"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 22,
    "content": "Privacy: Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once it‚Äôs no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations.\nCost: LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend for details."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 23,
    "content": "‚ÄãHow it works\nLangSmith now has two tiers of traces based on Data Retention with the following characteristics:\nBaseExtendedPrice$.50 / 1k traces$5 / 1k tracesRetention Period14 days400 days\nData deletion after retention ends\nAfter the specified retention period, traces are no longer accessible via the runs table or API. All user data associated with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata associated with each trace may be retained indefinitely for analytics and billing purposes.\nData retention auto-upgrades\nAuto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 24,
    "content": "Auto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs.\nWhen you use certain features with base tier traces, their data retention will be automatically upgraded to extended tier. This will increase both the retention period, and the cost of the trace.\nThe complete list of scenarios in which a trace will upgrade when:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 25,
    "content": "Feedback is added to any run on the trace\nAn Annotation Queue receives any run from the trace\nA Run Rule matches any run within a trace\n\nWhy auto-upgrade traces?\nWe have two reasons behind the auto-upgrade model for tracing:\n\nWe think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer.\nWe philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 26,
    "content": "If you have questions or concerns about our pricing model, please feel free to reach out to support@langchain.dev and let us know your thoughts!\nHow does data retention affect downstream features?"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 27,
    "content": "Annotation Queues, Run Rules, and Feedback: Traces that use these features will be auto-upgraded.\nMonitoring: The monitoring tab will continue to work even after a base tier trace‚Äôs data retention period ends. It is powered by trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on base tier traces.\nDatasets: Datasets have an indefinite data retention period. Restated differently, if you add a trace‚Äôs inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature.\n\n‚ÄãBilling model\nBillable metrics\nOn your LangSmith invoice, you will see two metrics that we charge for:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 28,
    "content": "‚ÄãBilling model\nBillable metrics\nOn your LangSmith invoice, you will see two metrics that we charge for:\n\nLangSmith Traces (Base Charge)\nLangSmith Traces (Extended Data Retention Upgrades)."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 29,
    "content": "The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces.\nWhy measure all traces + upgrades instead of base and extended traces?\nA natural question to ask when considering our pricing is why not just show the number of base tier and extended tier traces directly on the invoice?\nWhile we understand this would be more straightforward, it doesn‚Äôt fit trace upgrades properly. Consider a base tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 30,
    "content": "If your trace was recorded as an extended retention trace, then the base and extended metrics will both be recorded with the same timestamp.\nCost breakdown\nThe Base Charge for a trace is .05¬¢ per trace. We priced the upgrade such that an extended retention trace costs 10x the price of a base tier trace (.50¬¢ per trace) including both metrics. Thus, each upgrade costs .45¬¢.\n‚ÄãRate Limits\nLangSmith has rate limits which are designed to ensure the stability of the service for all users.\nTo ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:\n‚ÄãScenarios\nTemporary throughput limit over a 1 minute period at our application load balancer"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 31,
    "content": "‚ÄãScenarios\nTemporary throughput limit over a 1 minute period at our application load balancer\nThis 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly ‚Äî it is not guaranteed to start at the start of a clock minute ‚Äî and may change depending on application deployment events.\nAfter the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats.\nThis 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 32,
    "content": "MethodEndpointLimitWindowDELETESessions301 minutePOST OR PATCHRuns50001 minutePOSTFeedback50001 minute**20001 minute\nThe LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.\nPlan-level hourly trace event limit\nThis 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.\nAn event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 33,
    "content": "This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.\nPlanLimitWindowDeveloper (no payment on file)50,000 events1 hourDeveloper (with payment on file)250,000 events1 hourStartup/Plus500,000 events1 hourEnterpriseCustomCustom\nPlan-level hourly trace data ingest limit\nThis 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 34,
    "content": "Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit.\nThis is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.\nPlanLimitWindowDeveloper (no payment on file)500MB1 hourDeveloper (with payment on file)2.5GB1 hourStartup/Plus5.0GB1 hourEnterpriseCustomCustom\nPlan-level monthly unique traces limit"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 35,
    "content": "Plan-level monthly unique traces limit\nThis 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.\nThis is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.\nPlanLimitWindowDeveloper (no payment on file)5,000 traces1 month\nSelf-configured monthly usage limits\nThis 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.\nThis is thrown by our application and varies by organization based on their configured settings."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 36,
    "content": "This is thrown by our application and varies by organization based on their configured settings.\n‚ÄãHandling 429s responses in your application\nSince some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter.\nFor convenience, LangChain applications built with the LangSmith SDK has this capability built-in."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 37,
    "content": "For convenience, LangChain applications built with the LangSmith SDK has this capability built-in.\nIt is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan.\n‚ÄãUsage Limits"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 38,
    "content": "‚ÄãUsage Limits\nLangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend.\nLangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 39,
    "content": "All traces limit\nExtended data retention traces limit"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 40,
    "content": "These let you limit the number of total traces, and extended data retention traces respectively.\n‚ÄãProperties of usage limiting\nUsage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.\n‚ÄãSide effects of extended data retention traces limit\nThe extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 41,
    "content": "match run rules\nadd feedback to traces\nadd runs to annotation queues\n\nEach of these features may cause an auto upgrade, so we shut them off when the limit is reached.\n‚ÄãUpdating usage limits\nUsage limits can be updated from the Settings page under Usage and Billing. Limit values are cached, so it may take a minute or two before the new limits apply.\n‚ÄãRelated content\n\nTutorial on how to optimize spend\n\n‚ÄãAdditional Resources\n\nRelease Versions: Learn about LangSmith‚Äôs version support policy, including Active, Critical, End of Life, and Deprecated support levels."
  },
  {
    "url": "https://docs.langchain.com/langsmith/administration-overview#rate-limits",
    "chunk_id": 42,
    "content": "Release Versions: Learn about LangSmith‚Äôs version support policy, including Active, Critical, End of Life, and Deprecated support levels.\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoCreate an account and API keyPreviousSet up a workspaceNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 0,
    "content": "How Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\nSee how Cisco Outshift built an AI Platform Engineer to boost productivity 10x, taking tasks like setting up CI/CD pipelines from a week to under an hour. \n\nCase Studies\n5 min read\nMay 4, 2025"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 1,
    "content": "Outshift is the incubation engine at Cisco, driving innovation in emerging technologies such as the Internet of Agents, Quantum, and next-generation infrastructure. The Platform Engineering team at Outshift offers foundational platform services to accelerate various incubation projects.Platform Engineers manage complex, distributed cloud-native SaaS environments involving multiple heterogeneous systems. Monitoring and diagnosing issues in these systems often requires rapidly locating information across these runtime environments, telemetry systems, and documentation sites.The small and mighty team of Platform engineers at Outshift had to context-switch and service frequent developer requests, ranging from access management to infrastructure provisioning, while developing new features to"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 2,
    "content": "and service frequent developer requests, ranging from access management to infrastructure provisioning, while developing new features to advance the platform. This led to:Long wait times for request fulfillment of simple and frequent requests, often taking days to complete.Increased cognitive load due to constant context switching between tools and workflows.Operational inefficiencies, where high-value engineering tasks were deprioritized in favor of routine platform maintenance.JARVIS: The AI Platform EngineerTo unlock a 10x productivity boost, the Cisco Outshift Platform Engineering team developed JARVIS, an AI Platform Engineer designed as a distributed Multi-Agent System (MAS).JARVIS is orchestrated using LangGraph for scalable and deterministic agent workflows and connected through"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 3,
    "content": "as a distributed Multi-Agent System (MAS).JARVIS is orchestrated using LangGraph for scalable and deterministic agent workflows and connected through the AGNTCY Agent Connect Protocol (ACP), an open source standard protocol, to enable seamless agent-to-agent collaboration across systems.Key Features of JARVISKnowledge ManagementJARVIS integrates with platform knowledge bases ‚Äî including documentation, policies, Jira, and code repositories ‚Äî using Retrieval-Augmented Generation (RAG) for unstructured data and GraphRAG for structured data to extract actionable insights from platform information.Self-Service CapabilitiesJARVIS automates many commonly requested developer tasks such as CI/CD onboarding, cloud resource provisioning, and developer sandbox environment setup ‚Äî dramatically"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 4,
    "content": "many commonly requested developer tasks such as CI/CD onboarding, cloud resource provisioning, and developer sandbox environment setup ‚Äî dramatically reducing turnaround times.Code GenerationJARVIS simplifies Kubernetes deployments by translating natural language inputs into K8s manifests and infrastructure templates through a hybrid machine learning approach.Seamless UX IntegrationJARVIS surfaces agentic AI capabilities directly into familiar developer interfaces ‚Äî including Jira, Backstage, Webex, and CLI ‚Äî allowing developers to interact with autonomous workflows without changing their existing tools or habits.Agentic Blueprint Behind JARVISThe development of JARVIS was grounded in AGNTCY‚Äôs Four-Phase approach to building resilient multi-agent systems on the Internet of Agents, an"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 5,
    "content": "JARVISThe development of JARVIS was grounded in AGNTCY‚Äôs Four-Phase approach to building resilient multi-agent systems on the Internet of Agents, an open, interoperable platform for agent-to-agent collaboration:1. DiscoverWe mapped critical platform workflows with specialized first- and third-party agents, laying the foundation for multi-agentic system.2. ComposeUsing LangGraph and the AGNTCY Agent Connect Protocol, we designed flexible, modular workflows where agents collaborate seamlessly across distributed environments.3. DeployJARVIS was operationalized across our cloud-native ecosystem, powered by the AGNTCY Workflow Server for scalable execution and coordination.4. EvaluateThrough continuous tracing, benchmarking, and feedback loops with LangSmith and agentevals, we refined agent"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 6,
    "content": "execution and coordination.4. EvaluateThrough continuous tracing, benchmarking, and feedback loops with LangSmith and agentevals, we refined agent behavior to drive consistent improvements over time.This approach made JARVIS modular, scalable, and ready to evolve with our growing platform needs.How Developers Use JARVIS: Real Interfaces in ActionTo maximize accessibility and adoption, JARVIS was integrated across multiple developer interfaces:Jira: Developers can assign tasks directly to the JARVIS AI Platform Engineer via Jira tickets. JARVIS autonomously executes the request and reaches out for additional input if needed.Backstage: A chat-based assistant embedded within our internal developer portal allows developers to trigger workflows and retrieve platform services seamlessly.Webex:"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 7,
    "content": "assistant embedded within our internal developer portal allows developers to trigger workflows and retrieve platform services seamlessly.Webex: A secure, conversational interface that delivers real-time notifications, task updates, and direct messaging interactions with JARVIS.CLI: Developers interact with JARVIS via the command line to provision sandbox applications, deploy infrastructure, and automate repetitive tasks with ease.By meeting developers where they already work, JARVIS drives adoption while enhancing platform usability and responsiveness.Below are some examples of JARVIS in action:Assigning Jira task to JARVIS AI EngineerInternal Developer Portal Chat Interface: User requesting an LLM Key using JARIVSLangGraph Studio Demonstration of multi-agent tool callingImpact of JARVIS"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 8,
    "content": "Developer Portal Chat Interface: User requesting an LLM Key using JARIVSLangGraph Studio Demonstration of multi-agent tool callingImpact of JARVIS at OutshiftJARVIS is¬† delivering significant productivity gains for Platform Engineering at Outshift:Tasks that previously took a week, such as setting up CI/CD pipelines, can now be completed in under an hour.Provisioning resources (e.g., S3 buckets, EC2 instances, LLM access keys) is now instantaneous, reducing what used to be half-day tasks to just seconds.Back-and-forth communication between developers and Platform Engineering for routine tasks has been eliminated, thanks to JARVIS‚Äôs ability to autonomously guide developers and retrieve needed information.The organization now handles a significantly higher volume of requests with the same"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 9,
    "content": "to autonomously guide developers and retrieve needed information.The organization now handles a significantly higher volume of requests with the same team size, while also reducing burnout and improving overall efficiency.Key Learnings in Building JARVIS AI Platform EngineerInternet of Agents (IoA) unlocks the true potential of Multi-Agent Systems: The future of platform engineering lies in multi-agent systems, where the seamless integration of first-party and third-party distributed agents automate complex platform workflows.Open standards like the AGNTCY Agent Connect Protocol (ACP) enable reliable agent-to-agent communication across heterogeneous systems, while frameworks like LangGraph provide scalable, deterministic workflow orchestrationStructuring multi-agent systems around the"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 10,
    "content": "systems, while frameworks like LangGraph provide scalable, deterministic workflow orchestrationStructuring multi-agent systems around the Four Phases ‚Äî Discover, Compose, Deploy, and Evaluate ‚Äî enables agent discoverability, promotes agent-to-agent collaboration, drives reuse, and simplifies the creation of complex, deterministic multi-agent systems.Seamless UX Integration is essential for agentic workflows. Embedding agentic capabilities directly into existing developer tools ‚Äî Jira, CLI, developer portals ‚Äî is critical for adoption. Combining GenAI-driven agent outputs with traditional interfaces ensures users can interact intuitively with complex workflows without changing their daily routines.Continuous evaluation and benchmarking ensure reliability, Delivering trustworthy agentic"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 11,
    "content": "complex workflows without changing their daily routines.Continuous evaluation and benchmarking ensure reliability, Delivering trustworthy agentic systems requires continuous tracing, monitoring, and performance evaluation. Using tracing solutions like LangSmith and evaluation frameworks like agentevals allows teams to analyze agent reasoning patterns, detect inconsistencies, and refine system performance to ensure high accuracy at scale.The Future of Agentic AI in Platform EngineeringOutshift is pioneering the integration of agentic AI into platform engineering ‚Äî building ecosystems where AI agents amplify human potential, enhance collaboration, and accelerate innovation. Their work with JARVIS is just the beginning. They're pushing the boundaries of what‚Äôs possible with AI-powered"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 12,
    "content": "and accelerate innovation. Their work with JARVIS is just the beginning. They're pushing the boundaries of what‚Äôs possible with AI-powered platforms, creating new foundations for the Internet of Agents.To see how the broader ecosystem is evolving, visit agntcy.org ‚Äî where the Outshift team is helping to build the collaboration layer that will let AI agents work together seamlessly.Explore how Outshift is driving the future of AI in platform engineering.Learn more about Outshift Incubations"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 13,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/cisco-outshift/",
    "chunk_id": 14,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow DocentPro Built a Multi-Agent Travel Companion with LangGraph\n\n\nCase Studies\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#tracing-and-human-oversight",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 0,
    "content": "Workflows and agents - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangGraph v1.0Release notesGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelAdd and manage memorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 1,
    "content": "APIsGraph APIFunctional APIRuntimeOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 2,
    "content": "This guide reviews common workflow and agent patterns."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 3,
    "content": "Workflows have predetermined code paths and are designed to operate in a certain order.\nAgents are dynamic and define their own processes and tool usage.\n\n\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\n‚ÄãSetup\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\n\nInstall dependencies:\n\nCopypip install langchain_core langchain-anthropic langgraph\n\n\nInitialize the LLM:\n\nCopyimport os\nimport getpass\n\nfrom langchain_anthropic import ChatAnthropic\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 4,
    "content": "def _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n\nllm = ChatAnthropic(model=\"claude-sonnet-4-5\")\n\n‚ÄãLLMs and augmentations\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\n\nCopy\n# Schema for structured output\nfrom pydantic import BaseModel, Field\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 5,
    "content": "# Augment the LLM with schema for structured output\nstructured_llm = llm.with_structured_output(SearchQuery)\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n\n# Define a tool\ndef multiply(a: int, b: int) -> int:\n    return a * b\n\n# Augment the LLM with tools\nllm_with_tools = llm.bind_tools([multiply])\n\n# Invoke the LLM with input that triggers the tool call\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\n\n# Get the tool call\nmsg.tool_calls\n\n‚ÄãPrompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It‚Äôs often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 6,
    "content": "Translating documents into different languages\nVerifying generated content for consistency\n\n\nGraph APIFunctional APICopyfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom IPython.display import Image, display\n\n\n# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    improved_joke: str\n    final_joke: str\n\n\n# Nodes\ndef generate_joke(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef check_punchline(state: State):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 7,
    "content": "def check_punchline(state: State):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n        return \"Pass\"\n    return \"Fail\"\n\n\ndef improve_joke(state: State):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n    return {\"improved_joke\": msg.content}\n\n\ndef polish_joke(state: State):\n    \"\"\"Third LLM call for final polish\"\"\"\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n    return {\"final_joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 8,
    "content": "# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_joke\", generate_joke)\nworkflow.add_node(\"improve_joke\", improve_joke)\nworkflow.add_node(\"polish_joke\", polish_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_joke\")\nworkflow.add_conditional_edges(\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n)\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\nworkflow.add_edge(\"polish_joke\", END)\n\n# Compile\nchain = workflow.compile()\n\n# Show workflow\ndisplay(Image(chain.get_graph().draw_mermaid_png()))"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 9,
    "content": "# Compile\nchain = workflow.compile()\n\n# Show workflow\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = chain.invoke({\"topic\": \"cats\"})\nprint(\"Initial joke:\")\nprint(state[\"joke\"])\nprint(\"\\n--- --- ---\\n\")\nif \"improved_joke\" in state:\n    print(\"Improved joke:\")\n    print(state[\"improved_joke\"])\n    print(\"\\n--- --- ---\\n\")\n\n    print(\"Final joke:\")\n    print(state[\"final_joke\"])\nelse:\n    print(\"Joke failed quality gate - no punchline detected!\")\n\n‚ÄãParallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 10,
    "content": "Split up subtasks and run them in parallel, which increases speed\nRun tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n\nGraph APIFunctional APICopy# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    story: str\n    poem: str\n    combined_output: str\n\n\n# Nodes\ndef call_llm_1(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 11,
    "content": "# Nodes\ndef call_llm_1(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef call_llm_2(state: State):\n    \"\"\"Second LLM call to generate story\"\"\"\n\n    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n    return {\"story\": msg.content}\n\n\ndef call_llm_3(state: State):\n    \"\"\"Third LLM call to generate poem\"\"\"\n\n    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n    return {\"poem\": msg.content}\n\n\ndef aggregator(state: State):\n    \"\"\"Combine the joke and story into a single output\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 12,
    "content": "def aggregator(state: State):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n    combined += f\"POEM:\\n{state['poem']}\"\n    return {\"combined_output\": combined}\n\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\nparallel_builder.add_node(\"aggregator\", aggregator)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 13,
    "content": "# Add edges to connect nodes\nparallel_builder.add_edge(START, \"call_llm_1\")\nparallel_builder.add_edge(START, \"call_llm_2\")\nparallel_builder.add_edge(START, \"call_llm_3\")\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\nparallel_builder.add_edge(\"aggregator\", END)\nparallel_workflow = parallel_builder.compile()\n\n# Show workflow\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\nprint(state[\"combined_output\"])"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 14,
    "content": "# Invoke\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\nprint(state[\"combined_output\"])\n\n‚ÄãRouting\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n\nGraph APIFunctional APICopyfrom typing_extensions import Literal\nfrom langchain.messages import HumanMessage, SystemMessage\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 15,
    "content": "# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\n\n# State\nclass State(TypedDict):\n    input: str\n    decision: str\n    output: str\n\n\n# Nodes\ndef llm_call_1(state: State):\n    \"\"\"Write a story\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_2(state: State):\n    \"\"\"Write a joke\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_3(state: State):\n    \"\"\"Write a poem\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_router(state: State):\n    \"\"\"Route the input to the appropriate node\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 16,
    "content": "def llm_call_router(state: State):\n    \"\"\"Route the input to the appropriate node\"\"\"\n\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=state[\"input\"]),\n        ]\n    )\n\n    return {\"decision\": decision.step}\n\n\n# Conditional edge function to route to the appropriate node\ndef route_decision(state: State):\n    # Return the node name you want to visit next\n    if state[\"decision\"] == \"story\":\n        return \"llm_call_1\"\n    elif state[\"decision\"] == \"joke\":\n        return \"llm_call_2\"\n    elif state[\"decision\"] == \"poem\":\n        return \"llm_call_3\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 17,
    "content": "# Build workflow\nrouter_builder = StateGraph(State)\n\n# Add nodes\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\n\n# Add edges to connect nodes\nrouter_builder.add_edge(START, \"llm_call_router\")\nrouter_builder.add_conditional_edges(\n    \"llm_call_router\",\n    route_decision,\n    {  # Name returned by route_decision : Name of next node to visit\n        \"llm_call_1\": \"llm_call_1\",\n        \"llm_call_2\": \"llm_call_2\",\n        \"llm_call_3\": \"llm_call_3\",\n    },\n)\nrouter_builder.add_edge(\"llm_call_1\", END)\nrouter_builder.add_edge(\"llm_call_2\", END)\nrouter_builder.add_edge(\"llm_call_3\", END)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 18,
    "content": "# Compile workflow\nrouter_workflow = router_builder.compile()\n\n# Show the workflow\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\nprint(state[\"output\"])\n\n‚ÄãOrchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:\n\nBreaks down tasks into subtasks\nDelegates subtasks to workers\nSynthesizes worker outputs into a final result"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 19,
    "content": "Breaks down tasks into subtasks\nDelegates subtasks to workers\nSynthesizes worker outputs into a final result\n\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\nGraph APIFunctional APICopyfrom typing import Annotated, List\nimport operator"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 20,
    "content": "# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 21,
    "content": "# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n\n‚ÄãCreating workers in LangGraph\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\nCopyfrom langgraph.types import Send"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 22,
    "content": "# Graph state\nclass State(TypedDict):\n    topic: str  # Report topic\n    sections: list[Section]  # List of report sections\n    completed_sections: Annotated[\n        list, operator.add\n    ]  # All workers write to this key in parallel\n    final_report: str  # Final report\n\n\n# Worker state\nclass WorkerState(TypedDict):\n    section: Section\n    completed_sections: Annotated[list, operator.add]\n\n\n# Nodes\ndef orchestrator(state: State):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n        ]\n    )\n\n    return {\"sections\": report_sections.sections}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 23,
    "content": "return {\"sections\": report_sections.sections}\n\n\ndef llm_call(state: WorkerState):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    section = llm.invoke(\n        [\n            SystemMessage(\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n            ),\n            HumanMessage(\n                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return {\"completed_sections\": [section.content]}\n\n\ndef synthesizer(state: State):\n    \"\"\"Synthesize full report from sections\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 24,
    "content": "def synthesizer(state: State):\n    \"\"\"Synthesize full report from sections\"\"\"\n\n    # List of completed sections\n    completed_sections = state[\"completed_sections\"]\n\n    # Format completed section to str to use as context for final sections\n    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n\n    return {\"final_report\": completed_report_sections}\n\n\n# Conditional edge function to create llm_call workers that each write a section of the report\ndef assign_workers(state: State):\n    \"\"\"Assign a worker to each section in the plan\"\"\"\n\n    # Kick off section writing in parallel via Send() API\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n\n\n# Build workflow\norchestrator_worker_builder = StateGraph(State)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 25,
    "content": "# Build workflow\norchestrator_worker_builder = StateGraph(State)\n\n# Add the nodes\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n\n# Add edges to connect nodes\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\norchestrator_worker_builder.add_conditional_edges(\n    \"orchestrator\", assign_workers, [\"llm_call\"]\n)\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\n\n# Compile the workflow\norchestrator_worker = orchestrator_worker_builder.compile()\n\n# Show the workflow\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 26,
    "content": "# Show the workflow\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n\nfrom IPython.display import Markdown\nMarkdown(state[\"final_report\"])"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 27,
    "content": "from IPython.display import Markdown\nMarkdown(state[\"final_report\"])\n\n‚ÄãEvaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\nEvaluator-optimizer workflows are commonly used when there‚Äôs particular success criteria for a task, but iteration is required to meet that criteria. For example, there‚Äôs not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 28,
    "content": "Graph APIFunctional APICopy# Graph state\nclass State(TypedDict):\n    joke: str\n    topic: str\n    feedback: str\n    funny_or_not: str\n\n\n# Schema for structured output to use in evaluation\nclass Feedback(BaseModel):\n    grade: Literal[\"funny\", \"not funny\"] = Field(\n        description=\"Decide if the joke is funny or not.\",\n    )\n    feedback: str = Field(\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nevaluator = llm.with_structured_output(Feedback)\n\n\n# Nodes\ndef llm_call_generator(state: State):\n    \"\"\"LLM generates a joke\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 29,
    "content": "# Nodes\ndef llm_call_generator(state: State):\n    \"\"\"LLM generates a joke\"\"\"\n\n    if state.get(\"feedback\"):\n        msg = llm.invoke(\n            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n        )\n    else:\n        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef llm_call_evaluator(state: State):\n    \"\"\"LLM evaluates the joke\"\"\"\n\n    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n\n\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\ndef route_joke(state: State):\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 30,
    "content": "if state[\"funny_or_not\"] == \"funny\":\n        return \"Accepted\"\n    elif state[\"funny_or_not\"] == \"not funny\":\n        return \"Rejected + Feedback\"\n\n\n# Build workflow\noptimizer_builder = StateGraph(State)\n\n# Add the nodes\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n\n# Add edges to connect nodes\noptimizer_builder.add_edge(START, \"llm_call_generator\")\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\noptimizer_builder.add_conditional_edges(\n    \"llm_call_evaluator\",\n    route_joke,\n    {  # Name returned by route_joke : Name of next node to visit\n        \"Accepted\": END,\n        \"Rejected + Feedback\": \"llm_call_generator\",\n    },\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 31,
    "content": "# Compile the workflow\noptimizer_workflow = optimizer_builder.compile()\n\n# Show the workflow\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\nprint(state[\"joke\"])\n\n‚ÄãAgents\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\nUsing toolsCopyfrom langchain.tools import tool"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 32,
    "content": "To get started with agents, see the quickstart or read more about how they work in LangChain.\nUsing toolsCopyfrom langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 33,
    "content": "Graph APIFunctional APICopyfrom langgraph.graph import MessagesState\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\n\n\n# Nodes\ndef llm_call(state: MessagesState):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n    return {\n        \"messages\": [\n            llm_with_tools.invoke(\n                [\n                    SystemMessage(\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                    )\n                ]\n                + state[\"messages\"]\n            )\n        ]\n    }\n\n\ndef tool_node(state: dict):\n    \"\"\"Performs the tool call\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 34,
    "content": "def tool_node(state: dict):\n    \"\"\"Performs the tool call\"\"\"\n\n    result = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool = tools_by_name[tool_call[\"name\"]]\n        observation = tool.invoke(tool_call[\"args\"])\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n    return {\"messages\": result}\n\n\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 35,
    "content": "messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then perform an action\n    if last_message.tool_calls:\n        return \"tool_node\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Build workflow\nagent_builder = StateGraph(MessagesState)\n\n# Add nodes\nagent_builder.add_node(\"llm_call\", llm_call)\nagent_builder.add_node(\"tool_node\", tool_node)\n\n# Add edges to connect nodes\nagent_builder.add_edge(START, \"llm_call\")\nagent_builder.add_conditional_edges(\n    \"llm_call\",\n    should_continue,\n    [\"tool_node\", END]\n)\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n# Compile the agent\nagent = agent_builder.compile()\n\n# Show the agent\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "chunk_id": 36,
    "content": "# Compile the agent\nagent = agent_builder.compile()\n\n# Show the agent\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nmessages = agent.invoke({\"messages\": messages})\nfor m in messages[\"messages\"]:\n    m.pretty_print()\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#problem",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 0,
    "content": "LangChain - Changelog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\n\n\n\n\n\n\nLangChain\nLangSmith\nLangGraph\n\n\n\n\n\n\nMethods\n\n\n\n\n\n\n\nRetrieval\nAgents\nEvaluation\n\n\n\n\n\n\nResources\n\n\n\n\n\n\n\nBlog\nCase Studies\nLangChain Academy\nCommunity\nExperts\nChangelog\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\nPython\n\nLangChain\nLangSmith\nLangGraph\n\n\n\nJavaScript\n\nLangChain\nLangSmith\nLangGraph\n\n\n\n\n\n\nCompany\n\n\n\n\n\n\n\nAbout\nCareers\n\n\n\n\n\nPricing\n\n\nGet a demo\n\n\nSign up\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain Changelog\n\n\n\n\n\nSign up for our newsletter to stay up to date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPowered by LaunchNotes\n\n\n\n\n\n\n\n\n\n\n\n  Loading...\n\n\n\n\n\n\n\n\n\n\n\n  Loading...\n\n\n\n\n    October 2025\n  \n\n\n\nProduct naming changes: LangSmith Deployment and LangSmith Studio"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 1,
    "content": "Loading...\n\n\n\n\n\n\n\n\n\n\n\n  Loading...\n\n\n\n\n    October 2025\n  \n\n\n\nProduct naming changes: LangSmith Deployment and LangSmith Studio\n\n\n      We‚Äôve updated product names across the LangSmith UI, docs, and website to make it clearer how our tools fit together. What changed You‚Äôll see these new names...\n    \n\nOctober 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith\n\n\n\n\nJavaScript Code Evals for faster eval workflows\n\n\n      JavaScript Code Evals allow you to write custom Code Evals in JavaScript in addition to Python. This gives you the flexibility to choose the language that...\n    \n\nOctober  7, 2025\n\n\n\n\n    September 2025\n  \n\n\n\n\n\n        v0.11.51\n      \n\n\n\n\n\n\nLangSmith\n\n\n\n\nComposite Evaluators in LangSmith"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 2,
    "content": "October  7, 2025\n\n\n\n\n    September 2025\n  \n\n\n\n\n\n        v0.11.51\n      \n\n\n\n\n\n\nLangSmith\n\n\n\n\nComposite Evaluators in LangSmith\n\n\n      If you‚Äôre running multiple evaluators to measure different aspects of your application, it can be difficult to see the overall picture. Composite Evaluators...\n    \n\nSeptember 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain\n\n\n\n\nMiddleware in LangChain 1.0 alpha\n\n\n      Middleware in LangChain 1.0 gives you fine-grained control around the core agent loop of models + tools. It includes: Read the blog about middleware and try...\n    \n\nSeptember 16, 2025\n\n\n\n\n\n\n\n\n        v0.11.31\n      \n\n\n\n\n\n\nLangSmith\n\n\n\n\nCreate org-scoped API keys with granular permissions"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 3,
    "content": "September 16, 2025\n\n\n\n\n\n\n\n\n        v0.11.31\n      \n\n\n\n\n\n\nLangSmith\n\n\n\n\nCreate org-scoped API keys with granular permissions\n\n\n      You can now create org-scoped service keys and optionally, assign roles for created keys. These updates give you more control, allowing you to: Learn more...\n    \n\nSeptember 10, 2025\n\n\n\n\n    August 2025\n  \n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith\n\n\n\n\nRevision queueing in LangSmith Deployment\n\n\n      Keep your deployments smooth and predictable. With Revision Queueing , any new revisions are automatically lined up and processed only after the current one...\n    \n\nAugust 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith Self-Hosted\n\n\n\n\nLangSmith Self-Hosted v0.11"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 4,
    "content": "August 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith Self-Hosted\n\n\n\n\nLangSmith Self-Hosted v0.11\n\n\n      This release brings customizable LangGraph Platform deployments, streamlined evaluation via Align Evals, running evals directly from Studio, and operational...\n    \n\nAugust 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith\n\n\n\n\nTrace Mode in LangSmith Studio\n\n\n      Debugging and improving agents just got a lot smoother. With Trace Mode , you can now view your LangSmith traces directly in Studio ‚Äîno more jumping between...\n    \n\nAugust 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph\n\n\n\n\nDynamic tool calling in LangGraph agents\n\n\n      Agents don‚Äôt always need the same tools at every step. With dynamic tool calling , you can now control which tools are available at different points in a...\n    \n\nAugust  6, 2025"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 5,
    "content": "August  6, 2025\n\n\n\n\n    July 2025\n  \n\n\n\n\n\n\n\n\n\n\n\n\nLangSmith\n\n\n\n\nConnect traces in LangSmith to agent deployment server logs\n\n\n      We‚Äôve made agentic observability even easier ‚Äî you can now connect traces in LangSmith to server logs within LangSmith Deployment. When viewing a trace...\n    \n\nJuly 31, 2025\n\n\n\n\n\n\n\n\n\n\n\nleft chevron\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n              ‚Ä¶\n            \n\n\n16\n\n\n\nleft chevron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJump to Month\n\nOctober 2025\n\n\nSeptember 2025\n\n\nAugust 2025\n\n\nJuly 2025\n\n\nJune 2025\n\n\nMay 2025\n\n\nApril 2025\n\n\nMarch 2025\n\n\nFebruary 2025\n\n\nJanuary 2025\n\n\nDecember 2024\n\n\nNovember 2024\n\n\nOctober 2024\n\n\nSeptember 2024\n\n\nAugust 2024\n\n\nJuly 2024\n\n\nJune 2024\n\n\nMay 2024\n\n\nApril 2024\n\n\nMarch 2024\n\n\nFebruary 2024\n\n\nJanuary 2024\n\n\nDecember 2023\n\n\nNovember 2023\n\n\nOctober 2023"
  },
  {
    "url": "https://changelog.langchain.com/",
    "chunk_id": 6,
    "content": "July 2024\n\n\nJune 2024\n\n\nMay 2024\n\n\nApril 2024\n\n\nMarch 2024\n\n\nFebruary 2024\n\n\nJanuary 2024\n\n\nDecember 2023\n\n\nNovember 2023\n\n\nOctober 2023\n\n\n\n\n\n\nPowered by LaunchNotes\n\n\n\n\n\n\n\n\n\n\nSubscribe to updates\n\n√ó\n\n\n\n\n\nEmail\n\n\n\n\n\n\n\n\nSubscribe\n\n              By clicking subscribe, you accept our privacy policy and\n              terms and conditions.\n\n                \n                  reCAPTCHA privacy and terms apply"
  },
  {
    "url": "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    "chunk_id": 0,
    "content": "These docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n\n\n          \n  \n    \n  \n  LangGraph\n\n        \n\n\n\n          \n  \n    \n  \n  Agents\n\n        \n\n\n\n          \n  \n    \n  \n  API reference\n\n        \n\n\n\n          \n  \n    \n  \n  Versions\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n    \n  \n    LangGraph\n  \n\n    \n  \n\n\n\n\n\n\n    \n  \n    Agents\n  \n\n    \n  \n\n\n\n\n\n\n    \n  \n    API reference\n  \n\n    \n  \n\n\n\n\n\n\n    \n  \n    Versions\n  \n\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n404 - Not found\n\n\n\n\n\n\n\n  Back to top"
  },
  {
    "url": "https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/",
    "chunk_id": 1,
    "content": "API reference\n  \n\n    \n  \n\n\n\n\n\n\n    \n  \n    Versions\n  \n\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n404 - Not found\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n      Copyright ¬© 2025 LangChain, Inc | Consent Preferences\n\n  \n  \n    Made with\n    \n      Material for MkDocs Insiders\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCookie consent\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. Clicking \"Accept\" makes our documentation better. Thank you! ‚ù§Ô∏è\n\n\n\n\n\n\n\n          GitHub\n        \n\n\n\n\nAccept\nReject"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#conclusion",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 0,
    "content": "How AppFolio transformed property management workflows with Realm-X, built using LangGraph and LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow AppFolio transformed property management workflows with Realm-X, built using LangGraph and LangSmith\nSee how AppFolio's AI-powered copilot Realm-X has saved property managers over 10 hours per week. Learn how they improved Realm-X's performance 2x using LangSmith and built an agent architecture with LangGraph.\n\nCase Studies\n4 min read\nDec 16, 2024"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 1,
    "content": "AppFolio, the technology leader powering the future of the real estate industry, has released Realm-X Assistant, an AI-powered copilot designed to improve the efficiency of property managers‚Äô day-to-day work. Realm-X is embedded generative AI that provides intelligent, real-time assistance by combining the latest foundation models with industry-specific context. Realm-X provides a conversational interface that helps users understand the state of their business, get help, and execute actions in bulk ‚Äì whether it‚Äôs querying information, sending messages, or scheduling actions related to residents, vendors, units, bills, or work orders and many more. Early users have reported saving over 10 hours a week in completing their to-do list.While designing Realm-X, AppFolio realized that they"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 2,
    "content": "many more. Early users have reported saving over 10 hours a week in completing their to-do list.While designing Realm-X, AppFolio realized that they needed a better natural language interface to help property managers engage with the platform and simplify operational processes. The team turned to LangChain, LangGraph, and LangSmith to improve their execution flow and add visibility into how users were interacting with Realm-X.¬†Realm-X assistant interfaceChoosing LangGraph for flexibility and controlRealm-X Assistant was initially built using LangChain for interoperability, making it easy to switch out model providers without changing any code. LangChain also made it easy to use tools and structured outputs when making requests with their tool calling agent.¬†As the Assistant evolved,"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 3,
    "content": "code. LangChain also made it easy to use tools and structured outputs when making requests with their tool calling agent.¬†As the Assistant evolved, AppFolio needed a way to handle even more complexity with their requests. Their team made a strategic transition from LangChain to LangGraph, which simplified response aggregation from different nodes to display to the user.¬†Realm-X architecture with LangGraphAfter switching to LangGraph, the AppFolio team could clearly see the execution flow of the Realm-X Assistant, which allowed them to design workflows that could reason before acting. By parallelizing branch execution, AppFolio has also been able to reduce latency and code complexity.¬†¬†One major benefit of LangGraph has been its ability to run independent code branches in parallel. While"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 4,
    "content": "able to reduce latency and code complexity.¬†¬†One major benefit of LangGraph has been its ability to run independent code branches in parallel. While determining relevant actions, it simultaneously calculates fallbacks and runs a question-answering bot over help pages. This allows Realm-X to offer related suggestions which enhances the user experience, while keeping latency at a minimum.¬†Leveraging LangSmith to monitor and pinpoint issues in productionTo add a layer of visibility into their production traffic, AppFolio used LangSmith to facilitate debugging during development and gain more insights into user interactions. In production, the AppFolio team keeps a close eye on real-time feedback charts in LangSmith ‚Äî tracking error rates, costs, and latency to keep everything on course. The"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 5,
    "content": "team keeps a close eye on real-time feedback charts in LangSmith ‚Äî tracking error rates, costs, and latency to keep everything on course. The team also added in automatic triggers to collect feedback when users submit an action drafted by Realm-X.¬†In addition, automatic feedback is generated based on LLM or heuristic evaluators to continuously monitor system health.LangSmith‚Äôs tracing also makes it easy for AppFolio to pinpoint issues when they occur. During development, engineers rely on comparison views and the LangSmith playground to iterate on workflows, ensuring they're battle-tested before deployment. Traces are also shareable across the team, streamlining the process of collaborating across stakeholders.Iterating on prompts to improve system performance¬†One key innovation that"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 6,
    "content": "the team, streamlining the process of collaborating across stakeholders.Iterating on prompts to improve system performance¬†One key innovation that LangSmith supports in AppFolio‚Äôs system is dynamic few-shot prompting ‚Äî which involves dynamically pulling relevant examples to deliver more personalized and accurate responses to Realm-X users. With LangSmith, the AppFolio team has been able to quickly identify whether wrong samples were used or if a relevant sample was missing or poorly formatted, ultimately helping optimize the examples pulled for a given query.The comparison view in LangSmith was particularly useful for identifying subtle differences in how prompts were constructed, helping AppFolio ensure precise outputs. The LangSmith Playground also enabled the team to rapidly iterate on"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 7,
    "content": "in how prompts were constructed, helping AppFolio ensure precise outputs. The LangSmith Playground also enabled the team to rapidly iterate on prompts, base models, or tool descriptions with modifying the underlying code ‚Äì shortening the feedback cycle between stakeholders.Dynamic few-shot prompting has been crucial in improving the performance of specific Realm-X features like its text-to-data functionality, where performance significantly increased from ~40% to ~80%. This showcases the impact of tailored examples in improving system accuracy. AppFolio has also maintained high performance as they‚Äôve increased the number of actions and data models that users can query in Realm-X.LLM evaluations for continuous improvementAppFolio prioritizes user experience and feedback in its testing and"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 8,
    "content": "models that users can query in Realm-X.LLM evaluations for continuous improvementAppFolio prioritizes user experience and feedback in its testing and evaluation process. Every step in the Realm-X workflow‚Äî from individual actions to end-to-end executions‚Äî is rigorously tested using custom evaluators and LangSmith‚Äôs tools.To ensure quality, AppFolio maintains a central repository of sample cases containing message history, metadata, and ideal outputs. These can be used as evals, unit tests, or examples. Evals are run in CI, with results tracked and integrated into PRs. To avoid regressions, code changes merge only when all unit tests pass and eval thresholds are met.LangGraph also helps streamline complex workflows like text-to-data processing by organizing intricate if-statement logic"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 9,
    "content": "and eval thresholds are met.LangGraph also helps streamline complex workflows like text-to-data processing by organizing intricate if-statement logic into clear, flexible code paths. This, combined with rigorous testing, ensures that Realm-X Assistant produces accurate and reliable responses for property managers.The path forward for AppFolioAs AppFolio continues to innovate with the Realm-X Assistant, the focus remains on enhancing user-centric features and optimizing performance. As an early LangSmith user, AppFolio seamlessly integrated LangGraph with LangSmith‚Äôs robust monitoring, which helped Realm-X Assistant aggregate responses from multiple system parts to ensure clear, actionable user outputs.Looking ahead, AppFolio is expanding Realm-X to further improve overall performance and"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 10,
    "content": "system parts to ensure clear, actionable user outputs.Looking ahead, AppFolio is expanding Realm-X to further improve overall performance and reliability by using LangGraph for state management and self-validation loops. By combining these tools with their commitment to innovation, AppFolio continues to empower property managers everywhere to succeed."
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 11,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-appfolio/",
    "chunk_id": 12,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#problem",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 0,
    "content": "LangChain partners with Elastic to launch the Elastic AI Assistant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain partners with Elastic to launch the Elastic AI Assistant\n\nCase Studies\n3 min read\nJan 30, 2024"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 1,
    "content": "Elastic, a leading search analytics company, serving over 20k customers worldwide, enables organizations to securely harness search-powered AI so anyone can find the answers they need in real-time using all their data, at scale. By integrating AI with search technology, the company¬† facilitates the discovery of actionable insights from large volumes of both structured and unstructured data, addressing the need for real-time, scalable data processing. They have cloud-based solutions for search, security, and observability, which help businesses deliver on the promise of AI, and recently, with the help of LangChain and LangSmith, they added an AI Assistant to their security suite.The Elastic AI Assistant for security is designed to be a premium product that supports the security analyst"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 2,
    "content": "an AI Assistant to their security suite.The Elastic AI Assistant for security is designed to be a premium product that supports the security analyst workflow. Specifically the product helps security teams with tasks such as:Alert summarization to explain why an alert was triggered along with a recommended playbook to remediate the attack. This feature generates a dynamic runbook for the organization to provide orderliness during an event.Workflow suggestions to guide users on how to complete tasks such as adding an alert exception or creating a custom dashboard.¬†Query generation and conversion to support users in migrating from other SIEMs to Elastic more easily. Now, a user can paste a query from another product, or in natural language, and Elastic AI Assistant will convert it into an"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 3,
    "content": "to Elastic more easily. Now, a user can paste a query from another product, or in natural language, and Elastic AI Assistant will convert it into an Elastic query using proper syntax.Agent integration advice to guide on the best way to collect data in Elastic.And much more.This is an enterprise-only feature, and since its initial launch in June 2023, Elastic has seen significant adoption of the Assistant.The AI Assistant has proven to significantly reduce a customer‚Äôs MTTR (Mean time to respond) to alerts generated by Elastic Security, as well as reduce the time it takes to write queries and detection rules, thanks to its ability to craft queries based off of natural language use cases.How LangChain and LangSmith supported the product developmentElastic designed their application to be"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 4,
    "content": "queries based off of natural language use cases.How LangChain and LangSmith supported the product developmentElastic designed their application to be agnostic to the LLM from the start, as they wanted their end users to be able to bring their own model and would need to support OpenAI, Azure OpenAI, and Bedrock (amongst others). Giving users this level of control and flexibility from the beginning was a requirement.Fortunately, much of the tooling to create a RAG application came natively with LangChain, and since LangChain also abstracts the application logic from each of the underlying components, the Elastic team was able to create swappable models and prompts, depending on the user‚Äôs preference of vendor, without much engineering overhead. As a bonus, LangChain already had a great"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 5,
    "content": "models and prompts, depending on the user‚Äôs preference of vendor, without much engineering overhead. As a bonus, LangChain already had a great integration with Elastic‚Äôs vector database, so it was a perfect fit for the job.As the team started to add additional functionality into the AI Assistant, such as the ability to generate queries in Elastic‚Äôs new query language - ES|QL, LangSmith was critical in helping the Elastic team understand what exactly got sent to the model, how long did the full trace take, and how many tokens were consumed in the process. LangSmith helped the team better understand how different models could be good at different tasks and at different price points. This visibility allowed the development team to think through tradeoffs and create as consistent of an"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 6,
    "content": "different tasks and at different price points. This visibility allowed the development team to think through tradeoffs and create as consistent of an experience as possible across all three models supported. As the team iterated on their application, LangSmith helped highlight variances and prevented regressions from making it to production.‚ÄúWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn‚Äôt have achieved¬† the product experience delivered to our customers without LangChain, and we couldn‚Äôt have done it at the same pace without LangSmith.‚Äù says James Spiteri, Director of Security Product Management, at Elastic.LangChain and LangSmith also supported"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 7,
    "content": "at the same pace without LangSmith.‚Äù says James Spiteri, Director of Security Product Management, at Elastic.LangChain and LangSmith also supported Elastic‚Äôs workflow to deliver a secure application to the enterprise. Mindful that their users were security experts and naturally skeptical, the Elastic team built out data masking in the app to obfuscate any sensitive data before it was sent to the LLM, exposed the token tracking directly in the end product, so that users have full visibility on usage, and integrated their role based access control with the experience so that admins could limit usage as they wanted.What‚Äôs next with Elastic AI Assistant?The goal of the AI Assistant is to alleviate as much work as possible for the security analyst and give them more time back in their day."
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 8,
    "content": "Assistant?The goal of the AI Assistant is to alleviate as much work as possible for the security analyst and give them more time back in their day. While the product supports three model providers today, the team wants to expand to more models to service an even wider audience.¬†The next big step in the AI Assistant is to leverage LangChain‚Äôs agent framework so that more¬† work can be achieved in the background and have users approve actions. Moving beyond knowledge assistance will take the application to the next level, and the Elastic team feels confident they can deliver with the help of LangChain and LangSmith.Giving back to the communityIn the spirit of open source, the Elastic team has made available a lot of their code that powers the Elastic AI Assistant. You can see exactly how the"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 9,
    "content": "the spirit of open source, the Elastic team has made available a lot of their code that powers the Elastic AI Assistant. You can see exactly how the team implemented their solution by checking out the repository here. For other exciting educational content on ML development with Elastic, check out Elastic Search Labs. Enjoy!"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 10,
    "content": "Tags\nCase StudiesBy LangChain\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.com/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 11,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/agents",
    "chunk_id": 0,
    "content": "Agents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upDesign agents with controlAdd human oversight and create stateful, scalable workflows with AI agents. Get started with PythonGet started with JavaScript"
  },
  {
    "url": "https://www.langchain.com/agents",
    "chunk_id": 1,
    "content": "Stay in the driver's seat. Build copilots that write first drafts for review, act on your behalf, or wait for approval before execution.Customize your agent runtime with LangGraphLangGraph provides control for custom agent and multi-agent workflows, seamless human-in-the-loop interactions, and native streaming support for enhanced agent reliability and execution.About LangGraph\n\n\n001Force call a tool002Wait for human-in-the-loop approval003Multiple agents collaborating on a common goal004Stream intermediate steps as they happenBuild the right cognitive architecture for your applicationIdentify and implement the best prompting strategies ‚Ä®and architectures so that your LLMs perform as intended.Go to Docs\n\n\n001\n\n\n\nPlan-and-execute002\n\n\n\nMulti-agent003\n\n\n\nCritique Revise004\n\n\n\nReAct005"
  },
  {
    "url": "https://www.langchain.com/agents",
    "chunk_id": 2,
    "content": "001\n\n\n\nPlan-and-execute002\n\n\n\nMulti-agent003\n\n\n\nCritique Revise004\n\n\n\nReAct005\n\n\n\nSelf-askTools for every taskLangChain offers an extensive library of off-the-shelf tools ‚Ä®and an intuitive framework for customizing your own.Autonomous, ‚Ä®but well‚ÄëbehavedGo to Docs\n\n\nDebugging agents got you down? LangSmith can help.LangSmith gives you the explainability to understand why your agents¬†go off track and how to get them humming again.Explore LangSmith"
  },
  {
    "url": "https://www.langchain.com/agents",
    "chunk_id": 3,
    "content": "001End-to-end trace observability002Tool selection visibility003Playground with prompt, model, and tool definition¬†variability004Cost, latency, and error tracking005LLM evaluators for agent runsResources for AgentsLangChain Academy CourseIntroduction to LangGraphdata ReportState of AI¬†Agents (2024)use casesBreakout Agentic AppsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your"
  },
  {
    "url": "https://www.langchain.com/agents",
    "chunk_id": 4,
    "content": "AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#ux",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 0,
    "content": "LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upBalance agent control with agencyGain control with LangGraph to design agents that reliably handle complex tasks.Start building"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 1,
    "content": "Introduction to LangGraphLearn the basics of LangGraph in this LangChain Academy Course. You'll learn how to build agents that automate real-world tasks with LangGraph orchestration.Enroll for freeBook enterprise trainingTrusted by companies shaping the future of agentsSee LangGraph use cases in production"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 2,
    "content": "Controllable cognitive architecture for any taskLangGraph's flexible framework supports diverse control flows ‚Äì single agent, multi-agent, hierarchical, sequential ‚Äì and robustly handles realistic, complex scenarios. Ensure reliability with easy-to-add moderation and quality loops that prevent agents from veering off course.Use LangGraph Platform to templatize your cognitive architecture so that tools, prompts, and models are easily configurable with LangGraph Platform Assistants.See the docs"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 3,
    "content": "Thousands of companies build AI apps better with LangChain products.Read our select customer stories.Designed for human-agent collaborationWith built-in statefulness, LangGraph agents seamlessly collaborate with humans by writing drafts for review and awaiting approval before acting. Easily inspect the agent‚Äôs actions and \"time-travel\" to roll back and take a different action to correct course.Read a conceptual guide\n\n\nHow does LangGraph help?Guide, moderate, and control your agent with human-in-the-loopPrevent agents from veering off course with easy-to-add moderation and quality controls. Add human-in-the-loop checks to steer and approve agent actions.Learn how to add human-in-the-loop"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 4,
    "content": "Build expressive, customizable agent workflowsLangGraph‚Äôs low-level primitives provide the flexibility needed to create fully customizable agents. Design diverse control flows ‚Äî single, multi-agent, hierarchical ‚Äî all using one framework.See different agent architectures\n\n\nPersist context for long-term interactionsLangGraph‚Äôs built-in memory stores conversation histories and maintains context over time, enabling rich, personalized interactions across sessions.Learn about agent memory\n\n\nFirst-class streaming for better UX designBridge user expectations and agent capabilities with native token-by-token streaming, showing agent reasoning and actions in real time.See how to use streaming"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 5,
    "content": "First class streaming support for better UX designBridge user expectations and agent capabilities with native token-by-token streaming and streaming of intermediate steps, helpful for showing agent reasoning and actions back to the user as they happen. Use LangGraph Platform's API to deliver dynamic and interactive user experiences.Learn more\n\n\nIntroduction to LangGraphLearn the basics of LangGraph in this LangChain Academy Course. You'll learn how to build agents that automate real-world tasks with LangGraph orchestration.Enroll for freeDeploy agents at scale, monitor carefully, iterate boldlyDesign agent-driven user experiences with LangGraph Platform's APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options."
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 6,
    "content": "Fault-tolerant scalabilityHandle large workloads gracefully with horizontally-scaling servers, task queues, and built-in persistence. Enhance resilience with intelligent caching and automated retries.\n\n\n\nDynamic APIs for designing agent experienceCraft personalized user experiences with APIs featuring long-term memory to recall information across conversation sessions. Track, update, and rewind your app's state for easy human steering and interaction. Kick off long-running background jobs for research-style or multi-step work."
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 7,
    "content": "Integrated developer experienceSimplify prototyping, debugging, and sharing of agents in our visual LangGraph Studio. Deploy your application with 1-click deploy with our SaaS offering or within your own VPC. Then, monitor app performance with LangSmith.Developers trust LangGraph to build reliable agentsLangGraph helps teams of all sizes, across all industries, build reliable agents ready for production.Hear how industry leaders use LangGraph"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 8,
    "content": "‚ÄúLangChain is streets ahead with what they've put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚ÄùGarrett SpongPrincipal SWE ‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 9,
    "content": "how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚ÄùAndres TorresSr. Solutions Architect‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù‚ÄúAs Ally advances its exploration of Generative AI, Sathish MuthukrishnanChief Information, Data and Digital Officer‚ÄúLangChain is streets ahead with what they've put forward with LangGraph. LangGraph sets the foundation for how we can build and"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 10,
    "content": "and Digital Officer‚ÄúLangChain is streets ahead with what they've put forward with LangGraph. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚ÄùGarrett SpongPrincipal SWE ‚ÄúLangGraph has been instrumental for our AI development. Its robust framework for building stateful, multi-actor applications with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 11,
    "content": "with LLMs has transformed how we evaluate and optimize the performance of our AI guest-facing solutions. LangGraph enables granular control over the agent's thought process, which has empowered us to make data-driven and deliberate decisions to meet the diverse needs of our guests.‚ÄùAndres TorresSr. Solutions Architect‚ÄúAs Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments with multi-actor agentic workflows. We are committed to deepening our partnership with LangChain.‚Äù‚ÄúAs Ally advances its exploration of Generative AI, Sathish MuthukrishnanChief Information, Data and Digital Officer"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 12,
    "content": "LangGraph FAQsHow is LangGraph different from other agent frameworks?\n\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company‚Äôs needs. LangGraph provides a more expressive framework to handle companies‚Äô unique tasks without restricting users to a single black-box cognitive architecture.Does LangGraph impact the performance of my app?\n\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.Is LangGraph open source? Is it free?\n\nYes. LangGraph is an MIT-licensed open-source library and is free to use.What are my options if I want to deploy my agent?"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 13,
    "content": "You can deploy your LangGraph application using LangSmith Deployment, available in the LangSmith Plus and Enterprise plans. which has the following agent deployment options: Cloud:¬†Fully managed and hosted as part of LangSmith (our unified observability¬†& evals platform).¬†Deploy quickly, with automatic updates and zero maintenance. ‚ÄçHybrid (SaaS control plane, self-hosted data plane). No data leaves your VPC. Provisioning and scaling is managed as a service.‚ÄçFully Self-Hosted: Deploy LangGraph entirely on your own infrastructure.‚ÄçReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Contact UsSign UpProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith"
  },
  {
    "url": "https://www.langchain.com/langgraph",
    "chunk_id": 14,
    "content": "for every step of the agent development lifecycle.Contact UsSign UpProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEngineer reliable agentsShip agents to production with LangChain's comprehensive platform for agent engineering.Request a demoSign Up"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 1,
    "content": "LangChain products power top engineering teams, from AI startups to global enterprisesVisibility &¬†controlSee exactly what's happening at every step of your agent. Steer your agent to accomplish critical tasks the way you intended.Fast iterationRapidly move through build, test, deploy, learn, repeat with workflows across the entire agent engineering lifecycle.Durable performanceShip at scale with agent infrastructure designed for long-running workloads and human oversight.Model neutralSwap models, tools, and databases without rewriting your app. Future-proof your stack as AI advances with no vendor lock-in.Your agent engineering stackOpen Source FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 2,
    "content": "FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph puts you in control with low-level primitives to build custom agent workflows.Agent Engineering PlatformLangSmithObservabilityEvaluationDeploymentObservabilityEvaluationDeploymentSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 3,
    "content": "Improve agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 4,
    "content": "Build agents your way, with templates or custom controlBring your own frameworkLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK¬†to gain visibility into your agent interactions, whether you use LangChain's frameworks or not.Open Source Frameworks¬†Bring your ownAgent Engineering PlatformLangSmithObservabilityEvaluationDeploymentOpen Source FrameworksBuild agents your way, with templates or custom controlLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraph puts you in control with low-level primitives to build custom agent workflows.LangSmith Agent Engineering PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 5,
    "content": "PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 6,
    "content": "LangSmith Agent Engineering PlatformImprove agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nLangSmith Agent Engineering PlatformDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 7,
    "content": "CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access to information and tools in a compliant manner so they can perform their best.Customer SupportImprove the speed and efficiency of support teams that handle customer requests.ResearchSynthesize data, summarize sources, and uncover insights faster for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 8,
    "content": "Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\n\n\nFinancial ServicesKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\n\n\nB2B SaaSElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\n\n\nAI/MLReplit's AI Agent serves 30+ million developers. Their AI engineers rely on LangSmith to debug complex traces."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-1",
    "chunk_id": 9,
    "content": "Learn alongside the 1 million+ practitioners who are pushing the industry forward90MMonthly downloads100k+GitHub stars#1Downloaded agent framework1000IntegrationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 0,
    "content": "GitHub - langchain-ai/langgraphjs: Framework to build resilient language agents as graphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation Menu\n\nToggle navigation\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n            Sign in\n          \n\n\n \n\n\nAppearance settings\n\n\n\n\n\n\n\n\n\n\n\n        Platform\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          GitHub Copilot\n\n        \n\n        Write better code with AI\n      \n\n\n\n\n\n\n\n\n          GitHub Spark\n\n            \n              New\n            \n\n\n        Build and deploy intelligent apps\n      \n\n\n\n\n\n\n\n\n          GitHub Models\n\n            \n              New\n            \n\n\n        Manage and compare prompts\n      \n\n\n\n\n\n\n\n\n          GitHub Advanced Security"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 1,
    "content": "New\n            \n\n\n        Manage and compare prompts\n      \n\n\n\n\n\n\n\n\n          GitHub Advanced Security\n\n        \n\n        Find and fix vulnerabilities\n      \n\n\n\n\n\n\n\n\n          Actions\n\n        \n\n        Automate any workflow\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Codespaces\n\n        \n\n        Instant dev environments\n      \n\n\n\n\n\n\n\n\n          Issues\n\n        \n\n        Plan and track work\n      \n\n\n\n\n\n\n\n\n          Code Review\n\n        \n\n        Manage code changes\n      \n\n\n\n\n\n\n\n\n          Discussions\n\n        \n\n        Collaborate outside of code\n      \n\n\n\n\n\n\n\n\n          Code Search\n\n        \n\n        Find more, search less\n      \n\n\n\n\n\n\nExplore\n\n\n\n      Why GitHub\n\n    \n\n\n\n      Documentation\n\n    \n\n\n\n\n\n      GitHub Skills\n\n    \n\n\n\n\n\n      Blog\n\n    \n\n\n\n\n\n\nIntegrations"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 2,
    "content": "Explore\n\n\n\n      Why GitHub\n\n    \n\n\n\n      Documentation\n\n    \n\n\n\n\n\n      GitHub Skills\n\n    \n\n\n\n\n\n      Blog\n\n    \n\n\n\n\n\n\nIntegrations\n\n\n\n      GitHub Marketplace\n\n    \n\n\n\n      MCP Registry\n\n    \n\n\n\n\n\n\n\n              View all features\n              \n\n\n \n\n\n\n\n        Solutions\n        \n\n\n\n\n\n\n\nBy company size\n\n\n\n      Enterprises\n\n    \n\n\n\n      Small and medium teams\n\n    \n\n\n\n      Startups\n\n    \n\n\n\n      Nonprofits\n\n    \n\n\n\n\n\n\nBy use case\n\n\n\n      App Modernization\n\n    \n\n\n\n      DevSecOps\n\n    \n\n\n\n      DevOps\n\n    \n\n\n\n      CI/CD\n\n    \n\n\n\n      View all use cases\n\n    \n\n\n\n\n\n\nBy industry\n\n\n\n      Healthcare\n\n    \n\n\n\n      Financial services\n\n    \n\n\n\n      Manufacturing\n\n    \n\n\n\n      Government\n\n    \n\n\n\n      View all industries"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 3,
    "content": "Financial services\n\n    \n\n\n\n      Manufacturing\n\n    \n\n\n\n      Government\n\n    \n\n\n\n      View all industries\n\n    \n\n\n\n\n\n\n\n              View all solutions\n              \n\n\n \n\n\n\n\n        Resources\n        \n\n\n\n\n\n\n\nTopics\n\n\n\n      AI\n\n    \n\n\n\n      DevOps\n\n    \n\n\n\n      Security\n\n    \n\n\n\n      Software Development\n\n    \n\n\n\n      View all\n\n    \n\n\n\n\n\n\nExplore\n\n\n\n      Learning Pathways\n\n    \n\n\n\n\n\n      Events & Webinars\n\n    \n\n\n\n      Ebooks & Whitepapers\n\n    \n\n\n\n      Customer Stories\n\n    \n\n\n\n      Partners\n\n    \n\n\n\n      Executive Insights\n\n    \n\n\n\n\n\n\n\n\n\n        Open Source\n        \n\n\n\n\n\n\n\n\n\n\n\n\n          GitHub Sponsors\n\n        \n\n        Fund open source developers\n      \n\n\n\n\n\n\n\n\n\n          The ReadME Project\n\n        \n\n        GitHub community articles"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 4,
    "content": "Fund open source developers\n      \n\n\n\n\n\n\n\n\n\n          The ReadME Project\n\n        \n\n        GitHub community articles\n      \n\n\n\n\nRepositories\n\n\n\n      Topics\n\n    \n\n\n\n      Trending\n\n    \n\n\n\n      Collections\n\n    \n\n\n\n\n\n\n\n\n\n        Enterprise\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Enterprise platform\n\n        \n\n        AI-powered developer platform\n      \n\n\n\n\nAvailable add-ons\n\n\n\n\n\n\n\n\n          GitHub Advanced Security\n\n        \n\n        Enterprise-grade security features\n      \n\n\n\n\n\n\n\n\n          Copilot for business\n\n        \n\n        Enterprise-grade AI features\n      \n\n\n\n\n\n\n\n\n          Premium Support\n\n        \n\n        Enterprise-grade 24/7 support\n      \n\n\n\n\n\n\n\n\nPricing\n\n\n\n\n\n\n\n\n\n\n\n\nSearch or jump to...\n\n\n\n\n\n\n\nSearch code, repositories, users, issues, pull requests..."
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 5,
    "content": "Pricing\n\n\n\n\n\n\n\n\n\n\n\n\nSearch or jump to...\n\n\n\n\n\n\n\nSearch code, repositories, users, issues, pull requests...\n\n \n\n\n\n\n        Search\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\nClear\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nSearch syntax tips \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Provide feedback\n      \n\n\n\n\n\n\n\n\n\n \nWe read every piece of feedback, and take your input very seriously.\n\n\nInclude my email address so I can be contacted\n\n\n     Cancel\n\n    Submit feedback\n\n\n\n\n\n\n\n\n\n\n        Saved searches\n      \nUse saved searches to filter your results more quickly\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nName\n\n\n\n\n\n\nQuery\n\n\n\n            To see all available qualifiers, see our documentation.\n          \n \n\n\n\n\n\n     Cancel\n\n    Create saved search\n\n\n\n\n\n\n\n\n                Sign in\n              \n\n\n                Sign up"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 6,
    "content": "Cancel\n\n    Create saved search\n\n\n\n\n\n\n\n\n                Sign in\n              \n\n\n                Sign up\n              \n\n\n \n\n\nAppearance settings\n\n\n\nResetting focus\n\n\n\n\n\n\n\n\n\nYou signed in with another tab or window. Reload to refresh your session.\nYou signed out in another tab or window. Reload to refresh your session.\nYou switched accounts on another tab or window. Reload to refresh your session.\n \n\n\nDismiss alert\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        langchain-ai\n \n/\n\nlanggraphjs\n\nPublic\n\n\n\n\n\n \n\nNotifications\n You must be signed in to change notification settings\n\n\n \n\nFork\n    331\n\n\n\n\n \n\n\n          Star\n 2.2k\n\n\n\n\n\n\n\n\n        Framework to build resilient language agents as graphs.\n      \n\n\n\n\n\ndocs.langchain.com/oss/javascript/langgraph/\n\n\nLicense\n\n\n\n\n\n     MIT license"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 7,
    "content": "docs.langchain.com/oss/javascript/langgraph/\n\n\nLicense\n\n\n\n\n\n     MIT license\n    \n\n\n\n\n\n\n2.2k\n          stars\n \n\n\n\n331\n          forks\n \n\n\n\nBranches\n \n\n\n\nTags\n \n\n\n\nActivity\n \n\n\n\n \n\n\n          Star\n\n\n\n\n \n\nNotifications\n You must be signed in to change notification settings\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\nIssues\n54\n\n\n\n\n\n\nPull requests\n17\n\n\n\n\n\n\nDiscussions\n\n\n\n\n\n\n\nActions\n\n\n\n\n\n\n\nProjects\n0\n\n\n\n\n\n\nSecurity\n\n\n\n\n\n        Uh oh!\n\n There was an error while loading. Please reload this page.\n\n \n \n\n\n\n\n\n\n\nInsights\n\n\n\n \n\n \n\n\nAdditional navigation options\n\n\n \n\n\n\n\n\n\n\n\n\n\n          Code\n\n\n\n\n\n\n\n\n\n\n\n          Issues\n\n\n\n\n\n\n\n\n\n\n\n          Pull requests\n\n\n\n\n\n\n\n\n\n\n\n          Discussions\n\n\n\n\n\n\n\n\n\n\n\n          Actions\n\n\n\n\n\n\n\n\n\n\n\n          Projects\n\n\n\n\n\n\n\n\n\n\n\n          Security\n\n\n\n\n\n\n\n\n\n\n\n          Insights"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 8,
    "content": "Actions\n\n\n\n\n\n\n\n\n\n\n\n          Projects\n\n\n\n\n\n\n\n\n\n\n\n          Security\n\n\n\n\n\n\n\n\n\n\n\n          Insights\n\n\n\n\n\n\n \n\n\n\n\n\nlangchain-ai/langgraphjs"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 9,
    "content": "Insights\n\n\n\n\n\n\n \n\n\n\n\n\nlangchain-ai/langgraphjs\n\n\n\n \n\n\n\n   ¬†mainBranchesTagsGo to fileCodeOpen more actions menuFolders and filesNameNameLast commit messageLast commit dateLatest commit¬†History2,470 Commits.changeset.changeset¬†¬†.devcontainer.devcontainer¬†¬†.github.github¬†¬†.vscode.vscode¬†¬†.yarn.yarn¬†¬†docsdocs¬†¬†examplesexamples¬†¬†internalinternal¬†¬†libslibs¬†¬†scriptsscripts¬†¬†.gitignore.gitignore¬†¬†.yarnrc.yml.yarnrc.yml¬†¬†CLAUDE.mdCLAUDE.md¬†¬†CONTRIBUTING.mdCONTRIBUTING.md¬†¬†LICENSELICENSE¬†¬†README.mdREADME.md¬†¬†deno.jsondeno.json¬†¬†int-test-deps-docker-compose.ymlint-test-deps-docker-compose.yml¬†¬†package.jsonpackage.json¬†¬†tsconfig.jsontsconfig.json¬†¬†turbo.jsonturbo.json¬†¬†yarn.lockyarn.lock¬†¬†View all filesRepository files navigationREADMEContributingMIT licenseü¶úüï∏Ô∏èLangGraph.js"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 10,
    "content": "NoteLooking for the Python version? See the Python repo and the Python docs."
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 11,
    "content": "LangGraph ‚Äî used by Replit, Uber, LinkedIn, GitLab and more ‚Äî is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration ‚Äî offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\nnpm install @langchain/langgraph @langchain/core\nTo learn more about how to use LangGraph, check out the docs. We show a simple example below of how to create a ReAct agent.\n// npm install @langchain-anthropic\nimport { createReactAgent, tool } from \"langchain\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nimport { z } from \"zod\";"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 12,
    "content": "import { z } from \"zod\";\n\nconst search = tool(\n  async ({ query }) => {\n    if (\n      query.toLowerCase().includes(\"sf\") ||\n      query.toLowerCase().includes(\"san francisco\")\n    ) {\n      return \"It's 60 degrees and foggy.\";\n    }\n    return \"It's 90 degrees and sunny.\";\n  },\n  {\n    name: \"search\",\n    description: \"Call to surf the web.\",\n    schema: z.object({\n      query: z.string().describe(\"The query to use in your search.\"),\n    }),\n  }\n);\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-7-sonnet-latest\",\n});\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [search],\n});"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 13,
    "content": "const model = new ChatAnthropic({\n  model: \"claude-3-7-sonnet-latest\",\n});\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [search],\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"what is the weather in sf\",\n    },\n  ],\n});\nFull-stack Quickstart\nGet started quickly by building a full-stack LangGraph application using the create-agent-chat-app CLI:\nnpx create-agent-chat-app@latest\nThe CLI sets up a chat interface and helps you configure your application, including:\n\nüß† Choice of 4 prebuilt agents (ReAct, Memory, Research, Retrieval)\nüåê Frontend framework (Next.js or Vite)\nüì¶ Package manager (npm, yarn, or pnpm)"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 14,
    "content": "üß† Choice of 4 prebuilt agents (ReAct, Memory, Research, Retrieval)\nüåê Frontend framework (Next.js or Vite)\nüì¶ Package manager (npm, yarn, or pnpm)\n\nWhy use LangGraph?\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 15,
    "content": "Why use LangGraph?\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\n\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives ‚Äì free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time."
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 16,
    "content": "LangGraph is trusted in production and powering agents for companies like:\n\nKlarna: Customer support bot for 85 million active users\nElastic: Security AI assistant for threat detection\nUber: Automated unit test generation\nReplit: Code generation\nAnd many more (see list here)\n\nLangGraph‚Äôs ecosystem\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 17,
    "content": "LangSmith ‚Äî Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\nLangGraph Platform ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in LangGraph Studio.\n\nPairing with LangGraph Platform\nWhile LangGraph is our open-source agent orchestration framework, enterprises that need scalable agent deployment can benefit from LangGraph Platform.\nLangGraph Platform can help engineering teams:"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 18,
    "content": "Accelerate agent development: Quickly create agent UXs with configurable templates and LangGraph Studio for visualizing and debugging agent interactions.\nDeploy seamlessly: We handle the complexity of deploying your agent. LangGraph Platform includes robust APIs for memory, threads, and cron jobs plus auto-scaling task queues & servers.\nCentralize agent management & reusability: Discover, reuse, and manage agents across the organization. Business users can also modify agents without coding.\n\nAdditional resources"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 19,
    "content": "LangChain Forum: Connect with the community and share all of your technical questions, ideas, and feedback.\nLangChain Academy: Learn the basics of LangGraph in our free, structured course.\nTutorials: Simple walkthroughs with guided examples on getting started with LangGraph.\nTemplates: Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted.\nHow-to Guides: Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.).\nAPI Reference: Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components."
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 20,
    "content": "API Reference: Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\nBuilt with LangGraph: Hear how industry leaders use LangGraph to ship powerful, production-ready AI applications."
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 21,
    "content": "Acknowledgements\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n   \n\n\n\n\n\n\n\n\nAbout\n\n        Framework to build resilient language agents as graphs.\n      \n\n\n\n\n\ndocs.langchain.com/oss/javascript/langgraph/\n\n\nTopics\n\n\n\n  node\n\n\n  typescript\n\n\n  ai\n\n\n  artificial-intelligence\n\n\n  agents\n\n\n  llm\n\n\n  generative-ai\n\n\n\nResources\n\n\n\n\n\n        Readme\n \nLicense\n\n\n\n\n\n     MIT license\n    \n\nContributing\n\n\n\n\n\n        Contributing\n      \n\n\n\n\n\n\n        Uh oh!\n\n There was an error while loading. Please reload this page.\n\n \n \n\n\n\n\n\nActivity \n\n\n\n\nCustom properties \nStars\n\n\n\n\n2.2k\n      stars \nWatchers\n\n\n\n\n24\n      watching \nForks\n\n\n\n\n331\n      forks"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 22,
    "content": "Activity \n\n\n\n\nCustom properties \nStars\n\n\n\n\n2.2k\n      stars \nWatchers\n\n\n\n\n24\n      watching \nForks\n\n\n\n\n331\n      forks \n\n\n          Report repository\n \n\n\n\n\n\n\nReleases\n      143\n\n\n\n\n\n\n@langchain/langgraph-cli@0.0.43\n\n          Latest\n \nJul 7, 2025\n\n \n+ 142 releases\n\n\n\n\n\nPackages\n      0\n\n        No packages published \n\n\n\n\n\n\nUsed by 7.7k \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        + 7,698\n      \n\n\n\n\n\n\nContributors\n      81\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+ 67 contributors\n\n\n\n\nLanguages\n\n\n\n\n\n\n\n\n\n\n\n\nTypeScript\n97.6%\n\n\n\n\n\n\n\nJavaScript\n1.8%\n\n\n\n\n\n\n\nOther\n0.6%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFooter\n\n\n\n\n\n\n\n\n        ¬© 2025 GitHub,¬†Inc.\n      \n\n\nFooter navigation\n\n\nTerms\n\n\nPrivacy\n\n\nSecurity\n\n\nStatus\n\n\nCommunity\n\n\nDocs\n\n\nContact\n\n\n\n\n       Manage cookies"
  },
  {
    "url": "https://github.com/langchain-ai/langgraphjs",
    "chunk_id": 23,
    "content": "Footer navigation\n\n\nTerms\n\n\nPrivacy\n\n\nSecurity\n\n\nStatus\n\n\nCommunity\n\n\nDocs\n\n\nContact\n\n\n\n\n       Manage cookies\n    \n\n\n\n\n\n      Do not share my personal information\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    You can‚Äôt perform that action at this time."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#prompt-engineering",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 0,
    "content": "LangGraph Server - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationDeployment componentsLangGraph ServerGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsOverviewLangGraph ServerData planeControl planeRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 1,
    "content": "modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesOn this pageApplication structureParts of a deploymentGraphsPersistence and task queueLearn moreConfigure for deploymentDeployment componentsLangGraph ServerCopy pageCopy pageLangGraph Server offers an API for creating and managing agent-based applications. It is built on the concept of assistants, which are agents"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 2,
    "content": "pageLangGraph Server offers an API for creating and managing agent-based applications. It is built on the concept of assistants, which are agents configured for specific tasks, and includes built-in persistence and a task queue. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions."
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 3,
    "content": "Use LangGraph Server to create and manage assistants, threads, runs, cron jobs, webhooks, and more.\nAPI reference\nFor detailed information on the API endpoints and data models, refer to the API reference docs.\nTo use the Enterprise version of the LangGraph Server, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, contact our sales team.\nYou can run the Enterprise version of the LangGraph Server on the following deployment options:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 4,
    "content": "Cloud\nHybrid\nSelf-hosted"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 5,
    "content": "‚ÄãApplication structure\nTo deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.\nRead the application structure guide to learn how to structure your LangGraph application for deployment.\n‚ÄãParts of a deployment\nWhen you deploy LangGraph Server, you are deploying one or more graphs, a database for persistence, and a task queue.\n‚ÄãGraphs\nWhen you deploy a graph with LangGraph Server, you are deploying a ‚Äúblueprint‚Äù for an Assistant.\nAn Assistant is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases\nthat can be served by the same graph."
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 6,
    "content": "that can be served by the same graph.\nUpon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph‚Äôs default configuration settings.\nWe often think of a graph as implementing an agent, but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple\nchatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use multiple agents working in tandem.\n‚ÄãPersistence and task queue\nLangGraph Server leverages a database for persistence and a task queue.\nPostgreSQL is supported as a database for LangGraph Server and Redis as the task queue."
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 7,
    "content": "PostgreSQL is supported as a database for LangGraph Server and Redis as the task queue.\nIf you‚Äôre deploying using LangSmith cloud, these components are managed for you. If you‚Äôre deploying LangGraph Server on your own infrastructure, you‚Äôll need to set up and manage these components yourself.\nFor more information on how these components are set up and managed, review the hosting options guide.\n‚ÄãLearn more"
  },
  {
    "url": "https://docs.langchain.com/langsmith/langgraph-server#persistence-and-task-queue",
    "chunk_id": 8,
    "content": "LangGraph Application Structure guide explains how to structure your LangGraph application for deployment.\nThe API Reference provides detailed information on the API endpoints and data models.\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoLangSmith componentsPreviousLangSmith data planeNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/resources-items/breakout-agentic-apps",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://blog.langchain.dev/tag/in-the-loop/",
    "chunk_id": 0,
    "content": "In the Loop - LangChain Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Loop\n\n\n17 Articles\n    \n\n\n\n\n\n\n\n\n\n\n\n\nNot Another Workflow Builder\nBy Harrison Chase\n\nOne of the most common requests we‚Äôve gotten from day zero of LangChain has been a visual workflow builder. We never\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Agents\nUsing an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are ‚Äúshallow‚Äù\n\n\nIn the Loop\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nThe rise of \"context engineering\"\nHeader image from Dex Horthy on Twitter."
  },
  {
    "url": "https://blog.langchain.dev/tag/in-the-loop/",
    "chunk_id": 1,
    "content": "In the Loop\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nThe rise of \"context engineering\"\nHeader image from Dex Horthy on Twitter.\n\nContext engineering is building dynamic systems to provide the right information and tools in the right format such\n\n\nIn the Loop\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow and when to build multi-agent systems\nLate last week two great blog posts were released with seemingly opposite titles. ‚ÄúDon‚Äôt Build Multi-Agents‚Äù by the Cognition team, and ‚ÄúHow we built\n\n\nIn the Loop\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Metric That Determines AI Product Success\nCo-authored by Assaf Elovic and Harrison Chase. You can also find a version of this post published on Assaf's Medium.\n\nWhy do some\n\n\nIn the Loop\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to think about agent frameworks\nTL;DR:"
  },
  {
    "url": "https://blog.langchain.dev/tag/in-the-loop/",
    "chunk_id": 2,
    "content": "Why do some\n\n\nIn the Loop\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to think about agent frameworks\nTL;DR:\n\n * The hard part of building reliable agentic systems is making sure the LLM has the appropriate context at each step. This includes both\n\n\nIn the Loop\n20 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I speed up my AI agent?\nI get this question a bunch. Developers generally first spend time getting the agent to work, but then they turn their attention to speed and\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nMCP: Flash in the Pan or Future Standard?\nModel Context Protocol (MCP) is creating quite the stir on Twitter ‚Äì but is it actually useful, or just noise? In this back and forth, Harrison\n\n\nIn the Loop\n5 min read"
  },
  {
    "url": "https://blog.langchain.dev/tag/in-the-loop/",
    "chunk_id": 3,
    "content": "In the Loop\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication is all you need\n‚ÄúWhat we‚Äôve got here is failure to communicate‚Äù - Cool Hand Luke (1967)\n\nCommunication is the hardest part of life. It‚Äôs also the\n\n\nIn the Loop\n7 min read\n\n\n\n\n\n\n\n\n\n\n\n\nMemory for agents\nAt Sequoia‚Äôs AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. Check out that talk here. In\n\n\nIn the Loop\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nUX for Agents, Part 3: Spreadsheet, Generative, and Collaborative UI/UX\nLearn about spreadsheet UX for batch agent workloads, Generative UI, and collaborative UX with agents.\n\n\nIn the Loop\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/tag/in-the-loop/",
    "chunk_id": 4,
    "content": "In the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nUX for Agents, Part 2: Ambient\nThis is our second post focused on UX for agents. We discuss ambient background agents, which can handle multiple tasks at the same time, and how they can be used in your workflow.\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n            Page\n            1\n            of\n            2\n            \n\n\n\nLoad More\nSomething went wrong with loading more posts\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#evaluation",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#cursor-steals-the-spotlight",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/resources",
    "chunk_id": 0,
    "content": "Resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upGuides\n\n\nFilters\n\nGuides & ReportsUse cases & inspirationMethodsThank you! Your submission has been received!Oops! Something went wrong while submitting the form.Use cases & inspirationUpcomingBuilt with LangGraphSee customer stories"
  },
  {
    "url": "https://www.langchain.com/resources",
    "chunk_id": 1,
    "content": "Use cases & inspirationUpcomingBuilt with LangGraphSee customer stories\n\n\nGuides & ReportsUpcomingThe Definitive Guide to Testing LLM ApplicationsDownload now\n\n\nGuides & ReportsUpcomingThe Definitive Guide to Testing LLM ApplicationsDownload now\n\n\nUse cases & inspirationUpcomingBreakout Agentic AppsGet inspired\n\n\nUse cases & inspirationUpcomingBreakout Agentic AppsGet inspired\n\n\nGuides & ReportsUpcomingLangChain State of AI 2024 ReportSee product data\n\n\nGuides & ReportsUpcomingLangChain State of AI 2024 ReportSee product data\n\n\nGuides & ReportsUpcomingState of AI Agents Survey & ReportRead the report\n\n\nGuides & ReportsUpcomingState of AI Agents Survey & ReportRead the report\n\n\nMethodsUpcomingLLM EvaluationsLearn the method\n\n\nMethodsUpcomingLLM EvaluationsLearn the method"
  },
  {
    "url": "https://www.langchain.com/resources",
    "chunk_id": 2,
    "content": "MethodsUpcomingLLM EvaluationsLearn the method\n\n\nMethodsUpcomingLLM EvaluationsLearn the method\n\n\nShow moreReady to start shipping ‚Ä®reliable, GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Contact UsSign UpProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upBuild agents faster, your wayLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool ‚Äî so you can build agents that adapt as fast as the ecosystem evolves.Get started"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 1,
    "content": "Why use LangChain?Ship fast with proven agent patternsBuild agents in minutes with templates for common use cases. create_agent provides a proven ReAct pattern on LangGraph's durable runtime.Open and neutral by designSwap models, tools, and databases without rewriting your application. With 1000+ integrations, you can future-proof your stack as AI advances, with no vendor lock-in.Customize without complexityExtend agent behavior through middleware without rewriting core logic. Add human-in-the-loop approval, compress long conversations, or remove sensitive data‚Äî all with simple, composable hooks.Durable runtimeLangChain runs on LangGraph‚Äôs durable runtime ‚Äî giving agents built-in persistence, rewind, checkpointing, and human-in-the-loop support.Ship fast with proven agent patternsBuild"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 2,
    "content": "durable runtime ‚Äî giving agents built-in persistence, rewind, checkpointing, and human-in-the-loop support.Ship fast with proven agent patternsBuild agents in minutes with templates for common use cases. create_agent provides a proven ReAct pattern on LangGraph's durable runtime.Open and neutral by designSwap models, tools, and databases without rewriting your application. With 1000+ integrations, you can future-proof your stack as AI advances, with no vendor lock-in.Customize without complexityExtend agent behavior through middleware without rewriting core logic. Add human-in-the-loop approval, compress long conversations, or remove sensitive data‚Äî all with simple, composable hooks.Durable runtimeLangChain runs on LangGraph‚Äôs durable runtime ‚Äî giving agents built-in persistence, rewind,"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 3,
    "content": "data‚Äî all with simple, composable hooks.Durable runtimeLangChain runs on LangGraph‚Äôs durable runtime ‚Äî giving agents built-in persistence, rewind, checkpointing, and human-in-the-loop support.Instantly connect to your preferred LLM.See all our integrations"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 4,
    "content": "LangChain FAQsIs LangChain open source?\n\nYes - LangChain is an MIT-licensed open-source library and is free to use.What are the most common ways people use LangChain?\n\nThere are many different use cases for LangChain. Some common ones that we see include: chatbots and conversational interfaces, document Q&A and knowledge retrieval systems, and data extraction. LangChain excels when you need to connect LLMs to external data sources, APIs, or tools‚Äì anywhere you need maximum integration flexibility.How do I use LangChain with LangSmith?"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 5,
    "content": "LangChain is an open source framework with pre-built agent architectures and as integrations to models, tools, and databases to start building agents quickly. The LangChain framework integrates seamlessly with LangSmith, our platform for agent observability, evaluation, and deployment ‚Äî you can set just one environment variable to get started.How is LangChain different from LangGraph?"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 6,
    "content": "Use LangChain to get started quickly on simple use cases. Use LangGraph when you need to build agents that can reliably handle complex tasks. ¬† ¬† LangChain is often used to build standard agents like SQL queries and document analysis, customer support bots, and predictable linear workflows with minimal customization. ¬† ¬†LangGraph is often used for multi-agent research with specialized roles, long-running processes, use cases requiring human-in-the-loop approvals, and custom architectures with advanced control flow.‚ÄçCan I use LangChain in production?"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 7,
    "content": "Yes. We‚Äôve released LangChain 1.0, which means we‚Äôve committed to no breaking changes until 2.0. Companies like Rakuten, Cisco, and Moody‚Äôs use LangChain in production for business-critical workflows.Ready to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/langchain",
    "chunk_id": 8,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 0,
    "content": "Community Code of Conduct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 1,
    "content": "AboutCareersPricingGet a demoSign upCommunityCode of ConductThis Code of Conduct is applicable to all LangChain Community spaces ‚Äî whether online or offline. This includes Slack, code repositories, and meetups. All participants are expected to be familiar with and adhere to this Code of Conduct, which is divided into three sections:‚ÄçLangChain Community Values: These values are fundamental to all of our community spaces.‚ÄçCommunity Member Expectations: We‚Äôve written these rules to help members understand how to best participate.‚ÄçAnti-harassment Policy: We‚Äôre committed to a harassment-free environment for everyone in our community. This section clearly defines what that entails.Thank you for helping us create a community we can all be proud of!‚Äç‚Äî The LangChain Community TeamCommunity"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 2,
    "content": "section clearly defines what that entails.Thank you for helping us create a community we can all be proud of!‚Äç‚Äî The LangChain Community TeamCommunity ValuesPay it forward.Strive to contribute more to the community than you receive from it. The strength of our community lies in collective effort and shared knowledge. You can demonstrate this value by:Sharing knowledge: Participate in discussions, offer solutions, and share resources with other members.Contributing code: Contribute to LangChain‚Äôs open-source packages. Beyond writing code, you can also create issues or participate in discussions.Contributing content: Suggest edits to our documentation, write blog posts, or create tutorials that benefit others.Organizing and participating in events: Lead a community meetup, host a workshop,"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 3,
    "content": "write blog posts, or create tutorials that benefit others.Organizing and participating in events: Lead a community meetup, host a workshop, or actively participate in events.Be friendly.Create a space where everyone feels welcome. You can demonstrate this value by:Extending a warm welcome: Make new members feel welcome and encourage participation from everyone.Having empathy: Understand and respect different perspectives and experiences.Stay curious.Seek out new knowledge, ask questions, and support others in their learning journey. You can demonstrate this value by:Problem-solving independently: Experiment and explore solutions on your own before asking for assistance.Sharing your thought process: When posing questions or providing answers, explain the reasoning behind your approaches.Be"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 4,
    "content": "asking for assistance.Sharing your thought process: When posing questions or providing answers, explain the reasoning behind your approaches.Be authentic.Authentic interactions build stronger connections and trust. You can demonstrate this value by:Creating a personal profile: Use your real name and photo in the Slack community, and introduce yourself!Sharing your perspectives: Share your personal experiences and advice; everyone has an unique perspective to offer.Community Member ExpectationsRule 1: Respect All MembersWe strive to bring a positive and rewarding experience for everyone in this community. Therefore, we have zero tolerance for any form of disrespectful behavior. Treat others as you would like to be treated and assume best intentions. We want to create a space where people"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 5,
    "content": "any form of disrespectful behavior. Treat others as you would like to be treated and assume best intentions. We want to create a space where people lift each other up and encourage each other‚Äôs work. We have all seen dark corners of the internet riddled with pessimism and harsh critique. While feedback is welcome and solicited regularly in the community, we want to be defined by collective optimism and support.We ask everyone to use real identities because we want you to be mindful of your tone and discourse in all forums. When in doubt, pay extra consideration if your words are constructive and supportive, and contribute to making the space a place one in which people can share and create freely.All participants in LangChain Community spaces ‚Äì including Slack, code repositories, and"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 6,
    "content": "a place one in which people can share and create freely.All participants in LangChain Community spaces ‚Äì including Slack, code repositories, and meetups ‚Äì must adhere to the Community Code of Conduct. If you cannot comply with these guidelines, we ask that you refrain from participating in the community. We will remove people from the community who do not follow our conduct guidelines.Rule 2: Practice Proper Messaging EtiquetteEnsure your questions are carefully considered, send your question in a single message, use threads, and post in the appropriate channels. Do not seek extra attention by tagging individuals, double-posting, or bumping your message by sending to the channel.Rule 3: Default to Using Public ChannelsBy default, keep interactions public in LangChain Community spaces. If"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 7,
    "content": "your message by sending to the channel.Rule 3: Default to Using Public ChannelsBy default, keep interactions public in LangChain Community spaces. If someone is directly messaging you in a way that doesn‚Äôt uphold the values of the community or is against the Code of Conduct, you are encouraged to report behavior to community@langchain.dev.Rule 4: Do Not SolicitThis community is designed for developers to share their work, ideas, and learnings. It is not meant for lead generation by vendors or recruiters. Any vendors or recruiters soliciting members will be permanently banned from the community.Anti-harassment PolicyWe are committed to a harassment-free environment for all community members. Harassment in any form will not be tolerated.Harassment includes:Making disrespectful or hurtful"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 8,
    "content": "environment for all community members. Harassment in any form will not be tolerated.Harassment includes:Making disrespectful or hurtful comments about someone's gender, sexual orientation, disability, appearance, age, race, or religion.Making threats of violence.Encouraging violence against anyone.Intentionally intimidating others.Stalking or persistently following someone.Taking photos or recording someone to harass them, including tracking online activities for harassment.Continuously disrupting conversations.Sharing private messages without consent, even if they are not harassing.Repeatedly making personal requests or advancing intimacy that makes others uncomfortable.Sharing sexual images or engaging in sexual behavior in inappropriate places.Giving unwelcome sexual"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 9,
    "content": "intimacy that makes others uncomfortable.Sharing sexual images or engaging in sexual behavior in inappropriate places.Giving unwelcome sexual attention.Persisting in one-on-one communication after being asked to stop.Making unwelcome remarks about someone's lifestyle choices, such as their diet, health, or job.Please be considerate and obtain permission before sharing anyone's image or name on social media, especially during in-person events.Reporting Harassment and ConsequencesIf you experience harassment within LangChain Community spaces, observe someone else being harassed, or have any related concerns, please reach out to us at community@langchain.dev or follow the #moderation-and-administration guidance on Slack.To protect victims of abuse, we will honor confidentiality requests."
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 10,
    "content": "or follow the #moderation-and-administration guidance on Slack.To protect victims of abuse, we will honor confidentiality requests. However, at our discretion, we may disclose the identity of an individual against whom harassment complaints have been made, either publicly or through private warnings to third parties, if it enhances the safety of LangChain community members or the general public. We will never disclose the identity of harassment victims without their explicit consent.Should a participant engage in harassment, the LangChain Community team may take any actions they consider necessary, which could include removal from all LangChain Community spaces and informing other community members or the public of the participant‚Äôs behavior.LangSmith for Startups and Education.  Seed"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 11,
    "content": "Community spaces and informing other community members or the public of the participant‚Äôs behavior.LangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups"
  },
  {
    "url": "https://www.langchain.com/community-code",
    "chunk_id": 12,
    "content": "Education\n\n\nEnquire about startup pricing\n\n\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\n\n\nEducation\n\n\nProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy---test-revenuehero",
    "chunk_id": 0,
    "content": "Contact Sales Copy - Test (RevenueHero)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy---test-revenuehero",
    "chunk_id": 1,
    "content": "AboutCareersPricingGet a demoSign upTrusted by the best teams building with LLMs:Talk to our teamLangChain provides a suite of products to help teams build AI applications. This includes LangSmith ‚Äî our platform for prompt engineering, evaluation, and observability of LLM apps ‚Äî and LangGraph Platform for building and deploying AI agents at scale.Get in touch. Our team will answer your questions and follow up with a product demo to help you drive results.Looking for support? Email support here.Trusted by the best teams building with LLMs:First NameLast NameCompany NameWork Email* ¬†personal emails will not be accepted.Job TitleWhich of our products are you interested in?LangSmithLangGraph PlatformCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global HeadquartersNA - West CoastNA -"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy---test-revenuehero",
    "chunk_id": 2,
    "content": "are you interested in?LangSmithLangGraph PlatformCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global HeadquartersNA - West CoastNA - CentralNA - East CoastAPACEMEALATAMMessage (optional)Thank you for requesting a demo. There‚Äôs an email waiting in your inbox. Talk to you soon!Oops! Something went wrong while submitting the form."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/assistants/",
    "chunk_id": 0,
    "content": "Redirecting...\n\n\n\n\n\n\nRedirecting..."
  },
  {
    "url": "https://www.langchain.com/resources-items/built-with-langgraph",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 0,
    "content": "Rakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients and employees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients and employees\n\nBy LangChain\n3 min read\nFeb 14, 2024"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 1,
    "content": "Rakuten Group is well known for operating one of the largest online shopping malls in Japan. The company has 70+ businesses in fields such as e-commerce, travel, digital content, fintech, communications and more.Adopting new technologies to push the frontiers of what‚Äôs possible is in the DNA of the company, and Rakuten Group has invested in delivering AI applications to better service their end users, clients and employees with a unified product suite.Rakuten AI for Business is a comprehensive AI platform that supports business clients in essential business operations, including market analysis and customer support. This platform also empowers clients to enhance productivity across various activities, such as sales, marketing, and IT support.LangChain and LangSmith are critical"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 2,
    "content": "empowers clients to enhance productivity across various activities, such as sales, marketing, and IT support.LangChain and LangSmith are critical technologies that the team leverages to build these solutions with greater speed and reliability.Rakuten AI: Empowering BusinessesRakuten invests heavily to make their ecosystem experience great for members and business clients. When clients onboard their businesses, they receive support from a dedicated onboarding consultant, and once live, continue to get help from them. The AI Team at Rakuten believes in using AI for empowerment, and they saw an opportunity to augment the assistance they give to their clients with new LLM-powered products built with LangChain and LangSmith. Specifically, their clients can benefit from:Rakuten AI Analyst which"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 3,
    "content": "their clients with new LLM-powered products built with LangChain and LangSmith. Specifically, their clients can benefit from:Rakuten AI Analyst which acts as a research assistant, providing valuable market intelligence. This helps clients get business insights backed by relevant data and charts.Rakuten AI Agent which supports clients in getting faster, self-serve customer support for their questions related to listing and transacting on the marketplace.Rakuten AI Librarian which summarizes all of the client‚Äôs documentation to answer questions from the client‚Äôs end customers and prospects in real time.¬†Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain‚Äôs OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 4,
    "content": "recently leveraged LangChain‚Äôs OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.¬†The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn‚Äôt want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain‚Äôs framework makes the ability to"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 5,
    "content": "& ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain‚Äôs framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what‚Äôs happening and why.General Manager of AI for Business at"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 6,
    "content": "they looked to LangSmith to harden their work and provide visibility into what‚Äôs happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, ‚ÄúLangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don‚Äôt. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we‚Äôre able to identify the right approaches for using LLMs"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 7,
    "content": "benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we‚Äôre able to identify the right approaches for using LLMs in an enterprise-setting faster.‚ÄùLangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten‚Äôs environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that‚Äôs par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 8,
    "content": "mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise."
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 9,
    "content": "Tags\nBy LangChainCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-rakuten/",
    "chunk_id": 10,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.youtube.com/@LangChain",
    "chunk_id": 0,
    "content": "LangChain - YouTubeAboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new features¬© 2025 Google LLC"
  },
  {
    "url": "https://lu.ma/langchain",
    "chunk_id": 0,
    "content": "LangChain Events ¬∑ Events Calendar¬†Explore EventsSign InSubscribeSubscribeLangChain EventsLooking to learn more about LangChain or meet other LangChain enthusiasts? Find LangChain-hosted and community-hosted events below ü•≥"
  },
  {
    "url": "https://lu.ma/langchain",
    "chunk_id": 1,
    "content": "If you have any questions, reach out to [email¬†protected].Submit EventOctober‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢UpcomingPastEventsSubmit EventYou have 0 events pending approval by the calendar admin.They will show up on the schedule once approved¬†LangChain x Zalion: Germany's first ever LangChain Meetup‚ÄãBy Zamal, Amada (LangChain) & MORITZ WEIMER‚ÄãM√ºnchen, Bayern‚ÄãCommunity-Hosted Event‚ÄãSold Out¬†LangChain Launch Week: San Francisco‚ÄãBy Matt D‚ÄãSan Francisco, California‚ÄãLangChain-Hosted Event¬†LangChain Launch Week: Boston‚ÄãBy Matt D‚ÄãBoston, Massachusetts‚ÄãLangChain-Hosted Event¬†AI Agent Night hosted by Dynatrace x LangChain‚ÄãBy Matt D, Christian & Kris Muhi‚ÄãBerlin, Berlin‚ÄãLangChain-Hosted"
  },
  {
    "url": "https://lu.ma/langchain",
    "chunk_id": 2,
    "content": "Massachusetts‚ÄãLangChain-Hosted Event¬†AI Agent Night hosted by Dynatrace x LangChain‚ÄãBy Matt D, Christian & Kris Muhi‚ÄãBerlin, Berlin‚ÄãLangChain-Hosted Event¬†LangChain Launch Week: New York City‚ÄãBy Matt D‚ÄãNew York, New York‚ÄãLangChain-Hosted Event¬†AI Demo-ish Day: A Vanta Community Show & Tell‚ÄãBy Vanta‚ÄãSan Francisco, California+81¬†AI Agents Meetup: LangChain + Rippling‚ÄãBy Matt D‚Äã501 2nd St suite 120‚ÄãLangChain-Hosted Event¬†Encuentro Comunidad LangChain - LATAM‚ÄãBy Ivan Castano & Amada (LangChain)‚ÄãZoom‚ÄãCommunity-Hosted Event+212"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 0,
    "content": "Definitive Guide to Testing LLM Applications - LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upThe Definitive Guide to Testing LLM ApplicationsEngineering teams building and testing LLM applications face unique challenges. The non-deterministic nature of LLMs makes it difficult to review natural language responses for style and accuracy, requiring robust testing with new success metrics.‚ÄçThis guide will help you add rigor to your testing process, so you can iterate faster without risking embarrassing or harmful regressions.In this guide, you‚Äôll learn:\n\nTips for testing across the product lifecycle\n\nMethods for building a dataset & defining testing metrics"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 2,
    "content": "Tips for testing across the product lifecycle\n\nMethods for building a dataset & defining testing metrics\n\nTemplates for evaluating RAG and agents, with visual examples Download your copyFirst Name*Last Name*Work Email** ¬†personal emails will not be accepted.Company Name*Job Title*"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 3,
    "content": "Thanks for your interest!PDF file has been sent to your email inbox.You can also open your copy of \"The Definitive Guide to Testing LLM Applications\" in your browser by clicking the button below.Open PDF in your browserOops! Something went wrong while submitting the form.Hear from our customersWalker WardStaff Software Engineer Architect\"LangSmith has made it easier than ever to curate and maintain high-signal LLM testing suites. With LangSmith, we‚Äôve seen a 43% performance increase over production systems, bolstering executive confidence to invest millions in new opportunities.\"Varadarajan SrinivasanVP of Data Science, AI & ML Engineering\"LangSmith has been instrumental in accelerating our AI adoption and enhancing our ability to identify and resolve issues that impact application"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 4,
    "content": "has been instrumental in accelerating our AI adoption and enhancing our ability to identify and resolve issues that impact application reliability. With LangSmith, we can also create custom feedback loops, improving our AI application accuracy by 40% and reducing deployment time by 50%.\"Padarn WilsonHead of Engineering, ML Platform\"Before LangSmith, we didn't have a systematic way to improve the quality of our LLM applications. By integrating LangSmith into our application framework, we now have a cohesive approach to benchmark prompts and models for 200+ applications. This supports our data-driven culture at Grab and allows us to drive continuous refinement of our LLM-powered solutions.\"What can you expect?Testing Guide EbookTake a peek at what's in our testing guideGet the eBook"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook#ebook-section",
    "chunk_id": 5,
    "content": "ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 0,
    "content": "Self-host LangSmith on Kubernetes - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmithSelf-host LangSmith on KubernetesGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewHosting LangSmithCloudOverviewCloud quickstartSetup guideCloud architecture and scalabilityHybridOverviewSetup guideSelf-hostedOverviewSetup guidesLangSmithInstall on KubernetesInstall on DockerWith deploymentStandalone serversManage an installationConfigurationConnect external servicesPlatform auth & access controlSelf-hosted observabilityScripts for management tasksOn this pagePrerequisitesDatabasesKubernetes"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 1,
    "content": "external servicesPlatform auth & access controlSelf-hosted observabilityScripts for management tasksOn this pagePrerequisitesDatabasesKubernetes cluster requirementsConfigure your Helm Charts:Deploying to Kubernetes:Validate your deployment:Using LangSmithSelf-hostedSetup guidesLangSmithSelf-host LangSmith on KubernetesCopy pageCopy pageSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment."
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 2,
    "content": "This page describes how to set up the LangSmith (observability, tracing, and evaluation) in a Kubernetes cluster. You‚Äôll use Helm to install LangSmith and its dependencies.\nThis guide installs the base LangSmith platform which includes observability and evaluation, but not the deployment management features. Review the self-hosted options if you‚Äôre unsure which you need.\nAfter completing this page, you‚Äôll have:"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 3,
    "content": "‚úÖ LangSmith UI and APIs: for observability, tracing, and evaluation.\n‚úÖ Backend services: (queue, playground, ACE).\n‚úÖ Datastores: (PostgreSQL, Redis, ClickHouse, optional blob storage).\n‚ùå Deployment management: To add deployment capabilities, complete this guide first, then follow Self-host LangSmith with deployment.\n\nWe‚Äôve successfully tested LangSmith on the following Kubernetes distributions:\n\nGoogle Kubernetes Engine (GKE)\nAmazon Elastic Kubernetes Service (EKS)\nAzure Kubernetes Service (AKS)\nOpenShift (4.14+)\nMinikube and Kind (for development purposes)"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 4,
    "content": "We have several Terraform modules the help in the provisioning of resources for LangSmith. You can find those in our public Terraform repo.Supported cloud providers include:\nAWS terraform modules\nAzure terraform modules\nYou can click on the links above to see the documentation for each module. These modules are designed to help you quickly set up the necessary infrastructure for LangSmith, including Kubernetes clusters, storage, and networking.\n‚ÄãPrerequisites\nEnsure you have the following tools/items ready. Some items are marked optional:\n\n\nLangSmith License Key\n\nYou can get this from your LangChain representative. Contact our sales team for more information.\n\n\n\nApi Key Salt"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 5,
    "content": "LangSmith License Key\n\nYou can get this from your LangChain representative. Contact our sales team for more information.\n\n\n\nApi Key Salt\n\nThis is a secret key that you can generate. It should be a random string of characters.\nYou can generate this using the following command:\n\nCopyopenssl rand -base64 32\n\n\n\nJWT Secret (Optional but used for basic auth)\n\nThis is a secret key that you can generate. It should be a random string of characters.\nYou can generate this using the following command:\n\nCopyopenssl rand -base64 32"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 6,
    "content": "Copyopenssl rand -base64 32\n\n\n\n‚ÄãDatabases\nLangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider‚Äôs managed services.\nFor more information, refer to the following setup guides for external services:\n\nPostgreSQL\nRedis\nClickHouse\n\n‚ÄãKubernetes cluster requirements\n\n\nYou will need a working Kubernetes cluster that you can access via kubectl. Your cluster should have the following minimum requirements:\n\n\nRecommended: At least 16 vCPUs, 64GB Memory available"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 7,
    "content": "Recommended: At least 16 vCPUs, 64GB Memory available\n\nYou may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found here.\nWe recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.\nWe recommend setting up the metrics server so that autoscaling can be turned on.\nIf you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory allocatable as ClickHouse will request this amount of resources by default.\n\n\n\nValid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 8,
    "content": "Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)\n\nTo enable persistence, we will try to provision volumes for any database running in-cluster.\nIf using PVs in your cluster, we highly recommend setting up backups in a production environment.\nWe strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.\nOn EKS, you may need to ensure you have the ebs-csi-driver installed and configured for dynamic provisioning. Refer to the EBS CSI Driver documentation for more information.\n\nYou can verify this by running:\nCopykubectl get storageclass"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 9,
    "content": "You can verify this by running:\nCopykubectl get storageclass\n\nThe output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:\nCopyNAME            PROVISIONER                 RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2 (default)   ebs.csi.eks.amazonaws.com   Delete          WaitForFirstConsumer   true                   161d\n\nWe highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.\nRefer to the Kubernetes documentation for more information on storage classes.\n\n\n\n\nHelm\n\nTo install helm refer to the Helm documentation"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 10,
    "content": "Helm\n\nTo install helm refer to the Helm documentation\n\n\n\nEgress to https://beacon.langchain.com (if not running in offline mode)\n\nLangSmith requires egress to https://beacon.langchain.com for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the Egress section.\n\n\n\n‚ÄãConfigure your Helm Charts:\n\n\nCreate a new file called langsmith_config.yaml with the configuration options from the previous step."
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 11,
    "content": "‚ÄãConfigure your Helm Charts:\n\n\nCreate a new file called langsmith_config.yaml with the configuration options from the previous step.\n\nThere are several configuration options that you can set in the langsmith_config.yaml file. You can find more information on specific configuration options in the Configuration section.\nIf you are new to Kubernetes or Helm, we‚Äôd recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: LangSmith helm chart examples.\nYou can see a full list of configuration options in the values.yaml file in the Helm Chart repository here: LangSmith Helm Chart"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 12,
    "content": "At a minimum, you will need to set the following configuration options (using basic auth):\nCopyconfig:\n  langsmithLicenseKey: \"<your license key>\"\n  apiKeySalt: \"<your api key salt>\"\n  authType: mixed\n  basicAuth:\n    enabled: true\n    initialOrgAdminEmail: \"admin@langchain.dev\" # Change this to your admin email address\n    initialOrgAdminPassword: \"secure-password\" # Must be at least 12 characters long and have at least one lowercase, uppercase, and symbol\n    jwtSecret: <your jwt salt> # A random string of characters used to sign JWT tokens for basic auth.\n\n\n\nYou will also need to specify connection details for any external databases you are using.\n‚ÄãDeploying to Kubernetes:"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 13,
    "content": "You will also need to specify connection details for any external databases you are using.\n‚ÄãDeploying to Kubernetes:\n\n\nVerify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)\n\n\nRun kubectl get pods\nOutput should look something like:\nCopykubectl get pods                                                                                                                                                                     ‚éà langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.\n\n\n\nIf you are using a namespace other than the default namespace, you will need to specify the namespace in the helm and kubectl commands by using the -n <namespace> flag."
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 14,
    "content": "Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts)\nCopyhelm repo add langchain https://langchain-ai.github.io/helm/\"langchain\" has been added to your repositories\n\n\n\nFind the latest version of the chart. You can find the available versions in the Helm Chart repository.\n\nWe generally recommend using the latest version.\nYou can also run helm search repo langchain/langsmith --versions to see the available versions. The output will look something like this:"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 15,
    "content": "Copylangchain/langsmith     0.10.14         0.10.32         Helm chart to deploy the langsmith application ...\nlangchain/langsmith     0.10.13         0.10.32         Helm chart to deploy the langsmith application ...\nlangchain/langsmith     0.10.12         0.10.32         Helm chart to deploy the langsmith application ...\nlangchain/langsmith     0.10.11         0.10.29         Helm chart to deploy the langsmith application ...\nlangchain/langsmith     0.10.10         0.10.29         Helm chart to deploy the langsmith application ...\n\n\n\nRun helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 16,
    "content": "Run helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug\n\nReplace <namespace> with the namespace you want to deploy LangSmith to.\nReplace <version> with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.\n\nOnce the helm install command runs and finishes successfully, you should see output similar to this:\nCopyNAME: langsmith\nLAST DEPLOYED: Fri Sep 17 21:08:47 2021\nNAMESPACE: langsmith\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\nThis may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services."
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 17,
    "content": "Run kubectl get pods Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):\nCopylangsmith-backend-6ff46c99c4-wz22d       1/1     Running   0          3h2m\nlangsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0          3h2m\nlangsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0          3h2m\nlangsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2m\nlangsmith-postgres-0                     1/1     Running   0          9h\nlangsmith-queue-5898b9d566-tv6q8         1/1     Running   0          3h2m\nlangsmith-redis-0                        1/1     Running   0          9h\n\n\n\n‚ÄãValidate your deployment:"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 18,
    "content": "Run kubectl get services\nOutput should look something like:\nCopyNAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE\nlangsmith-backend       ClusterIP      172.20.140.77    <none>                                                                    1984/TCP       35h\nlangsmith-frontend      LoadBalancer   172.20.253.251   <external ip>                                                             80:31591/TCP   35h\nlangsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35h"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 19,
    "content": "langsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35h\nlangsmith-playground    ClusterIP      172.20.153.194   <none>                                                                    3001/TCP       9h\nlangsmith-postgres      ClusterIP      172.20.244.82    <none>                                                                    5432/TCP       35h\nlangsmith-redis         ClusterIP      172.20.81.217    <none>                                                                    6379/TCP       35h"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 20,
    "content": "Curl the external ip of the langsmith-frontend service:\nCopycurl <external ip>/api/tenants\n\nExpected output:\nCopy[{\"id\":\"00000000-0000-0000-0000-000000000000\",\"has_waitlist_access\":true,\"created_at\":\"2023-09-13T18:25:10.488407\",\"display_name\":\"Personal\",\"config\":{\"is_personal\":true,\"max_identities\":1},\"tenant_handle\":\"default\"}]\n\n\n\nVisit the external ip for the langsmith-frontend service on your browser\nThe LangSmith UI should be visible/operational"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 21,
    "content": "Visit the external ip for the langsmith-frontend service on your browser\nThe LangSmith UI should be visible/operational\n\n\n\n‚ÄãUsing LangSmith\nNow that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide.\nYour LangSmith instance is now running but may not be fully setup yet.\nIf you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the langsmith_config.yaml file.\nAs a next step, it is strongly recommended you work with your infrastructure administrators to:"
  },
  {
    "url": "https://docs.smith.langchain.com/self_hosting/kubernetes",
    "chunk_id": 22,
    "content": "Setup DNS for your LangSmith instance to enable easier access\nConfigure SSL to ensure in-transit encryption of traces submitted to LangSmith\nConfigure LangSmith with Single Sign-On to secure your LangSmith instance\nConnect LangSmith to external Postgres and Redis instances\nSet up Blob Storage for storing large files\n\nReview our configuration section for more information on how to configure these options.\n\nEdit the source of this page on GitHubWas this page helpful?YesNoSelf-hosted LangSmithPreviousSelf-host LangSmith with DockerNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 0,
    "content": "LangSmith - Observability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangSmith ObservabilityKnow what your agents are really doingLangSmith Observability gives you complete visibility into agent behavior with tracing, real-time monitoring, alerting, and high-level insights into usage.Sign UpGet a demoHelping top teams ship reliable agentsFind failures fast with agent tracingQuickly debug and understand non-deterministic LLM app behavior with tracing. See what your agent is doing step by step ‚Äîthen fix issues to improve latency and response quality.Get started tracing your app"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 2,
    "content": "Monitor what matters ‚Ä®to the businessTrack business-critical metrics like costs, latency, and response quality with live dashboards. Get alerts when issues happen and drill into the root cause.See how to create a custom dashboard\n\n\nDiscover usage patterns and issues automaticallySee clusters of similar conversations to understand what users actually want and quickly find all instances of similar problems to address systemic issues.Learn more about Insights"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 3,
    "content": "Ready to get visibility into your agents?LangSmith works with any framework. If you‚Äôre already using LangChain or LangGraph, just set one environment variable to get started with tracing your AI application.Sign up for freeBook a demoResources for LangSmith ObservabilitywebinarGet started with LangSmith TracingDOCSLangSmith Observability conceptsblogLangSmith OTel supportFAQs for LangSmith ObservabilityWhat frameworks and libraries does LangSmith work with?\n\nLangSmith works with any framework. You can use it with or without our open source frameworks LangChain and LangGraph. Learn more.Does LangSmith support OTel?"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 4,
    "content": "Yes, LangSmith supports with OTel to unify your observability stack across services. Your application does not need to be written in Python or Typescript. See the docs.Can I use LangSmith Observability without LangSmith Evaluation?\n\nYes. You can use LangSmith Observability with or without Evaluation. For all plan types, you'll get access to both and only pay for what you use.I can‚Äôt have data leave my environment. Can I self-host LangSmith?\n\nYes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.Where is my data stored?"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 5,
    "content": "When using LangSmith hosted at smith.langchain.com, data is stored in GCP us-central-1. If you‚Äôre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment. For more information, check out our documentation.Will LangSmith add latency to my application?\n\nNo, LangSmith does not add any latency to your application. In the LangSmith SDK, there‚Äôs a callback handler that sends traces to a LangSmith trace collector which runs as an async, distributed process. Additionally, if LangSmith experiences an incident, your application performance will not be disrupted.Will you train on the data that I send LangSmith?"
  },
  {
    "url": "https://www.langchain.com/langsmith",
    "chunk_id": 6,
    "content": "We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information.How much does LangSmith cost?\n\nSee our pricing page for more information, and find a plan that works for you.ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 0,
    "content": "LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Skip to content\n        \n\n\n\n\n\n\n\n            \n            \nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            LangGraph\n          \n\n\n\n            \n              LangGraph\n            \n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n\n\n          \n  \n  \n    \n  \n  Get started\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Guides\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Reference\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Examples\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Additional resources\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    LangGraph\n  \n\n\n\n\n\n\n    GitHub"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 1,
    "content": "Additional resources\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    LangGraph\n  \n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n    Get started\n    \n  \n\n\n\n\n\n\n\n\n            Get started\n          \n\n\n\n\n\n    Quickstarts\n    \n  \n\n\n\n\n\n            Quickstarts\n          \n\n\n\n\n    Start with a prebuilt agent\n    \n  \n\n\n\n\n\n    Build a custom workflow\n    \n  \n\n\n\n\n\n\n    Run a local server\n    \n  \n\n\n\n\n\n\n\n\n\n    General concepts\n    \n  \n\n\n\n\n\n            General concepts\n          \n\n\n\n\n    Workflows & agents\n    \n  \n\n\n\n\n\n    Agent architectures\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n    Guides\n    \n  \n\n\n\n\n\n\n    Reference\n    \n  \n\n\n\n\n\n\n    Examples\n    \n  \n\n\n\n\n\n\n    Additional resources\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Get started\n    \n\n\n\n\n\n      Core benefits"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 2,
    "content": "Additional resources\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Get started\n    \n\n\n\n\n\n      Core benefits\n    \n\n\n\n\n\n      LangGraph‚Äôs ecosystem\n    \n\n\n\n\n\n      Additional resources\n    \n\n\n\n\n\n      Acknowledgements\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrusted by companies shaping the future of agents ‚Äì including Klarna, Replit, Elastic, and more ‚Äì LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\nGet started¬∂\nInstall LangGraph:\npip install -U langgraph\n\nThen, create an agent using prebuilt components:\nAPI Reference: create_react_agent\n# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langgraph.prebuilt import create_react_agent"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 3,
    "content": "from langgraph.prebuilt import create_react_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    prompt=\"You are a helpful assistant\"\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 4,
    "content": "# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n\nFor more information, see the Quickstart. Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials.\nCore benefits¬∂\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 5,
    "content": "Durable execution: Build agents that persist through failures and can run for extended periods, automatically resuming from exactly where they left off.\nHuman-in-the-loop: Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\nComprehensive memory: Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions.\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 6,
    "content": "Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 7,
    "content": "LangGraph‚Äôs ecosystem¬∂\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 8,
    "content": "LangSmith ‚Äî Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\nLangSmith Deployment ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in LangGraph Studio.\nLangChain ‚Äì Provides integrations and composable components to streamline LLM application development.\n\n\nNote\nLooking for the JS version of LangGraph? See the JS repo and the JS docs.\n\nAdditional resources¬∂"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 9,
    "content": "Additional resources¬∂\n\nGuides: Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.).\nReference: Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\nExamples: Guided examples on getting started with LangGraph.\nLangChain Forum: Connect with the community and share all of your technical questions, ideas, and feedback.\nLangChain Academy: Learn the basics of LangGraph in our free, structured course.\nTemplates: Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted.\nCase studies: Hear how industry leaders use LangGraph to ship AI applications at scale."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/",
    "chunk_id": 10,
    "content": "Acknowledgements¬∂\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n                Next\n              \n\n                Start with a prebuilt agent\n              \n\n\n\n\n\n\n\n\n\n\n      Copyright ¬© 2025 LangChain, Inc | Consent Preferences\n\n  \n  \n    Made with\n    \n      Material for MkDocs"
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 0,
    "content": "LangSmith docs - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith docsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APILangGraph Server APIControl Plane API for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQLangSmith docsCopy pageCopy"
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 1,
    "content": "DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQLangSmith docsCopy pageCopy pageLangSmith provides tools for developing, debugging, and deploying LLM applications."
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 2,
    "content": "It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\nLangSmith is framework agnostic, so you can use it with or without LangChain‚Äôs open-source libraries\nlangchain and langgraph.\nPrototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n‚ÄãGet started\nCreate an accountSign up at smith.langchain.com (no credit card required).\nYou can log in with Google, GitHub, or email.Create an API keyGo to your Settings page ‚Üí API Keys ‚Üí Create API Key.\nCopy the key and save it securely.\nOnce your account and API key are ready, choose a quickstart to begin building with LangSmith:"
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 3,
    "content": "ObservabilityGain visibility into every step your application takes to debug faster and improve reliability.Start tracingEvaluationMeasure and track quality over time to ensure your AI applications are consistent and trustworthy.Evaluate your appDeploymentDeploy your agents as LangGraph Servers, ready to scale in production.Deploy your agentsPrompt TestingIterate on prompts with built-in versioning and collaboration to ship improvements faster.Test your promptsStudioUse a visual interface to design, test, and refine applications end-to-end.Develop with StudioHostingHost LangSmith in the cloud, in your environment, or hybrid to match your infrastructure and compliance needs.Choose your hosting mode\n‚ÄãWorkflow"
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 4,
    "content": "‚ÄãWorkflow\nLangSmith combines observability, evaluation, deployment, and hosting in one integrated workflow‚Äîfrom local development to production."
  },
  {
    "url": "https://docs.smith.langchain.com/",
    "chunk_id": 5,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoPricing plansNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 0,
    "content": "LangChain State of AI 2024 Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain State of AI 2024 Report\nDive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving. \n\nBy LangChain\n6 min read\nDec 19, 2024"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 1,
    "content": "Another year of building with LLMs is coming to an end ‚Äî¬†and 2024 didn‚Äôt disappoint. With nearly 30k users signing up for LangSmith every month, we‚Äôre lucky to have front row seats to what‚Äôs happening in the industry.¬†As we did last year, we want to share some product usage patterns that showcase how the AI¬†ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, we‚Äôve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows.¬†Dive into the stats below to learn exactly what developers are building, testing, and prioritizing.Infrastructure usageWith Large Language Models"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 2,
    "content": "into the stats below to learn exactly what developers are building, testing, and prioritizing.Infrastructure usageWith Large Language Models (LLMs) eating the world, everyone‚Äôs asking the mirror-mirror-on-the-wall question: ‚ÄúWhich model is the most utilized of them all?‚Äù Let‚Äôs unpack what we‚Äôve seen.Top LLM providersLike last year‚Äôs results, OpenAI reigns as the most used LLM provider among LangSmith users ‚Äî¬†used more than 6x as much as Ollama, the next-most popular provider (counted by LangSmith organization usage).Interestingly, Ollama and Groq (which both allow users to run open source models, with the former focusing on local execution and the latter on cloud deployment) have accelerated in momentum this year, breaking into the top 5. This shows a growing interest in more flexible"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 3,
    "content": "and the latter on cloud deployment) have accelerated in momentum this year, breaking into the top 5. This shows a growing interest in more flexible deployment options and customizable AI infrastructure.When it comes to providers that offer open-source models, the top providers have stayed relatively consistent compared to last year - Ollama, Mistral, and Hugging Face have made it easy for developers to run open source models on their platforms. These OSS providers‚Äô collective usage represents 20% of the top 20 LLM providers (by the number of organizations using them).¬†Top Retrievers / Vector StoresPerforming retrieval is still critical for many GenAI workflows. The top 3 vector stores have remained the same as last year, with Chroma and FAISS as the most popular choices. This year,"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 4,
    "content": "for many GenAI workflows. The top 3 vector stores have remained the same as last year, with Chroma and FAISS as the most popular choices. This year, Milvus, MongoDB, and Elastic‚Äôs vector databases have also entered the top 10.¬†Building with LangChain productsAs developers have gained more experience utilizing generative AI, they are also building more dynamic applications. From the growing sophistication of workflows, to the rise of AI agents ‚Äî we‚Äôre seeing a few trends that point to an evolving ecosystem of innovation.Observability isn‚Äôt limited to LangChain applicationsWhile langchain (our open source framework) is central to many folks‚Äô LLM app development journeys, 15.7% of LangSmith traces this year come from non-langchain frameworks. This reflects a broader trend where observability"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 5,
    "content": "app development journeys, 15.7% of LangSmith traces this year come from non-langchain frameworks. This reflects a broader trend where observability is needed regardless of what framework you‚Äôre using to build the LLM app ‚Äî and that interoperability is supported by LangSmith.Python remains dominant, while JavaScript usage growsDebugging, testing, and monitoring certainly has a special place in our Python developers‚Äô hearts, with 84.7% usage coming from the Python SDK. But there is a notable and growing interest in JavaScript as developers pursue web-first applications ‚Äî¬†the JavaScript SDK accounts for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.¬†AI agents are gaining tractionAs companies are getting more serious about incorporating AI agents across"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 6,
    "content": "increasing 3x compared to the previous year.¬†AI agents are gaining tractionAs companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction ‚Äî¬†with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.This growth aligns with the rise in agentic behavior: we see that on average 21.9% of traces now involve tool calls, up from an average of 0.5% in 2023. Tool calling allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action."
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 7,
    "content": "allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action. Increased use of tool calling can enhance an agent‚Äôs ability to interact with external systems and perform tasks like writing to databases.¬†Performance and optimizationBalancing speed and sophistication is a key challenge when developing applications ‚Äî especially those leveraging LLM resources. Below, we explore how organizations are interacting with their applications to align the complexity of their needs with efficient performance.Complexity is growing, but tasks are being handled efficiently¬†¬†The average number of steps per trace has more than doubled over the past year, rising from on average 2.8 steps (2023) to 7.7 steps (2024). We"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 8,
    "content": "average number of steps per trace has more than doubled over the past year, rising from on average 2.8 steps (2023) to 7.7 steps (2024). We define a step as a distinct operation within a trace, such as a call to an LLM, retriever, or tool. This growth in steps signals that organizations are leveraging more complex and multi-faceted workflows. Rather than a simple question-answer interaction, users are building systems that chain together multiple tasks, such as retrieving information, processing it, and generating actionable results.In contrast, the average number of LLM calls per trace has grown more modestly‚Äî from on average 1.1 to 1.4 LLM calls. This speaks to how developers are designing systems to achieve more with fewer LLM calls, balancing functionality while keeping expensive LLM"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 9,
    "content": "calls. This speaks to how developers are designing systems to achieve more with fewer LLM calls, balancing functionality while keeping expensive LLM requests in checkLLM testing & evaluationWhat are organizations doing to test their LLM applications to guard against inaccurate or low-quality LLM-generated responses? While it‚Äôs no easy feat to keep the quality of your LLM app high, we see organizations using LangSmith‚Äôs evaluation capabilities to automate testing and generate user feedback loops to create more robust, reliable applications.LLM-as-Judge: Evaluating what mattersLLM-as-Judge evaluators capture grading rules into an LLM prompt and use the LLM to score whether the output adheres to specific criteria. We see developers testing for these characteristics the most: Relevance,"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 10,
    "content": "and use the LLM to score whether the output adheres to specific criteria. We see developers testing for these characteristics the most: Relevance, Correctness, Exact Match, and HelpfulnessThese highlight that most developers are doing coarse checks for response quality to make sure AI generated outputs don‚Äôt completely miss the mark.¬†Iterating with human feedback¬†Human feedback is a key part of the iteration loop for folks building LLM apps. LangSmith speeds up the process of collecting and incorporating human feedback on traces and runs (i.e. spans) ‚Äì so that users can create rich datasets for improvement and optimization. Over the past year, annotated runs grew 18x, scaling linearly with growth in LangSmith usage.Feedback volume per run also increased slightly, rising from 2.28 to 2.59"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 11,
    "content": "annotated runs grew 18x, scaling linearly with growth in LangSmith usage.Feedback volume per run also increased slightly, rising from 2.28 to 2.59 feedback entries per run. Still, feedback is relatively sparse per run. Users may be prioritizing speed in reviewing runs over providing comprehensive feedback, or commenting on only the most critical or problematic runs that need attention.¬†ConclusionIn 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation. As more LLM apps are created, we‚Äôre excited to see how folks dig into smarter workflows, better performance, and stronger reliability.¬†Learn more here about how LangSmith can bring more"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 12,
    "content": "excited to see how folks dig into smarter workflows, better performance, and stronger reliability.¬†Learn more here about how LangSmith can bring more visibility into your LLM app development and improve performance over time ‚Äî¬†from debugging bottlenecks to evaluating response quality to monitoring regressions."
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 13,
    "content": "Tags\nBy LangChain\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\n\nBy LangChain\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing OpenTelemetry support for LangSmith\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nEasier evaluations with LangSmith SDK v0.2\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph Platform in beta: New deployment options for scalable agent infrastructure\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nFew-shot prompting to improve tool-calling performance\n\n\nBy LangChain\n8 min read"
  },
  {
    "url": "https://blog.langchain.dev/langchain-state-of-ai-2024/",
    "chunk_id": 14,
    "content": "By LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nFew-shot prompting to improve tool-calling performance\n\n\nBy LangChain\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nImproving core tool interfaces and docs in LangChain\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 0,
    "content": "Prompt engineering quickstart - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOn this pagePrerequisitesNext stepsVideo guidePrompt engineering quickstartCopy pageCopy pagePrompts guide the behavior of"
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 1,
    "content": "multi-turn conversationsOn this pagePrerequisitesNext stepsVideo guidePrompt engineering quickstartCopy pageCopy pagePrompts guide the behavior of large language models (LLM). Prompt engineering is the process of crafting, testing, and refining the instructions you give to an LLM so it produces reliable and useful responses."
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 2,
    "content": "LangSmith provides tools to create, version, test, and collaborate on prompts. You‚Äôll also encounter common concepts like prompt templates, which let you reuse structured prompts, and variables, which allow you to dynamically insert values (such as a user‚Äôs question) into a prompt.\nIn this quickstart, you‚Äôll create, test, and improve prompts using either the UI or the SDK. This quickstart will use OpenAI as the example LLM provider, but the same workflow applies across other providers.\nIf you prefer to watch a video on getting started with prompt engineering, refer to the quickstart Video guide.\n‚ÄãPrerequisites\nBefore you begin, make sure you have:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 3,
    "content": "A LangSmith account: Sign up or log in at smith.langchain.com.\nA LangSmith API key: Follow the Create an API key guide.\nAn OpenAI API key: Generate this from the OpenAI dashboard."
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 4,
    "content": "Select the tab for UI or SDK workflows:\n UI SDK‚Äã1. Set workspace secretIn the LangSmith UI, ensure that your OpenAI API key is set as a workspace secret.\nNavigate to  Settings and then move to the Secrets tab.\nSelect Add secret and enter the OPENAI_API_KEY and your API key as the Value.\nSelect Save secret.\n When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.‚Äã2. Create a prompt\nIn the LangSmith UI, navigate to the Prompts section in the left-hand menu.\nClick on + Prompt to create a prompt.\nModify the prompt by editing or adding prompts and input variables as needed.\n‚Äã3. Test a prompt"
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 5,
    "content": "Under the Prompts heading select the gear  icon next to the model name, which will launch the Prompt Settings window on the Model Configuration tab.\n\n\nSet the model configuration you want to use. The Provider and Model you select will determine the parameters that are configurable on this configuration page. Once set, click Save as.\n\n\n\nSpecify the input variables you would like to test in the Inputs box and then click  Start.\n\nTo learn about more options for configuring your prompt in the Playground, refer to Configure prompt settings.\n\n\nAfter testing and refining your prompt, click Save to store it for future use."
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 6,
    "content": "After testing and refining your prompt, click Save to store it for future use.\n\n‚Äã4. Iterate on a promptLangSmith allows for team-based prompt iteration. Workspace members can experiment with prompts in the playground and save their changes as a new commit when ready.To improve your prompts:\n\nReference the documentation provided by your model provider for best practices in prompt creation, such as:\n\nBest practices for prompt engineering with the OpenAI API\nGemini‚Äôs Introduction to prompt design\n\n\n\nBuild and refine your prompts with the Prompt Canvas‚Äîan interactive tool in LangSmith. Learn more in the Prompt Canvas guide.\n\n\nTag specific commits to mark important moments in your commit history."
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 7,
    "content": "Tag specific commits to mark important moments in your commit history.\n\nTo create a commit, navigate to the Playground and select Commit. Choose the prompt to commit changes to and then Commit.\nNavigate to Prompts in the left-hand menu. Select the prompt. Once on the prompt‚Äôs detail page, move to the Commits tab. Find the tag icon  to Add a Commit Tag.\n\n\n\n\n‚ÄãNext steps\n\nLearn more about how to store and manage prompts using the Prompt Hub in the Create a prompt guide.\nLearn how to set up the Playground to Test multi-turn conversations in this tutorial.\nLearn how to test your prompt‚Äôs performance over a dataset instead of individual examples, refer to Run an evaluation from the Prompt Playground.\n\n‚ÄãVideo guide"
  },
  {
    "url": "https://docs.langchain.com/langsmith/prompt-engineering-quickstart",
    "chunk_id": 8,
    "content": "‚ÄãVideo guide\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoPrompt engineeringPreviousPrompt Engineering ConceptsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#introduction",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 0,
    "content": "Retrieval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upContextualize your LLM¬†AppRetrieval Augmented Generation (RAG) with LangChain connects your company data to the power of LLMs.Get started with PythonGet started with JavaScript"
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 1,
    "content": "With LangChain‚Äôs built-in ingestion and retrieval methods, developers can augment the LLM‚Äôs knowledge with company or user data.150+001Document Loaders60+002Vector Stores50+003Embedding Models40+004RetrieversA complete set ‚Ä®of RAG building blocksBuild best-in-class RAG systems with LangChain's comprehensive integrations, state-of-the-art techniques, and¬†infinite composability.See Integrations\n\n\nThe data connections and infrastructure you need¬†for your retrieval use-caseLangChain offers an extensive library of off-the-shelf tools‚Ä®and an intuitive framework for customizing your own.Document loaders ‚Ä®for¬†any¬†type of data.PDFSQLMARKDOWNHTMLTEXTSEARCHCODEAPIDOCUMENTSJSONPDFSQLMARKDOWNHTMLTEXTSEARCHCODEAPIDOCUMENTSJSONRetrieval algorithms that provide greater¬†precision and results"
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 2,
    "content": "Self Query RetrieverThis retriever inspects the natural language query and writes a structured query to run on the underlying VectorStore.\n\n\n\nContextual CompressionCompress the retrieved document using the context of the query, so that only the relevant information ‚Ä®in the source is returned.\n\n\n\nMulti Vector RetrieverThis retriever lets you query across multiple stored vectors per document, including ones on¬†smaller chunks, summaries, and hypothetical questions.\n\n\n\nMulti Vector RetrieverThis retriever lets you query across multiple stored vectors per document, including ones on¬†smaller chunks, summaries, and hypothetical questions.\n\n\n\nTime-Weighted Vector StoreCombine semantic similarity with ‚Ä®a time decay to factor in recency ‚Ä®in your retrieval.Go to docs"
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 3,
    "content": "Time-Weighted Vector StoreCombine semantic similarity with ‚Ä®a time decay to factor in recency ‚Ä®in your retrieval.Go to docs\n\n\n\n\n\n\nParent Document RetrieverEmbed small chunks, which are better for similarity search, but retrieve larger chunks, which help with generation.Go to docs\n\n\n\n\n\n\nSelf Query RetrieverThis retriever inspects the natural language query and writes a structured query to run on the underlying VectorStore.\n\n\n\nContextual CompressionCompress the retrieved document using the context of the query, so that only the relevant information ‚Ä®in the source is returned.\n\n\n\nMulti Vector RetrieverThis retriever lets you query across multiple stored vectors per document, including ones on¬†smaller chunks, summaries, and hypothetical questions."
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 4,
    "content": "Multi Vector RetrieverThis retriever lets you query across multiple stored vectors per document, including ones on¬†smaller chunks, summaries, and hypothetical questions.\n\n\n\nTime-Weighted Vector StoreCombine semantic similarity with ‚Ä®a time decay to factor in recency ‚Ä®in your retrieval.Go to docs\n\n\n\n\n\n\nParent Document RetrieverEmbed small chunks, which are better for similarity search, but retrieve larger chunks, which help with generation.Go to docs\n\n\n\n\n\nMinimize writing duplicated contentAvoid re-writing unchanged contentNever recompute embeddings over unchanged contentIngestion done rightThe LangChain Indexing API syncs your data from ‚Ä®any¬†source into a vector store, helping you save money and¬†time."
  },
  {
    "url": "https://www.langchain.com/retrieval",
    "chunk_id": 5,
    "content": "Resources for LangSmith EvaluationTUTORIALRAG Video SeriesblogDeconstructing RAGDEMOBuilding a Research Assistant from ScratchReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEngineer reliable agentsShip agents to production with LangChain's comprehensive platform for agent engineering.Request a demoSign Up"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 1,
    "content": "LangChain products power top engineering teams, from AI startups to global enterprisesVisibility &¬†controlSee exactly what's happening at every step of your agent. Steer your agent to accomplish critical tasks the way you intended.Fast iterationRapidly move through build, test, deploy, learn, repeat with workflows across the entire agent engineering lifecycle.Durable performanceShip at scale with agent infrastructure designed for long-running workloads and human oversight.Model neutralSwap models, tools, and databases without rewriting your app. Future-proof your stack as AI advances with no vendor lock-in.Your agent engineering stackOpen Source FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 2,
    "content": "FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph puts you in control with low-level primitives to build custom agent workflows.Agent Engineering PlatformLangSmithObservabilityEvaluationDeploymentObservabilityEvaluationDeploymentSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 3,
    "content": "Improve agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 4,
    "content": "Build agents your way, with templates or custom controlBring your own frameworkLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK¬†to gain visibility into your agent interactions, whether you use LangChain's frameworks or not.Open Source Frameworks¬†Bring your ownAgent Engineering PlatformLangSmithObservabilityEvaluationDeploymentOpen Source FrameworksBuild agents your way, with templates or custom controlLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraph puts you in control with low-level primitives to build custom agent workflows.LangSmith Agent Engineering PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 5,
    "content": "PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 6,
    "content": "LangSmith Agent Engineering PlatformImprove agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nLangSmith Agent Engineering PlatformDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 7,
    "content": "CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access to information and tools in a compliant manner so they can perform their best.Customer SupportImprove the speed and efficiency of support teams that handle customer requests.ResearchSynthesize data, summarize sources, and uncover insights faster for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 8,
    "content": "Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\n\n\nFinancial ServicesKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\n\n\nB2B SaaSElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\n\n\nAI/MLReplit's AI Agent serves 30+ million developers. Their AI engineers rely on LangSmith to debug complex traces."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-0",
    "chunk_id": 9,
    "content": "Learn alongside the 1 million+ practitioners who are pushing the industry forward90MMonthly downloads100k+GitHub stars#1Downloaded agent framework1000IntegrationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#problem",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 0,
    "content": "LangSmith Deployment - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith DeploymentGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom"
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 1,
    "content": "assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesLangSmith DeploymentCopy pageCopy pageStart here if you‚Äôre building or operating agent applications. This section is about deploying your application. Need to set up LangSmith infrastructure first? Refer to the Hosting section."
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 2,
    "content": "This section covers how to package, build, and deploy your agents and applications as LangGraph Servers.\nA typical deployment workflow consists of the following steps:"
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 3,
    "content": "Test locally: Run your application on a local server.\nChoose hosting: Select Cloud, Hybrid, or Self-hosted.\nDeploy your app: Push code or build images to your chosen environment.\nMonitor & manage: Track traces, alerts, and dashboards.\n\nPrerequisites: Before deploying applications, you need a LangSmith instance to deploy to. Choose a hosting option first:\nCloud: Fully managed\nHybrid: Enterprise option for data residency requirements\nSelf-hosted: Full control and data isolation\n\n‚ÄãWhat you‚Äôll learn"
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 4,
    "content": "‚ÄãWhat you‚Äôll learn\n\nConfigure your app for deployment (dependencies, project setup, and monorepo support).\nBuild, deploy, and update LangGraph Servers.\nSecure your deployments with authentication and access control.\nCustomize your server runtime (lifespan hooks, middleware, and routes).\nDebug, observe, and troubleshoot deployed agents using the Studio UI.\n\nGet started with deploymentPackage, build, and deploy your agents and graphs to LangGraph Server.Configure your app\n‚ÄãRelated\n\nLangGraph Server\nApplication structure\nLocal server testing"
  },
  {
    "url": "https://docs.langchain.com/langgraph-platform/index#quickstarts",
    "chunk_id": 5,
    "content": "LangGraph Server\nApplication structure\nLocal server testing\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoRun a LangGraph app locallyNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://docs.langchain.com/",
    "chunk_id": 0,
    "content": "Home - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageHomeSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationDocumentationLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChain‚Äôs products to engineer reliable agents.Our open source frameworks help you build agents:\nLangChain helps you quickly get started building agents, with any model provider of your choice.\nLangGraph allows you to control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support. You can manage long-running tasks with durable execution."
  },
  {
    "url": "https://docs.langchain.com/",
    "chunk_id": 1,
    "content": "LangSmith is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:\nObservability to see exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.\nEvaluation to test and score agent behavior on production data and offline datasets for continuous improvement.\nDeployment to ship your agent in one click, using scalable infrastructure built for long-running tasks."
  },
  {
    "url": "https://docs.langchain.com/",
    "chunk_id": 2,
    "content": "Get startedBuild your first agentGet startedSign up for LangSmithTry LangSmithSend your first traceGet startedBuild an advanced agentGet startedOpen source agent frameworks Python TypeScriptLangChain (Python)Quickly get started building agents, with any model provider of your choice.Learn moreLangGraph (Python)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreLangSmithObservabilitySee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.Learn moreEvaluationTest and score agent behavior on production data or offline datasets to continuously improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn"
  },
  {
    "url": "https://docs.langchain.com/",
    "chunk_id": 3,
    "content": "improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn moreDeploymentShip your agent in one click, using scalable infrastructure built for long-running tasks.Learn more"
  },
  {
    "url": "https://docs.langchain.com/",
    "chunk_id": 4,
    "content": "Edit the source of this page on GitHub"
  },
  {
    "url": "http://docs.langchain.com/",
    "chunk_id": 0,
    "content": "Home - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageHomeSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationDocumentationLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChain‚Äôs products to engineer reliable agents.Our open source frameworks help you build agents:\nLangChain helps you quickly get started building agents, with any model provider of your choice.\nLangGraph allows you to control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support. You can manage long-running tasks with durable execution."
  },
  {
    "url": "http://docs.langchain.com/",
    "chunk_id": 1,
    "content": "LangSmith is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:\nObservability to see exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.\nEvaluation to test and score agent behavior on production data and offline datasets for continuous improvement.\nDeployment to ship your agent in one click, using scalable infrastructure built for long-running tasks."
  },
  {
    "url": "http://docs.langchain.com/",
    "chunk_id": 2,
    "content": "Get startedBuild your first agentGet startedSign up for LangSmithTry LangSmithSend your first traceGet startedBuild an advanced agentGet startedOpen source agent frameworks Python TypeScriptLangChain (Python)Quickly get started building agents, with any model provider of your choice.Learn moreLangGraph (Python)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreLangSmithObservabilitySee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.Learn moreEvaluationTest and score agent behavior on production data or offline datasets to continuously improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn"
  },
  {
    "url": "http://docs.langchain.com/",
    "chunk_id": 3,
    "content": "improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn moreDeploymentShip your agent in one click, using scalable infrastructure built for long-running tasks.Learn more"
  },
  {
    "url": "http://docs.langchain.com/",
    "chunk_id": 4,
    "content": "Edit the source of this page on GitHub"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#cognitive-architecture",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/time_weighted_vectorstore",
    "chunk_id": 0,
    "content": "Page Not Found | ü¶úÔ∏èüîó LangChain\n\n\n\n\n\n\n\n\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchPage Not FoundWe could not find what you were looking for.Please contact the owner of the site that linked you to the original URL and let them know their link is broken.CommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 0,
    "content": "Observability concepts - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationObservability conceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback &"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 1,
    "content": "trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pageRunsTracesProjectsFeedbackTagsMetadataData storage and retentionDeleting traces from LangSmithObservability conceptsCopy pageCopy pageThis page covers key concepts that are important to understand when logging traces to LangSmith."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 2,
    "content": "A trace records the sequence of steps your application takes‚Äîfrom receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a run. Multiple traces are grouped together within a project.\nThe following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 3,
    "content": "‚ÄãRuns\nA run is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span.\n\n‚ÄãTraces\nA trace is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 4,
    "content": "‚ÄãProjects\nA project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n\n‚ÄãFeedback\nFeedback allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.\nYou can collect feedback on runs in a number of ways:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 5,
    "content": "Sent up along with a trace from the LLM application.\nGenerated by a user in the app inline or in an annotation queue.\nGenerated by an automatic evaluator during offline evaluation.\nGenerated by an online evaluator.\n\nTo learn more about how feedback is stored in the application, refer to the Feedback data format guide.\n\n‚ÄãTags\nTags are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:\n\nCategorize runs for easier search.\nFilter runs.\nGroup runs together for analysis.\n\nLearn how to attach tags to your traces."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 6,
    "content": "Categorize runs for easier search.\nFilter runs.\nGroup runs together for analysis.\n\nLearn how to attach tags to your traces.\n\n‚ÄãMetadata\nMetadata is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.\nLearn how to add metadata to your traces."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 7,
    "content": "‚ÄãData storage and retention\nFor traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.\nAfter 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.\nIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\n‚ÄãDeleting traces from LangSmith"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 8,
    "content": "‚ÄãDeleting traces from LangSmith\nIf you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.\nYou can delete a project with one of the following ways:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability-concepts",
    "chunk_id": 9,
    "content": "In the LangSmith UI, select the Delete option on the project‚Äôs overflow menu.\nWith the delete_tracer_sessions API endpoint\nWith the delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK.\n\nLangSmith does not support self-service deletion of individual traces.\nIf you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should reach out to LangSmith Support with the organization ID and trace IDs.\n\nEdit the source of this page on GitHubWas this page helpful?YesNoTracing quickstartPreviousTrace a RAG application tutorialNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 0,
    "content": "Privacy policy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upPrivacy Policy"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 1,
    "content": "Last Updated April 28, 2024‚ÄçLangChain, Inc. (‚ÄúLangChain,‚Äù ‚Äúwe,‚Äù ‚Äúus,‚Äù or ‚Äúour‚Äù) has prepared this Privacy Policy to explain (1) what personal information we collect, (2) how we use and share that information, and (3) your choices concerning our privacy and information practices.‚ÄçApplicability of this Privacy PolicyWe provide a unified developer platform LangSmith for developing, collaborating, testing, deploying and monitoring LLM applications (the ‚ÄúServices‚Äù). This Privacy Policy applies to personal information that we collect in connection with the Services, our website(s) (including https://www.langchain.com/), and any other products and/or services that specifically link to this Privacy Policy. ¬†This Privacy Policy does not apply to other products and services of LangChain if they do"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 2,
    "content": "services that specifically link to this Privacy Policy. ¬†This Privacy Policy does not apply to other products and services of LangChain if they do not link to this Privacy Policy, such as the LangChain Journaling Application and LangChain OpenGPTs .‚ÄçIf you are a customer of LangChain, this Privacy Policy does not apply to personal information that we process on your behalf (if any) as your service provider. Such personal information shall instead be governed by the terms and conditions of the separate customer agreement or terms of service that you have agreed to with LangChain. In addition, our Services are designed for businesses and are not intended for personal, family, or household use. Accordingly, we treat all personal information covered by this Privacy Policy as pertaining to"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 3,
    "content": "not intended for personal, family, or household use. Accordingly, we treat all personal information covered by this Privacy Policy as pertaining to individuals acting as business representatives, rather than in their personal capacity. ¬†‚ÄçPersonal information we collectInformation you provide to us:Account information:¬†When you create an account to use the Services, we collect information such as your email address, password, and other similar account registration information.Payment Information: If you are using a paid version of the Services, you may need to provide us with payment information, such as credit card information, banking information, or a billing address. We may use third-party payment providers to process payments on the Services. ¬†In particular, credit card information is"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 4,
    "content": "or a billing address. We may use third-party payment providers to process payments on the Services. ¬†In particular, credit card information is stored and processed by our payment providers on our behalf.Business Contact Information: If you are a representative of one of our actual or prospective customers, suppliers or business partners, we may collect personal information about you (such as your name, contact details and role) when entering into an agreement with your company or otherwise during the course of our relationship with your company. ¬†Feedback or correspondence, such as information you provide when you contact us with questions, feedback, reviews, or otherwise correspond with us online.Usage information, such as information about how you use the Services and interact with"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 5,
    "content": "feedback, reviews, or otherwise correspond with us online.Usage information, such as information about how you use the Services and interact with us.Marketing information, such as your preferences for receiving communications about our activities, services, newsletters, and publications, and details about how you engage with our communications.Other information that we may collect which is not specifically listed here, but which we will use in accordance with this Privacy Policy or as otherwise disclosed at the time of collection.Information we obtain from third parties. ¬†We may obtain your personal information from other third parties, such as marketing partners, publicly-available sources and data providers, for the purposes of marketing products and services that may interest you,"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 6,
    "content": "such as marketing partners, publicly-available sources and data providers, for the purposes of marketing products and services that may interest you, delivering personalized communications, and other similar activities. ¬†In addition, we may maintain pages on social media platforms, such as Facebook, Twitter, Instagram, and other third-party platforms. When you visit or interact with our pages on those platforms, the platform provider‚Äôs privacy policy will apply to your interactions and their collection, use and processing of your personal information. You or the platforms may provide us with information through the platform, and we will treat such information in accordance with this Privacy Policy. ‚ÄçAutomatic data collection. We and our service providers may automatically log information"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 7,
    "content": "such information in accordance with this Privacy Policy. ‚ÄçAutomatic data collection. We and our service providers may automatically log information about you, your computer or mobile device, and your interaction over time with our Services, our communications and other online services, such as:Device data, such as your computer‚Äôs or mobile device‚Äôs operating system type and version, manufacturer and model, browser type, screen resolution, RAM and disk size, CPU usage, device type (e.g., phone, tablet), IP address, unique identifiers (including identifiers used for advertising purposes), language settings, mobile device carrier, radio/network information (e.g., WiFi, LTE, 4G), and general location information such as city, state or geographic area.Online activity data, such as pages or"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 8,
    "content": "information (e.g., WiFi, LTE, 4G), and general location information such as city, state or geographic area.Online activity data, such as pages or screens you viewed, how long you spent on a page or screen, browsing history, navigation paths between pages or screens, information about your activity on a page or screen, access times, and duration of access, and whether you have opened our marketing emails or clicked links within them. We may use third party tools to assist with capturing online activity data.Email Open/Click Information. We may use pixels in our email campaigns that allow us to collect your email and IP address as well as the date and time you open an email or click on any links in the email that we may send to you.‚ÄçWe may use the following tools for automatic data"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 9,
    "content": "as the date and time you open an email or click on any links in the email that we may send to you.‚ÄçWe may use the following tools for automatic data collection:Cookies, which are text files that websites store on a visitor‚Äôs device to uniquely identify the visitor‚Äôs browser or to store information or settings in the browser for the purpose of helping you navigate between pages efficiently, remembering your preferences, enabling functionality, helping us understand user activity and patterns, and facilitating online advertising.Local storage technologies, like HTML5, that provide cookie-equivalent functionality but can store larger amounts of data, including on your device outside of your browser in connection with specific applications.Web beacons, also known as pixel tags or clear GIFs,"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 10,
    "content": "data, including on your device outside of your browser in connection with specific applications.Web beacons, also known as pixel tags or clear GIFs, which are used to demonstrate that a webpage or email was accessed or opened, or that certain content was viewed or clicked.For additional details, please see our Cookie Policy below.‚ÄçHow we use your personal informationTo operate our Services:Provide, operate, maintain, secure and improve our Services.Provide information about our Services.Communicate with you about our Services, including by sending you announcements, updates, security alerts, and support and administrative messages.Respond to your requests, questions and feedback.Marketing and advertising. We may from time-to-time send you direct marketing communications as permitted by"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 11,
    "content": "to your requests, questions and feedback.Marketing and advertising. We may from time-to-time send you direct marketing communications as permitted by law, including, but not limited to, notifying you of special promotions, offers and events via email. You may opt out of our marketing communications as described in the ‚ÄúOpt out of marketing communications‚Äù section below.‚ÄçFor research and development. We may use your personal information for research and development purposes, including to analyze and improve our Services and our business. As part of these activities, we may create aggregated, de-identified, or other anonymous data from personal information we collect. We make personal information into anonymous data by removing information that makes the data personally identifiable to you."
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 12,
    "content": "information we collect. We make personal information into anonymous data by removing information that makes the data personally identifiable to you. We may use this anonymous data and share it with third parties for our lawful business purposes, including to analyze and improve our Services and promote our business.‚ÄçCompliance and protection. We may use personal information to:Comply with applicable laws, lawful requests, and legal process, such as to respond to subpoenas or requests from government authorities.Protect our, your or others‚Äô rights, privacy, safety or property (including by making and defending legal claims).Audit our internal processes for compliance with legal and contractual requirements and internal policies.Enforce the terms and conditions that govern our"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 13,
    "content": "our internal processes for compliance with legal and contractual requirements and internal policies.Enforce the terms and conditions that govern our Services.Prevent, identify, investigate and deter fraudulent, harmful, unauthorized, unethical or illegal activity, including cyberattacks and identity theft.Legal bases for processing (for United Kingdom and EEA individuals)If you are an individual in the United Kingdom or European Economic Area (EEA), we collect and process information about you only where we have legal bases for doing so under applicable United Kingdom and/or EU laws. ¬†The legal bases depend on the Services you use and how you use them. This means we collect and use your information only where:We need it to provide you the Services, including to operate the Services,"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 14,
    "content": "you use them. This means we collect and use your information only where:We need it to provide you the Services, including to operate the Services, provide customer support and personalized features and to protect the safety and security of the Services;It satisfies a legitimate interest (which is not overridden by your data protection interests), such as for research and development, to market and promote the Services and to protect our legal rights and interests;You give us consent to do so for a specific purpose; orWe need to process your data to comply with a legal obligation.If you have consented to our use of information about you for a specific purpose, you have the right to change your mind at any time, but this will not affect any processing that has already taken place. ¬†Where we"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 15,
    "content": "specific purpose, you have the right to change your mind at any time, but this will not affect any processing that has already taken place. ¬†Where we are using your information because we or a third party (e.g. your employer) have a legitimate interest to do so, you have the right to object to that use though, in some cases, this may mean no longer using the Services.‚ÄçHow we share your personal informationService providers. We may share your personal information with third party companies and individuals that provide services on our behalf or help us operate our Services (such as lawyers, bankers, auditors, insurers, and providers that assist with hosting, analytics, email delivery, marketing, and database management).‚ÄçAuthorities and others. We may disclose your personal information to"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 16,
    "content": "with hosting, analytics, email delivery, marketing, and database management).‚ÄçAuthorities and others. We may disclose your personal information to law enforcement, government authorities, and private parties, as we believe in good faith to be necessary or appropriate for the compliance and protection purposes described above.‚ÄçBusiness transfers. We may transfer or otherwise share some or all of our business or assets, including your personal information, in connection with a business transaction (or potential business transaction) such as a corporate divestiture, merger, consolidation, acquisition, reorganization or sale of assets, or in the event of bankruptcy or dissolution. In such a case, we will make reasonable efforts to require the recipient to honor this Privacy"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 17,
    "content": "assets, or in the event of bankruptcy or dissolution. In such a case, we will make reasonable efforts to require the recipient to honor this Privacy Policy.‚ÄçAffiliates: We may share personal information with our current and future affiliates, meaning an entity that controls, is controlled by, or is under common control with us. Our affiliates may use the personal information we share in a manner consistent with this Privacy Policy.‚ÄçCross-border processing of your personal informationWe are headquartered in the United States. To provide and operate our services, it is necessary for us to process your personal information in the United States. If we transfer personal information across borders such that we are required to apply appropriate safeguards to personal information under applicable"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 18,
    "content": "If we transfer personal information across borders such that we are required to apply appropriate safeguards to personal information under applicable data protection laws, we will do so. Please contact us for further information about any such transfers or the specific safeguards applied.‚ÄçYour choicesPersonal information requests: In certain circumstances (including based on where you are located), you may have the following rights in relation to your personal information:the right to learn more about what personal information of yours is being processed, how and why such information is processed and the third parties who have access to such personal information. We have made this information available to you without having to request it by including it in this Privacy Policy;the right to"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 19,
    "content": "personal information. We have made this information available to you without having to request it by including it in this Privacy Policy;the right to access your personal information;the right to rectify/correct your personal information;the right to restrict the use of your personal information where permitted under applicable law; the right to request that your personal information is erased/deleted where permitted under applicable law; the right to data portability (i.e. receive your personal information or have it transferred to another controller in a structured, commonly-used, machine readable format) where permitted under applicable law; andthe right to object to processing of your personal information or to direct us not to share your personal information with a non-affiliated"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 20,
    "content": "law; andthe right to object to processing of your personal information or to direct us not to share your personal information with a non-affiliated third party where permitted under applicable law.To make a request, please contact us as provided in the ‚ÄúHow to Contact Us‚Äù section below. We may ask for specific information from you to help us confirm your identity. We will require authorized agents to confirm their identity and authority, in accordance with applicable laws. You are entitled to exercise the rights described above free from discrimination.‚ÄçIn addition, where you have provided your consent to processing for the purposes indicated above, you may withdraw your consent at any time (or otherwise exercise your aforementioned rights in relation to your personal information) by"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 21,
    "content": "above, you may withdraw your consent at any time (or otherwise exercise your aforementioned rights in relation to your personal information) by contacting us below(see contact details below). ‚ÄçPlease note that in some circumstances, we may not be able to fully comply with your request, for example if we are required to retain certain information about you to comply with applicable laws and regulations or if the information is necessary in order for us to provide the services you requested. In particular, we, and our collection and processing of your personal information, may be governed by laws and regulations on anti-money laundering, fraud prevention, taxation and financial services. We will not discriminate against you for exercising your rights. We will not deny you access to our"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 22,
    "content": "prevention, taxation and financial services. We will not discriminate against you for exercising your rights. We will not deny you access to our services, or provide you a lower quality of services if you exercise your rights.‚ÄçYou also have the right to lodge a complaint with the relevant authority (as applicable) or a supervisory authority in the UK or EU member state of your usual residence or place of work or of the place of the alleged breach, if you consider that the processing of your personal information carried out by LangChain or any of our affiliates or third-party service providers, has breached data protection laws. Individuals and data protection supervisory authorities in the EU and the UK may contact our data protection representatives according to Articles 27 EU and UK"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 23,
    "content": "and data protection supervisory authorities in the EU and the UK may contact our data protection representatives according to Articles 27 EU and UK GDPR:‚ÄçYou may also appeal to certain courts against (A) any failure of the relevant authority to give written notice of whether the complaint is either being investigated or not being investigated and, where applicable, the progress and the outcome of the investigation or (B) a determination of the relevant authority not to investigate the complaint or a determination that a controller or processor has not breached or is not likely to breach an operative provision in connection with the complaint.‚ÄçOpt out of marketing communications. You may opt out of email communications by following the opt-out or unsubscribe instructions at the bottom of"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 24,
    "content": "out of marketing communications. You may opt out of email communications by following the opt-out or unsubscribe instructions at the bottom of the email. ¬†‚ÄçOnline tracking opt-out. There are a number of ways to opt out of having your online activity and device data collected through our Services, for additional details please see our Cookie Policy below. ¬†‚ÄçJob ApplicantsWhen you visit the careers portion of our website, we collect personal information that you provide to us in connection with your job application. This includes business and personal contact information, professional credentials and skills, educational and work history, and other information of the type that may be included in a resume. This may also include diversity information that you voluntarily provide. We use this"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 25,
    "content": "other information of the type that may be included in a resume. This may also include diversity information that you voluntarily provide. We use this information to facilitate our recruitment activities and process employment applications, such as by evaluating a job candidate for an employment activity, and monitoring recruitment statistics. We may also use and share this information to provide improved administration of the website, and as otherwise necessary: (a) to comply with relevant laws or to respond to subpoenas or warrants served on us; (b) to protect and defend the rights or property of us or others; or (c ) in connection with a legal investigation.Data RetentionWe may retain your personal information for as long as it is reasonably needed in order to maintain and expand our"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 26,
    "content": "legal investigation.Data RetentionWe may retain your personal information for as long as it is reasonably needed in order to maintain and expand our relationship and provide you with our services; in order to comply with our legal and contractual obligations; or to protect ourselves from any potential disputes. To determine the appropriate retention period for personal information, we consider the amount, nature, and sensitivity of such information, the potential risk of harm from unauthorized use or disclosure of such information, the purposes for which we process it, and the applicable legal requirements. ¬†Other sites, mobile applications and servicesOur Services may contain links to other websites, mobile applications, and other online services operated by third parties. These links"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 27,
    "content": "and servicesOur Services may contain links to other websites, mobile applications, and other online services operated by third parties. These links are not an endorsement of, or representation that we are affiliated with, any third party. In addition, our content may be included on web pages or in mobile applications or online services that are not associated with us. We do not control third party websites, mobile applications or online services, and we are not responsible for their actions. Other websites and services follow different rules regarding the collection, use and sharing of your personal information. We encourage you to read the privacy policies of the other websites and mobile applications and online services you use.Security practicesWe use reasonable organizational,"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 28,
    "content": "the privacy policies of the other websites and mobile applications and online services you use.Security practicesWe use reasonable organizational, technical and administrative measures designed to protect against unauthorized access, misuse, loss, disclosure, alteration and destruction of personal information we maintain. Unfortunately, data transmission over the Internet cannot be guaranteed as completely secure. Therefore, while we strive to protect your personal information, we cannot guarantee the security of personal information. In the event that we are required to notify you about a situation involving your data, we may do so by email or telephone to the extent permitted by law.Children Our Services are not intended for children, and we do not collect personal information from"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 29,
    "content": "or telephone to the extent permitted by law.Children Our Services are not intended for children, and we do not collect personal information from them. We define ‚Äúchildren‚Äù as anyone under 18 years old. If we learn we have collected or received personal information from a child without verification of parental consent, we will delete the information. If you believe we might have any information from or about a child, please contact us via the contract information noted below.Changes to this Privacy PolicyWe reserve the right to modify this Privacy Policy at any time. If we make material changes to this Privacy Policy, we will notify you by updating the date of this Privacy Policy and posting it on our Services. We may also provide notification of changes in another way that we believe is"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 30,
    "content": "the date of this Privacy Policy and posting it on our Services. We may also provide notification of changes in another way that we believe is reasonably likely to reach you, such as via e-mail (if you have an account where we have your contact information) or another manner through our Services.‚ÄçAny modifications to this Privacy Policy will be effective upon our posting the new terms and/or upon implementation of the new changes on our Services (or as otherwise indicated at the time of posting). In all cases, your continued use of the Services after the posting of any modified Privacy Policy indicates your acceptance of the terms of the modified Privacy Policy.How to contact usIf you would like to submit a Data Subject Access Request, please fill out our form here. If you have any"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 31,
    "content": "modified Privacy Policy.How to contact usIf you would like to submit a Data Subject Access Request, please fill out our form here. If you have any questions or concerns, you can reach us by email at support@langchain.dev. ‚ÄçNotice to UK and EU ResidentsIf you would like to submit a complaint about our use of your personal information or our response to your requests regarding your personal information, you can contact us or submit a complaint to the data protection regulator in your jurisdiction. You can find your data protection regulator here.‚ÄçUK and EU Representatives.We have appointed a representative in the EU and the UK. For the EU, please contact our representative by post at INSTANT EU GDPR REPRESENTATIVE LIMITED Office 2 12A Lower Main Street, Lucan Co. Dublin K78 X5P8 Ireland, or"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 32,
    "content": "contact our representative by post at INSTANT EU GDPR REPRESENTATIVE LIMITED Office 2 12A Lower Main Street, Lucan Co. Dublin K78 X5P8 Ireland, or by email at contact@gdprlocal.com. In the UK, please contact our representative by post at GDPR Local Ltd 1st Floor Front Suite 27-29 North Street, Brighton England BN1 1EB, or by email at contact@gdprlocal.com.‚ÄÉCookie PolicyThis Cookie Policy explains how LangChain uses cookies and similar technologies in connection with our Services. This Cookie Policy should be read in conjunction with our Privacy Policy.‚ÄçIf you have any questions or concerns about the Cookie Policy, please contact us at support@langchain.dev or as otherwise described in our Privacy Policy.‚ÄçWhat are cookies and similar technologies?Cookies are text files that websites store"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 33,
    "content": "or as otherwise described in our Privacy Policy.‚ÄçWhat are cookies and similar technologies?Cookies are text files that websites store and access on a visitor‚Äôs device to uniquely identify the visitor‚Äôs browser or to store information or settings in the browser to allow us distinguish you from other users of our Services for the purpose of helping you navigate between pages efficiently, remembering your preferences, enabling functionality, helping us understand activity and patterns, and facilitating online advertising.Local storage technologies, like HTML5, provide cookie-equivalent functionality but can store larger amounts of data, including on your device outside of your browser in connection with specific applications.Web beacons, also known as pixel tags or clear GIFs, are used to"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 34,
    "content": "on your device outside of your browser in connection with specific applications.Web beacons, also known as pixel tags or clear GIFs, are used to demonstrate that a webpage or email was accessed or opened, or that certain content was viewed or clicked.This Cookie Policy refers to all these technologies, and other types of tracking technologies used through our Services, collectively as ‚Äúcookies.‚Äù‚ÄçHow do we use cookies and other similar technologies?We may use both persistent cookies and session cookies. Persistent cookies stay on your device for a set period of time or until you delete them, while session cookies are deleted once you close your web browser. The cookies placed through your use of our website are either set by us (first-party cookies) or by a third party at our request"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 35,
    "content": "your web browser. The cookies placed through your use of our website are either set by us (first-party cookies) or by a third party at our request (third-party cookies).We may also allow our advertising partners to collect this information through our website.‚ÄçWhat types of cookies and similar technologies do we use?We may use the following categories of cookies:‚ÄçStrictly Necessary Cookies: These cookies are necessary for the Services to function and cannot be switched off in our systems. They allow us to enable security, prevent fraud and debug the Services and are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 36,
    "content": "for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but then some parts of the Services will not work.‚ÄçFunctional Cookies. These cookies are used to recognize you when you return to our Services or to enable the Services to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of the Services may not function properly.‚ÄçAnalytics Cookies. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our Services. They help us to know which pages are the most and least popular and see how visitors move"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 37,
    "content": "we can measure and improve the performance of our Services. They help us to know which pages are the most and least popular and see how visitors move around the Services. All information these cookies collect is aggregated. If you do not allow these cookies we will not know when you have visited our Services and will not be able to monitor its performance. ‚ÄçHow can you control the use of cookies?Depending on where you access the Services from, you may be presented with a cookie banner or other tool to provide permissions prior to non-Strictly Necessary cookies being set. In this case, we only set these non-Strictly Necessary cookies with your consent. ‚ÄçYou can also limit online tracking by:Blocking cookies in your browser. Most browsers let you remove or reject cookies, including cookies"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 38,
    "content": "consent. ‚ÄçYou can also limit online tracking by:Blocking cookies in your browser. Most browsers let you remove or reject cookies, including cookies used for interest-based advertising. To do this, follow the instructions in your browser settings. Many browsers accept cookies by default until you change your settings. For more information about cookies, including how to see what cookies have been set on your device and how to manage and delete them, visit www.allaboutcookies.org. Use the following links to learn more about how to control cookies and online tracking through your browser:‚ÄçFirefox; Chrome; Microsoft Edge; Safari (Mac); Safari (Mobile/iOS)Blocking advertising ID use in your mobile settings. Your mobile device settings can provide functionality to limit use of the advertising"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 39,
    "content": "advertising ID use in your mobile settings. Your mobile device settings can provide functionality to limit use of the advertising ID associated with your mobile device for interest-based advertising purposes.Using privacy plug-ins or browsers. You can block our websites from setting cookies used for interest-based ads by using a browser with privacy features, like Brave, or installing browser plugins like Privacy Badger, Ghostery, or uBlock Origin, and configuring them to block third party cookies/trackers. Platform opt-outs. Some advertising companies offer opt-out features that let you opt out of use of your information for interest-based advertising, including:GoogleFacebookNote that because these opt out mechanisms are specific to the device or browser on which they are exercised, you"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 40,
    "content": "including:GoogleFacebookNote that because these opt out mechanisms are specific to the device or browser on which they are exercised, you will need to opt out on every browser and device that you use.‚ÄçDo Not Track. Some Internet browsers can be configured to send ‚ÄúDo Not Track‚Äù signals to the online services that you visit. We currently do not respond to ‚ÄúDo Not Track‚Äù or similar signals. To find out more about ‚ÄúDo Not Track,‚Äù please visit http://www.allaboutdnt.coProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops!"
  },
  {
    "url": "https://www.langchain.com/privacy-policy",
    "chunk_id": 41,
    "content": "AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/resources-items/langchain-state-of-ai-2024-report",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#introduction",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 0,
    "content": "Welcome Contributors | ü¶úÔ∏èüîó LangChain"
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 1,
    "content": "Skip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchWelcome ContributorsTutorialsMake your first docs PRTutorialsHow-to guidesTestingContribute codeContribute documentationHow-to guidesContribute integrationsReference & FAQRepository StructureFAQReferenceReview ProcessWelcome ContributorsOn this pageWelcome Contributors\nHi there! Thank you for your interest in contributing to LangChain."
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 2,
    "content": "Hi there! Thank you for your interest in contributing to LangChain.\nAs an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.\nTutorials‚Äã\nMore coming soon! We are working on tutorials to help you make your first contribution to the project."
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 3,
    "content": "Make your first docs PR\n\nHow-to guides‚Äã\n\nDocumentation: Help improve our docs, including this one!\nCode: Help us write code, fix bugs, or improve our infrastructure.\nIntegrations: Help us integrate with your favorite vendors and tools.\nStandard Tests: Ensure your integration passes an expected set of tests.\n\nReference‚Äã\n\nRepository Structure: Understand the high level structure of the repository.\nReview Process: Learn about the review process for pull requests.\nFrequently Asked Questions (FAQ): Get answers to common questions about contributing."
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 4,
    "content": "Community‚Äã\nüí≠ Forum‚Äã\nWe have a LangChain Forum where users can ask usage questions, discuss design decisions, and propose new features.\nIf you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.\nüö© GitHub Issues‚Äã\nOur issues page is kept up to date with bugs, docs improvements, and triaged feature requests that are being worked on.\nThere is a taxonomy of labels\nto help with sorting and discovery of issues of interest. Please use these to help\norganize issues. Check out the Help Wanted\nand Good First Issue\ntags for recommendations.\nIf you start working on an issue, please assign it to yourself.\nIf you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature."
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 5,
    "content": "If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.\nIf two issues are related, or blocking, please link them rather than combining them.\nWe will try to keep these issues as up-to-date as possible, though\nwith the rapid rate of development in this field some may get out of date.\nIf you notice this happening, please let us know.\nüì¢ Community Slack‚Äã\nWe have a community slack where you can ask questions, get help, and discuss the project with other contributors and users.\nüôã Getting Help‚Äã\nOur goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please\nask in community slack or open a forum post."
  },
  {
    "url": "https://python.langchain.com/docs/contributing/",
    "chunk_id": 6,
    "content": "ask in community slack or open a forum post.\nIn a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.\nIf you are finding these difficult (or even just annoying) to work with, feel free to ask in community slack!Edit this pageNextTutorialsTutorialsHow-to guidesReferenceCommunityüí≠ Forumüö© GitHub Issuesüì¢ Community Slacküôã Getting HelpCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 0,
    "content": "Create and manage datasets in the UI - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCreate a datasetCreate and manage datasets in the UIGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetWith the UIWith the SDKManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 1,
    "content": "an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageCreate a dataset and add examplesManually from a tracing projectAutomatically from a tracing projectFrom examples in an Annotation QueueFrom the Prompt PlaygroundImport a dataset from a CSV or JSONL fileCreate a new dataset from the Datasets & Experiments pageAdd synthetic examples created by an LLMManage a datasetCreate a dataset schemaCreate and manage dataset splitsEdit example metadataFilter"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 2,
    "content": "pageAdd synthetic examples created by an LLMManage a datasetCreate a dataset schemaCreate and manage dataset splitsEdit example metadataFilter examplesDatasetsCreate a datasetCreate and manage datasets in the UICopy pageCopy pageDatasets enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of examples, which store inputs, outputs, and optionally, reference outputs."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 3,
    "content": "This page outlines the various methods for creating and managing datasets in the LangSmith UI.\n‚ÄãCreate a dataset and add examples\nThe following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 4,
    "content": "Manually from a tracing project\nAutomatically from a tracing project\nFrom examples in an Annotation Queue\nFrom the Prompt Playground\nImport a dataset from a CSV or JSONL file\nCreate a new dataset from the dataset page\nAdd synthetic examples created by an LLM via the Datasets UI"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 5,
    "content": "‚ÄãManually from a tracing project\nA common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have configured tracing to LangSmith.\nA technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to Filter traces guide.\nThere are two ways to add data manually from a tracing project to datasets. Navigate to Tracing Projects and select a project.\n\n\nMulti-select runs from the runs table. On the Runs tab, multi-select runs. At the bottom of the page, click  Add to Dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 6,
    "content": "Multi-select runs from the runs table. On the Runs tab, multi-select runs. At the bottom of the page, click  Add to Dataset.\n\n\n\nOn the Runs tab, select a run from the table. On the individual run details page, select  Add to -> Dataset in the top right corner.\n\nWhen you select a dataset from the run details page, a modal will pop up letting you know if any transformations were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.\n\nYou can then optionally edit the run before adding it to the dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 7,
    "content": "‚ÄãAutomatically from a tracing project\nYou can use run rules to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are tagged with a specific use case or have a low feedback score.\n‚ÄãFrom examples in an Annotation Queue\nIf you rely on subject matter experts to build meaningful datasets, use annotation queues to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 8,
    "content": "Annotation queues can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click Add to Dataset or hit the hot key D to add the run to it.\nAny modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 9,
    "content": "Note you can also set up rules to add runs that meet specific criteria to an annotation queue using automation rules.\n‚ÄãFrom the Prompt Playground\nOn the Prompt Playground page, select Set up Evaluation, click +New if you‚Äôre starting a new dataset or select from an existing dataset.\nCreating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit from the datasets page.\nTo edit the examples:\n\nUse +Row to add a new example to the dataset\nDelete an example using the ‚ãÆ dropdown on the right hand side of the table\nIf you‚Äôre creating a reference-free dataset remove the ‚ÄúReference Output‚Äù column using the x button in the column. Note: this action is not reversible."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 10,
    "content": "‚ÄãImport a dataset from a CSV or JSONL file\nOn the Datasets & Experiments page, click +New Dataset, then Import an existing dataset from CSV or JSONL file.\n‚ÄãCreate a new dataset from the Datasets & Experiments page\n\nNavigate to the Datasets & Experiments page from the left-hand menu.\nClick + New Dataset.\nOn the New Dataset page, select the Create from scratch tab.\nAdd a name and description for the dataset.\n(Optional) Create a dataset schema to validate your dataset.\nClick Create, which will create an empty dataset.\nTo add examples inline, on the dataset‚Äôs page, go to the Examples tab. Click + Example.\nDefine examples in JSON and click Submit. For more details on dataset splits, refer to Create and manage dataset splits."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 11,
    "content": "‚ÄãAdd synthetic examples created by an LLM\nIf you have existing examples and a schema defined on your dataset, when you click + Example there is an option to  Add AI-Generated Examples. This will use an LLM to create synthetic examples.\nIn Generate examples, do the following:\n\n\nClick API Key in the top right of the pane to set your OpenAI API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step.\n\n\nSelect few-shot examples: Toggle Automatic or Manual reference examples. You can select these examples manually from your dataset or use the automatic selection option.\n\n\nEnter the number of synthetic examples you want to generate.\n\n\nClick Generate."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 12,
    "content": "Enter the number of synthetic examples you want to generate.\n\n\nClick Generate.\n\n\n\nThe examples will appear on the Select generated examples page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click Save Examples.\n\n\nEach example will be validated against your specified dataset schema and tagged as synthetic in the source metadata."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 13,
    "content": "‚ÄãManage a dataset\n‚ÄãCreate a dataset schema\nLangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard JSON schema, with the addition of a few prebuilt types that make it easier to type common primitives like messages and tools.\nCertain fields in your schema have a + Transformations option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the convert to OpenAI messages transformation will convert message-like objects, like LangChain messages, to OpenAI message format.\nFor the full list of available transformations, see our reference."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 14,
    "content": "For the full list of available transformations, see our reference.\nIf you plan to collect production traces in your dataset from LangChain ChatModels or from OpenAI calls using the LangSmith OpenAI wrapper, we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.Please see the dataset transformations reference for more information.\n‚ÄãCreate and manage dataset splits"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 15,
    "content": "Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas"
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 16,
    "content": "via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 17,
    "content": "In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.\nIn order to create and manage splits in the app, you can select some examples in your dataset and click ‚ÄúAdd to Split‚Äù. From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 18,
    "content": "‚ÄãEdit example metadata\nYou can add metadata to your examples by clicking on an example and then clicking ‚ÄúEdit‚Äù on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then group by when analyzing experiment results or filter by when you call list_examples in the SDK.\n\n‚ÄãFilter examples\nYou can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table."
  },
  {
    "url": "https://docs.langchain.com/langsmith/manage-datasets-in-application",
    "chunk_id": 19,
    "content": "Filter by split: Select split > Select a split to filter by\nFilter by metadata: Filters > Select ‚ÄúMetadata‚Äù from the dropdown > Select the metadata key and value to filter on\nFull-text search: Filters > Select ‚ÄúFull Text‚Äù from the dropdown > Enter your search criteria\n\nYou may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoApplication-specific evaluation approachesPreviousHow to create and manage datasets programmaticallyNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 0,
    "content": "LangChain | LinkedIn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n      Skip to main content\n    \n\n\n\nLinkedIn\n \n\n\n\n\n\n\n\n\n        Top Content\n      \n\n\n\n\n\n\n\n        People\n      \n\n\n\n\n\n\n\n        Learning\n      \n\n\n\n\n\n\n\n        Jobs\n      \n\n\n\n\n\n\n\n        Games\n      \n\n\n\n\n\n          Sign in\n      \n\n      Join now\n \n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n\n\n\n                      LangChain\n                      \n                    \n\n                    Technology, Information and Internet\n                \n\n\n\n\n            See jobs\n          \n\n            Follow\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n          Discover all 148 employees\n        \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this company\n                    \n    \n\n\n\n\n \n\n \n\n\n\n\n\n\n            About us"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 1,
    "content": "Report this company\n                    \n    \n\n\n\n\n \n\n \n\n\n\n\n\n\n            About us\n          \n\n\n\n              LangChain is the platform for building reliable agents. Our products power top engineering teams ‚Äî from fast-growing startups like Lovable, Mercor, and Clay to global brands including AT&T, Home Depot, and Klarna.\n\nLangGraph is a low-level orchestration framework for building controllable agents and long-running workflows. It‚Äôs used in production by teams at Replit, Uber, LinkedIn, GitLab, and more.\n\nLangSmith offers unified evaluation and monitoring to help developers debug, evaluate, and improve their agents at scale."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 2,
    "content": "LangSmith offers unified evaluation and monitoring to help developers debug, evaluate, and improve their agents at scale.\n\nLangChain provides hundreds of integrations and composable components, making it easy to connect with the latest models, tools, and databases ‚Äî with minimal engineering overhead.\n\nTogether, these tools help teams build, deploy, and manage enterprise-grade agents, faster.\n          \n\n\n\n          Website\n      \n\n\n      langchain.com\n      \n\nExternal link for LangChain\n\n\n\n\n          Industry\n      \n\n        \n                  Technology, Information and Internet\n                \n      \n\n\n\n          Company size\n      \n\n        \n                  \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n        51-200 employees\n  \n\n                \n      \n\n\n\n\n          Type"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 3,
    "content": "51-200 employees\n  \n\n                \n      \n\n\n\n\n          Type\n      \n\n        \n                  Privately Held\n                \n      \n\n\n \n\n\n\n\n\n            Products\n          \n\n\n\n\n\n\n      \n          LangChain\n        \n      \n          \n\n\n\n \n\n \n\n      \n          LangChain\n        \n          \n\n\n              Software Development Kits (SDK)\n            \n\n\n                  \n          LangChain is the platform for building reliable agents. Our products power top engineering teams ‚Äî from fast-growing startups like Loveable, Mercor, and Clay to global brands including AT&T, Home Depot, and Klarna.\n        \n                \n \n\n\n\n\n\n\n\n\n            Employees at LangChain\n          \n\n\n\n\n\n\n\n\n \n            \n        Allison (Alli) Ewing"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 4,
    "content": "Employees at LangChain\n          \n\n\n\n\n\n\n\n\n \n            \n        Allison (Alli) Ewing\n\n\n\n\n              \n        Senior Technical Recruiter @LangChain\n      \n            \n\n \n\n\n\n\n\n\n\n\n \n            \n        Alex Kira\n\n\n\n\n              \n        Engineering @ LangChain\n      \n            \n\n \n\n\n\n\n\n\n\n\n \n            \n        Darren Moffett\n\n\n\n\n              \n        Vice President Sales\n      \n            \n\n \n\n\n\n\n\n\n\n\n \n            \n        Steve Lee\n\n\n\n\n              \n        Rev Ops at LangChain\n      \n            \n\n \n\n\n\n\n\n          See all employees\n        \n\n\n\n\n\n            Updates\n          \n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 5,
    "content": "LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      5h\n  \n                      \n\n                        Edited\n                      \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nLaunch Week is right around the corner ‚Äî and we're kicking it off in person! \n\nWe'll be hosting meetups in San Francisco, Boston, and NYC to celebrate LangChain's 3rd birthday and share what's coming next during Launch Week."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 6,
    "content": "We'll be hosting meetups in San Francisco, Boston, and NYC to celebrate LangChain's 3rd birthday and share what's coming next during Launch Week.\n\nCome hang with the team, connect with our amazing community of builders shipping agents in production, and get the inside scoop on what's happening during Launch Week.\n\nExpect great food, drinks, and good conversation. It‚Äôs going to be an exciting evening!\n\nüìç Pick your city and RSVP: \n\nüåâ SF:¬†https://luma.com/7baj9rx5\nü¶™ Boston:¬†https://luma.com/135zbg4u\nüóΩ NYC:¬†https://luma.com/f5jrv7t6\n\nSee you there¬†üëã\n \n\n\n\n\n\n\n\n\n\n\n\n                    34\n              \n\n\n \n\n \n\n\n\n\n\n        \n                3 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 7,
    "content": "Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      9h\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 8,
    "content": "9h\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nIn this blog piece, you‚Äôll learn why and how we built LangGraph for production agents. \nBuilding upon feedback from the super popular LangChain framework, we aimed to find the right abstraction for AI agents, and decided that was little to no abstraction at all. Instead, we focused on control and durability, and the core features needed to scale.\nhttps://lnkd.in/gaY9gHVH¬†\n \n\n\n\n\n\n\n\n      Building LangGraph: Designing an Agent Runtime from first principles\n    \n\n      blog.langchain.com\n    \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n                    124\n              \n\n\n \n\n \n\n\n\n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\nLangChain reposted this"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 9,
    "content": "Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\nLangChain reposted this\n    \n\n \n\n\n\n \n\n\n\n              Grace Ge\n            \n \n\n                Partner at Amplify\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      1d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 10,
    "content": "1d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nFOUNDING ENGINEER: probably the most important and under-discussed role at an early stage startup. I sat down with founding engineers from LangChain (Nuno Campos), Perplexity (Nikhil Thota), and Stytch (Alex Zaldastani) to talk about what makes a good one, what they look for, how they make technical decisions, and more:\n\n      ‚Ä¶more\n    \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n                    31\n              \n\n\n \n\n \n\n\n\n\n\n        \n                2 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 11,
    "content": "LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      1d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nLearn how to use¬†LangSmith¬†to debug your AI applications.¬†üõ†Ô∏è  In this video, we‚Äôll show you how¬†\"Studio,\" our¬†IDE for building agents,¬†works, and how you can use it with any¬†LangGraph agent¬†you‚Äôve built. You‚Äôll also get everything you need to get started with Studio.\nhttps://lnkd.in/duhc72Qr\n\n      ‚Ä¶more\n    \n\n\n\n\n\n\n    Getting Started with LangSmith (3/8): Debugging with Studio"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 12,
    "content": "‚Ä¶more\n    \n\n\n\n\n\n\n    Getting Started with LangSmith (3/8): Debugging with Studio\n  \n\n\n              https://www.youtube.com/\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n                    70\n              \n\n\n \n\n \n\n\n\n\n\n        \n                3 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\nLangChain reposted this\n    \n\n \n\n\n\n \n\n\n\n              Karan Singh\n            \n \n\n                Partnerships at LangChain\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      3d\n  \n                      \n\n                        Edited"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 13,
    "content": "3d\n  \n                      \n\n                        Edited\n                      \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nAs your organization builds agentic AI, one architectural imperative stands out: ùóµùòÇùó∫ùóÆùóª-ùó∂ùóª-ùóπùóºùóºùóΩ ùó∞ùóºùóªùòÄùó≤ùóªùòÅ + ùóÆùòÇùòÅùóµùóºùóøùó∂ùòáùóÆùòÅùó∂ùóºùóª.\n\nAgents need dynamic access, not permanent keys.\n\nLanggraph enables exactly this human-in-loop consent flow: agents interrupt, ask for permission, then resume."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 14,
    "content": "Langgraph enables exactly this human-in-loop consent flow: agents interrupt, ask for permission, then resume. \n\nùó™ùóµùòÜ ùóµùòÇùó∫ùóÆùóª-ùó∂ùóª-ùóπùóºùóºùóΩ ùó∂ùòÄ ùóªùóºùóª-ùóªùó≤ùó¥ùóºùòÅùó∂ùóÆùóØùóπùó≤ ùó≥ùóºùóø ùòÅùóµùó≤ ùó≤ùóªùòÅùó≤ùóøùóΩùóøùó∂ùòÄùó≤:\nüå± Prevent privilege creep - agents only gain new permissions when a human explicitly approves\nüìú Ensure auditability - every access request and consent step is logged, reviewed, and reportable\nüõ°Ô∏è Build governance confidence - executives, compliance, and security teams can see oversight in action\nüîí Least privilege + just-in-time - agents receive only the access they need, when they need it\n\nùóõùóºùòÑ ùóüùóÆùóªùó¥ùóöùóøùóÆùóΩùóµ / ùóüùóÆùóªùó¥ùòÄùó∫ùó∂ùòÅùóµ ùó±ùó≤ùóπùó∂ùòÉùó≤ùóø:\n1. You register OAuth providers (GitHub, internal APIs, etc.). \n\n2. When the agent needs new access, it pauses and surfaces a consent URL¬†for the human to approve."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 15,
    "content": "2. When the agent needs new access, it pauses and surfaces a consent URL¬†for the human to approve. \n\n3. On consent, the agent resumes with the scoped token - future flows are smoother thanks to token refresh logic. \n\n4. Optionally, tokens can be human-scoped (shared across agents) rather than tied to a single agent. \n\nCheck out link to blog on Agent Authorization and docs link on how to set up Agent Auth in Langgraph.\n \n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                    118\n              \n\n\n \n\n \n\n\n\n\n\n        \n                4 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 16,
    "content": "LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      2d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nWe're hosting an intimate evening in Boston to celebrate LangChain's 3rd birthday and share our biggest product releases of the year. \n\nIf you're an AI builder and want to connect with the community (plus get an early look at what we've been working on), join us. \n\nüëâ  RSVP: https://luma.com/135zbg4u\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                    43"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 17,
    "content": "üëâ  RSVP: https://luma.com/135zbg4u\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                    43\n              \n\n\n \n\n \n\n\n\n\n\n        \n                3 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      2d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nLearn about the different types of¬†runs¬†you can create while tracing in¬†LangSmith‚Äîand how they help you understand your application‚Äôs execution."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 18,
    "content": "Learn about the different types of¬†runs¬†you can create while tracing in¬†LangSmith‚Äîand how they help you understand your application‚Äôs execution.\n\nUnlike traditional logs (or¬†traces), which can be hard to interpret for LLM applications, LangSmith makes it easy to explore and debug with an intuitive, purpose-built UX.\n\nhttps://lnkd.in/e5u-Sgar\n\n      ‚Ä¶more\n    \n\n\n\n\n\n\n    Getting Started with LangSmith (2/8): Types of Runs\n  \n\n\n              https://www.youtube.com/\n            \n\n\n\n\n\n\n\n\n\n\n\n\n                    32\n              \n\n\n \n\n \n\n\n\n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 19,
    "content": "LangChain\n            \n \n\n                459,120 followers\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      2d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\n\n\n\n\n \n\n\n\n              Sydney Runkle\n            \n \n\n                Software Engineer at LangChain\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      2d\n  \n \n \n\n \n\nWe just added a long awaited guardrails guide to the LangChain docs!"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 20,
    "content": "2d\n  \n \n \n\n \n\nWe just added a long awaited guardrails guide to the LangChain docs!\n\nShip safer agents faster with: \nüõ°Ô∏è Built-in PII redaction \nü´Ö Human-in-the-loop approvals \n\nOr build custom guardrails that trigger before/after your model calls.\n\nhttps://lnkd.in/eWWFH8PB\n \n\n\n\n\n\n\n\n      Guardrails - Docs by LangChain\n    \n\n      docs.langchain.com\n    \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n                    439\n              \n\n\n \n\n \n\n\n\n\n\n        \n                9 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain reposted this\n    \n\n \n\n\n\n \n\n\n\n              Sydney Runkle\n            \n \n\n                Software Engineer at LangChain"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 21,
    "content": "Sydney Runkle\n            \n \n\n                Software Engineer at LangChain\n            \n\n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      3d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nMost agent frameworks fall short when it comes to customization beyond the core agent loop (a model with a prompt, calling tools). LangChain V1 (alpha out now!) introduces a new concept of agent middleware that enables hooking into this loop at every step."
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 22,
    "content": "üìÇ before_agent ‚Äî Load files, validate input\n‚úÇÔ∏è before_model ‚Äî Summarize convos, trim messages\n‚ôªÔ∏è wrap_model_call ‚Äî Dynamic prompts, model, tools\nüõ†Ô∏è wrap_tool_call ‚Äî Tool retries, error handling\nüßë after_model ‚Äî Human in the loop\nüõ°Ô∏è after_agent ‚Äî Save results, final guardrails\n\nPlus, new docs: https://lnkd.in/ejusf3AP!\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                    681\n              \n\n\n \n\n \n\n\n\n\n\n        \n                21 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n\n\n              LangChain\n            \n \n\n                459,120 followers"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 23,
    "content": "3d\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\n\nAgents don‚Äôt just chat ‚Äî they act by fetching data, sending messages, calling APIs, and updating records, which makes securing them a whole new challenge.\n\nOur latest blog post breaks down how to implement authentication and authorization for agents.\n\nüîíIn this blog post, learn about:\n‚Ä¢ The unique access control challenges agents introduce\n‚Ä¢ How OAuth 2.0 and OIDC apply to agent architectures\n‚Ä¢ Which flows (Auth Code, OBO, Client Credentials) you‚Äôll actually need\n\nüëâ Read the full post: https://lnkd.in/gMkqbzcK"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 24,
    "content": "üëâ Read the full post: https://lnkd.in/gMkqbzcK\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n                    1,127\n              \n\n\n \n\n \n\n\n\n\n\n        \n                24 Comments\n            \n      \n \n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n      Share\n    \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n              Join now to see what you are missing\n            \n\n\n\n\n\n\n                  Find people you know at LangChain\n                \n\n\n\n\n\n\n                  Browse recommended jobs for you\n                \n\n\n\n\n\n\n                  View all updates, news, and articles\n                \n\n\n \n              Join now\n            \n\n\n\n\n\n\n\n\n\n              Similar pages\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n            \n        Hugging Face\n      \n \n\n              \n        Software Development"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 25,
    "content": "Hugging Face\n      \n \n\n              \n        Software Development\n      \n            \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        LlamaIndex\n      \n \n\n              \n        Technology, Information and Internet\n      \n            \n\n              \n        San Francisco, California\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        Perplexity\n      \n \n\n              \n        Software Development\n      \n            \n\n              \n        San Francisco, California\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        Anthropic\n      \n \n\n              \n        Research Services\n      \n            \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        CrewAI\n      \n \n\n              \n        Software Development"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 26,
    "content": "CrewAI\n      \n \n\n              \n        Software Development\n      \n            \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        Mistral AI\n      \n \n\n              \n        Technology, Information and Internet\n      \n            \n\n              \n        Paris, France\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        OpenAI\n      \n \n\n              \n        Research Services\n      \n            \n\n              \n        San Francisco, CA\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        Ollama\n      \n \n\n              \n        Technology, Information and Internet\n      \n            \n\n              \n        Palo Alto, California\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        DeepLearning.AI"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 27,
    "content": "Palo Alto, California\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        DeepLearning.AI\n      \n \n\n              \n        Software Development\n      \n            \n\n              \n        Mountain View, California\n      \n            \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n            \n        Generative AI\n      \n \n\n              \n        Technology, Information and Internet\n      \n            \n\n\n\n \n\n\n\n\n\n                \n            Show more similar pages\n          \n              \n\n\n                \n            Show fewer similar pages\n          \n              \n\n\n\n\n\n\n              Browse jobs\n            \n\n\n\n\n\n\n\n\n\n\n\n\n            \n        Engineer jobs\n      \n \n\n\n\n\n              \n        555,845 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Scientist jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 28,
    "content": "555,845 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Scientist jobs\n      \n \n\n\n\n\n              \n        48,969 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Analyst jobs\n      \n \n\n\n\n\n              \n        694,057 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Machine Learning Engineer jobs\n      \n \n\n\n\n\n              \n        148,937 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Software Engineer jobs\n      \n \n\n\n\n\n              \n        300,699 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Developer jobs\n      \n \n\n\n\n\n              \n        258,935 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Intern jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 29,
    "content": "258,935 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Intern jobs\n      \n \n\n\n\n\n              \n        71,196 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Manager jobs\n      \n \n\n\n\n\n              \n        1,880,925 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Director jobs\n      \n \n\n\n\n\n              \n        1,220,357 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Project Manager jobs\n      \n \n\n\n\n\n              \n        253,048 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Enterprise Account Executive jobs\n      \n \n\n\n\n\n              \n        44,389 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Associate jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 30,
    "content": "44,389 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Associate jobs\n      \n \n\n\n\n\n              \n        1,091,945 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Account Executive jobs\n      \n \n\n\n\n\n              \n        71,457 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Product Designer jobs\n      \n \n\n\n\n\n              \n        45,389 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Senior Software Engineer jobs\n      \n \n\n\n\n\n              \n        78,145 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Python Developer jobs\n      \n \n\n\n\n\n              \n        46,642 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Graduate jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 31,
    "content": "46,642 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Graduate jobs\n      \n \n\n\n\n\n              \n        361,130 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Full Stack Engineer jobs\n      \n \n\n\n\n\n              \n        38,546 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Data Analyst jobs\n      \n \n\n\n\n\n              \n        329,009 open jobs\n      \n            \n \n\n\n\n\n\n\n\n\n\n\n            \n        Writer jobs\n      \n \n\n\n\n\n              \n        26,384 open jobs\n      \n            \n \n\n\n\n\n\n                \n            Show more jobs like this\n          \n              \n\n\n                \n            Show fewer jobs like this\n          \n              \n\n\n\n\n\n\n\n              Funding"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 32,
    "content": "Show fewer jobs like this\n          \n              \n\n\n\n\n\n\n\n              Funding\n            \n\n\n\n\n                  LangChain\n                \n\n                    2 total rounds\n                  \n\n\n\n                Last Round\n              \n\n                    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      Series A\n  \n                    \n                      Mar 15, 2024\n                    \n\n\n                    External Crunchbase Link for last round of funding\n                  \n\n                    US$ 25.0M\n                \n\n\n\n                Investors\n              \n\n\n      \n  Sequoia Capital\n\n \n \n\n              See more info on crunchbase"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 33,
    "content": "Investors\n              \n\n\n      \n  Sequoia Capital\n\n \n \n\n              See more info on crunchbase\n\n\n\n\n\n\n\n\n\n\n\n                \n          \n            More searches\n        \n              \n\n\n                \n          \n            More searches\n        \n              \n\n\n\n\n\n\n \n\n\n                Engineer jobs\n              \n\n\n\n                Analyst jobs\n              \n\n\n\n                Scientist jobs\n              \n\n\n\n                Software Engineer jobs\n              \n\n\n\n                Developer jobs\n              \n\n\n\n                Intern jobs\n              \n\n\n\n                Associate jobs\n              \n\n\n\n                Machine Learning Engineer jobs\n              \n\n\n\n                Vice President jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 34,
    "content": "Machine Learning Engineer jobs\n              \n\n\n\n                Vice President jobs\n              \n\n\n\n                Account Executive jobs\n              \n\n\n\n                Project Manager jobs\n              \n\n\n\n                Data Analyst jobs\n              \n\n\n\n                Director jobs\n              \n\n\n\n                Manager jobs\n              \n\n\n\n                Principal Product Manager jobs\n              \n\n\n\n                Senior Software Engineer jobs\n              \n\n\n\n                Product Manager jobs\n              \n\n\n\n                Sales Director jobs\n              \n\n\n\n                Enterprise Account Executive jobs\n              \n\n\n\n                Specialist jobs\n              \n\n\n \n\n \n\n\n                President jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 35,
    "content": "Specialist jobs\n              \n\n\n \n\n \n\n\n                President jobs\n              \n\n\n\n                Data Engineer jobs\n              \n\n\n\n                Head of Product jobs\n              \n\n\n\n                Product Designer jobs\n              \n\n\n\n                Chief Executive Officer jobs\n              \n\n\n\n                Consultant jobs\n              \n\n\n\n                Associate Product Manager jobs\n              \n\n\n\n                User Experience Designer jobs\n              \n\n\n\n                Solutions Architect jobs\n              \n\n\n\n                Data Science Specialist jobs\n              \n\n\n\n                Senior Product Manager jobs\n              \n\n\n\n                Site Reliability Engineer jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 36,
    "content": "Senior Product Manager jobs\n              \n\n\n\n                Site Reliability Engineer jobs\n              \n\n\n\n                Python Developer jobs\n              \n\n\n\n                Marketing Manager jobs\n              \n\n\n\n                Junior Developer jobs\n              \n\n\n\n                User Experience Researcher jobs\n              \n\n\n\n                Full Stack Engineer jobs\n              \n\n\n\n                Director of Product Management jobs\n              \n\n\n\n                Sales Manager jobs\n              \n\n\n\n                Quality Assurance Analyst jobs\n              \n\n\n \n\n \n\n\n                Lead Scientist jobs\n              \n\n\n\n                Software Intern jobs\n              \n\n\n\n                Account Manager jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 37,
    "content": "Software Intern jobs\n              \n\n\n\n                Account Manager jobs\n              \n\n\n\n                Director of Engineering jobs\n              \n\n\n\n                Customer Service Specialist jobs\n              \n\n\n\n                Salesperson jobs\n              \n\n\n\n                Solutions Engineer jobs\n              \n\n\n\n                Business Development Representative jobs\n              \n\n\n\n                Group Product Manager jobs\n              \n\n\n\n                Vice President of Product Management jobs\n              \n\n\n\n                Science Manager jobs\n              \n\n\n\n                Mechanical Engineer jobs\n              \n\n\n\n                Head of Partnerships jobs\n              \n\n\n\n                Student jobs"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 38,
    "content": "Head of Partnerships jobs\n              \n\n\n\n                Student jobs\n              \n\n\n\n                Vice President of Sales jobs\n              \n\n\n\n                Senior Manager jobs\n              \n\n\n\n                Recruiter jobs\n              \n\n\n\n                Finance Officer jobs\n              \n\n\n\n                Head jobs\n              \n\n\n\n                Engineering Manager jobs\n              \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nLinkedIn\n\n¬© 2025\n\n\n\n          \n          About\n        \n        \n\n\n\n          \n          Accessibility\n        \n        \n\n\n\n          \n          User Agreement\n        \n        \n\n\n\n          \n          Privacy Policy\n        \n        \n\n\n\n\n          \n          Cookie Policy"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 39,
    "content": "Privacy Policy\n        \n        \n\n\n\n\n          \n          Cookie Policy\n        \n        \n\n\n\n          \n          Copyright Policy\n        \n        \n\n\n\n          \n          Brand Policy\n        \n        \n\n\n\n          \n            Guest Controls\n          \n        \n\n\n\n          \n          Community Guidelines\n        \n        \n\n\n\n\n\n\n\n\n\n                ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n            \n\n\n\n\n                ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\n            \n\n\n\n\n                ƒåe≈°tina (Czech)\n            \n\n\n\n\n                Dansk (Danish)\n            \n\n\n\n\n                Deutsch (German)\n            \n\n\n\n\n                ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\n            \n\n\n\n\nEnglish (English)\n\n\n\n\n\n                Espa√±ol (Spanish)\n            \n\n\n\n\n                ŸÅÿßÿ±ÿ≥€å (Persian)"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 40,
    "content": "English (English)\n\n\n\n\n\n                Espa√±ol (Spanish)\n            \n\n\n\n\n                ŸÅÿßÿ±ÿ≥€å (Persian)\n            \n\n\n\n\n                Suomi (Finnish)\n            \n\n\n\n\n                Fran√ßais (French)\n            \n\n\n\n\n                ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n            \n\n\n\n\n                Magyar (Hungarian)\n            \n\n\n\n\n                Bahasa Indonesia (Indonesian)\n            \n\n\n\n\n                Italiano (Italian)\n            \n\n\n\n\n                ◊¢◊ë◊®◊ô◊™ (Hebrew)\n            \n\n\n\n\n                Êó•Êú¨Ë™û (Japanese)\n            \n\n\n\n\n                ÌïúÍµ≠Ïñ¥ (Korean)\n            \n\n\n\n\n                ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\n            \n\n\n\n\n                Bahasa Malaysia (Malay)\n            \n\n\n\n\n                Nederlands (Dutch)\n            \n\n\n\n\n                Norsk (Norwegian)"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 41,
    "content": "Nederlands (Dutch)\n            \n\n\n\n\n                Norsk (Norwegian)\n            \n\n\n\n\n                ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\n            \n\n\n\n\n                Polski (Polish)\n            \n\n\n\n\n                Portugu√™s (Portuguese)\n            \n\n\n\n\n                Rom√¢nƒÉ (Romanian)\n            \n\n\n\n\n                –†—É—Å—Å–∫–∏–π (Russian)\n            \n\n\n\n\n                Svenska (Swedish)\n            \n\n\n\n\n                ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\n            \n\n\n\n\n                ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n            \n\n\n\n\n                Tagalog (Tagalog)\n            \n\n\n\n\n                T√ºrk√ße (Turkish)\n            \n\n\n\n\n                –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n            \n\n\n\n\n                Ti·∫øng Vi·ªát (Vietnamese)\n            \n\n\n\n\n                ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 42,
    "content": "Ti·∫øng Vi·ªát (Vietnamese)\n            \n\n\n\n\n                ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n            \n\n\n\n\n                Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n            \n\n\n\n\n\n            Language\n          \n\n\n\n\n\n \n\n\n\n\n\n\n\n\n              Agree & Join LinkedIn\n            \n\n      By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n                  Sign in to see who you already know at LangChain\n                \n \n\n\n\n \n \n\n\n\n              Sign in\n          \n\n\n\n \n\n\n\n\n\n\n\n                Welcome back\n                          \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n          Email or phone\n        \n\n\n\n\n\n\n\n\n\n          Password\n        \n\n\nShow\n\n\n\n\n\n \n\nForgot password?\n\n\n\n          Sign in"
  },
  {
    "url": "https://www.linkedin.com/company/langchain/",
    "chunk_id": 43,
    "content": "Email or phone\n        \n\n\n\n\n\n\n\n\n\n          Password\n        \n\n\nShow\n\n\n\n\n\n \n\nForgot password?\n\n\n\n          Sign in\n        \n\n\n\n              or\n            \n\n\n\n\n\n      By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n    \n\n \n\n\n\n                New to LinkedIn? Join now\n\n\n\n \n\n\n\n \n\n\n                          or\n                        \n\n\n\n                    New to LinkedIn? Join now\n\n\n      By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy."
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 0,
    "content": "LangSmith Deployment - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith DeploymentGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom"
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 1,
    "content": "assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesLangSmith DeploymentCopy pageCopy pageStart here if you‚Äôre building or operating agent applications. This section is about deploying your application. Need to set up LangSmith infrastructure first? Refer to the Hosting section."
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 2,
    "content": "This section covers how to package, build, and deploy your agents and applications as LangGraph Servers.\nA typical deployment workflow consists of the following steps:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 3,
    "content": "Test locally: Run your application on a local server.\nChoose hosting: Select Cloud, Hybrid, or Self-hosted.\nDeploy your app: Push code or build images to your chosen environment.\nMonitor & manage: Track traces, alerts, and dashboards.\n\nPrerequisites: Before deploying applications, you need a LangSmith instance to deploy to. Choose a hosting option first:\nCloud: Fully managed\nHybrid: Enterprise option for data residency requirements\nSelf-hosted: Full control and data isolation\n\n‚ÄãWhat you‚Äôll learn"
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 4,
    "content": "‚ÄãWhat you‚Äôll learn\n\nConfigure your app for deployment (dependencies, project setup, and monorepo support).\nBuild, deploy, and update LangGraph Servers.\nSecure your deployments with authentication and access control.\nCustomize your server runtime (lifespan hooks, middleware, and routes).\nDebug, observe, and troubleshoot deployed agents using the Studio UI.\n\nGet started with deploymentPackage, build, and deploy your agents and graphs to LangGraph Server.Configure your app\n‚ÄãRelated\n\nLangGraph Server\nApplication structure\nLocal server testing"
  },
  {
    "url": "https://docs.langchain.com/langsmith/deployments",
    "chunk_id": 5,
    "content": "LangGraph Server\nApplication structure\nLocal server testing\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoRun a LangGraph app locallyNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 0,
    "content": "Introducing End-to-End OpenTelemetry Support in LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.\n\nBy LangChain\n3 min read\nMar 26, 2025"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 1,
    "content": "Observability is critical for debugging and optimizing LLM applications ‚Äî but until now, getting a complete view of your system meant juggling multiple tools and formats. Now, LangSmith offers full end-to-end OpenTelemetry support for applications built on LangChain and/or LangGraph.¬†With our OpenTelemetry (OTel) integration, you can standardize tracing across your stack and send traces to LangSmith ‚Äî our testing & observability platform for the agent lifecycle ‚Äî or other observability platforms.¬†Previously, LangSmith supported OpenTelemetry as only a backend trace ingestion format. With this update, we‚Äôre completing the picture by adding native OpenTelemetry support directly into the LangSmith SDK.Why OpenTelemetry for LLM applications?OpenTelemetry (OTel) is an open-source observability"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 2,
    "content": "OpenTelemetry support directly into the LangSmith SDK.Why OpenTelemetry for LLM applications?OpenTelemetry (OTel) is an open-source observability framework¬† that standardizes how telemetry data is collected, exported, and analyzed. As applications grow more complex and distributed, OpenTelemetry provides a consistent way to track performance, understand system behavior, and troubleshoot issues.For LLM applications, observability presents unique challenges. Traditional application monitoring focuses on errors and compliance with expected behaviors ‚Äî however, LLM observability requires understanding multi-step workflows and monitoring dynamic, stochastic outputs with complex evaluation metrics that go beyond simple error rates.OpenTelemetry addresses these challenges by providing a unified,"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 3,
    "content": "stochastic outputs with complex evaluation metrics that go beyond simple error rates.OpenTelemetry addresses these challenges by providing a unified, vendor-neutral standard for instrumentation that works across different languages, frameworks, and backends.How our OpenTelemetry Pipeline WorksWith this update, LangSmith now offers a complete OpenTelemetry pipeline for LLM applications:LangChain instrumentation: Automatically generate detailed traces from your LangChain or LangGraph applicationsLangSmith SDK: Convert and transport these traces through our SDK using OpenTelemetry's standardized formatLangSmith platform: Ingest and visualize traces in a powerful, LLM-specific observability dashboardThis end-to-end integration unlocks several key benefits:Unified observability: View your"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 4,
    "content": "traces in a powerful, LLM-specific observability dashboardThis end-to-end integration unlocks several key benefits:Unified observability: View your entire application stack‚Äîfrom LangChain components to underlying infrastructure‚Äîin a single, cohesive viewDistributed tracing: Follow requests as they move through your microservices architecture, with context propagation ensuring that related spans are linked to the same traceInteroperability: Connect LangSmith with your existing observability tools and infrastructure through the OpenTelemetry standard, including platforms like Datadog, Grafana, and Jaeger.With this integration, you can trace the complete execution path of your LLM applications, from the initial prompt to the final response, with detailed visibility into each step along the"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 5,
    "content": "complete execution path of your LLM applications, from the initial prompt to the final response, with detailed visibility into each step along the way.Getting Started with OpenTelemetry in LangSmith1. Installation‚ÄãInstall the LangSmith package with OpenTelemetry support:pip install \"langsmith[otel]\""
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 6,
    "content": "pip install langchain2. Enable the OpenTelemetry integration‚ÄãYou can enable the OpenTelemetry integration by setting the LANGSMITH_OTEL_ENABLED environment variable:LANGSMITH_OTEL_ENABLED=true\nLANGSMITH_TRACING=true\nLANGSMITH_ENDPOINT=https://api.smith.langchain.com\nLANGSMITH_API_KEY=<your_langsmith_api_key>3. Create a LangChain application with tracing‚ÄãHere's a simple example showing how to use the OpenTelemetry integration with LangChain:import os\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 7,
    "content": "# LangChain will automatically use OpenTelemetry to send traces to LangSmith\n# because the LANGSMITH_OTEL_ENABLED environment variable is set\n\n\n# Create a chain\nprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nmodel = ChatOpenAI()\nchain = prompt | model"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 8,
    "content": "# Run the chain\nresult = chain.invoke({\"topic\": \"programming\"})"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 9,
    "content": "print(result.content)4. View the traces in LangSmith‚ÄãOnce your application runs, you'll see the traces in your LangSmith dashboard like this one.Performance ConsiderationsWhile our end-to-end OpenTelemetry support provides maximum flexibility and interoperability, it comes with slightly higher overhead compared to LangSmith‚Äôs native tracing format.¬†For users that are exclusively using LangSmith as their observability platform, we still recommend our native tracing format for optimal performance. It offers realtime tracing with pending runs, faster ingest speeds, and reduced memory overhead from the sdk.The native LangSmith tracing format has been specifically designed for LLM applications and offers several key advantages. It features significantly reduced overhead with a lower"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 10,
    "content": "format has been specifically designed for LLM applications and offers several key advantages. It features significantly reduced overhead with a lower computational and memory footprint compared to the more general-purpose OpenTelemetry format. Our native format is also custom-tailored for the unique data patterns and volumes found in LLM applications.Try it todayReady to get started tracing your LangChain and LangGraph applications with OpenTelemetry? Check out our full documentation for more details and examples ‚Äî¬†and try out LangSmith for free if you haven't already."
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 11,
    "content": "Tags\nBy LangChain\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nLangChain State of AI 2024 Report\n\n\nBy LangChain\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing OpenTelemetry support for LangSmith\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nEasier evaluations with LangSmith SDK v0.2\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph Platform in beta: New deployment options for scalable agent infrastructure\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nFew-shot prompting to improve tool-calling performance\n\n\nBy LangChain\n8 min read"
  },
  {
    "url": "https://blog.langchain.com/end-to-end-opentelemetry-langsmith/",
    "chunk_id": 12,
    "content": "By LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nFew-shot prompting to improve tool-calling performance\n\n\nBy LangChain\n8 min read\n\n\n\n\n\n\n\n\n\n\n\n\nImproving core tool interfaces and docs in LangChain\n\n\nBy LangChain\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 0,
    "content": "Set up Agent Auth (Beta) - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationAuth & access controlSet up Agent Auth (Beta)Get startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 1,
    "content": "capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesOn this pageInstallationQuickstart1. Initialize the client2. Set up OAuth providers3. Authenticate from an agentIn LangGraph contextOutside LangGraph contextAuth & access controlSet up Agent Auth (Beta)Copy pageEnable secure access from agents to any system using OAuth 2.0 credentials with Agent Auth.Copy pageAgent Auth is in"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 2,
    "content": "up Agent Auth (Beta)Copy pageEnable secure access from agents to any system using OAuth 2.0 credentials with Agent Auth.Copy pageAgent Auth is in Beta and under active development. To provide feedback or use this feature, reach out to the LangChain team."
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 3,
    "content": "‚ÄãInstallation\nInstall the Agent Auth client library from PyPI:\npipuvCopypip install langchain-auth"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 4,
    "content": "‚ÄãQuickstart\n‚Äã1. Initialize the client\nCopyfrom langchain_auth import Client\n\nclient = Client(api_key=\"your-langsmith-api-key\")\n\n‚Äã2. Set up OAuth providers\nBefore agents can authenticate, you need to configure an OAuth provider using the following process:\n\n\nSelect a unique identifier for your OAuth provider to use in LangChain‚Äôs platform (e.g., ‚Äúgithub-local-dev‚Äù, ‚Äúgoogle-workspace-prod‚Äù).\n\n\nGo to your OAuth provider‚Äôs developer console and create a new OAuth application.\n\n\nSet LangChain‚Äôs API as an available callback URL using this structure:\nCopyhttps://api.host.langchain.com/v2/auth/callback/{provider_id}\n\nFor example, if your provider_id is ‚Äúgithub-local-dev‚Äù, use:\nCopyhttps://api.host.langchain.com/v2/auth/callback/github-local-dev"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 5,
    "content": "For example, if your provider_id is ‚Äúgithub-local-dev‚Äù, use:\nCopyhttps://api.host.langchain.com/v2/auth/callback/github-local-dev\n\n\n\nUse client.create_oauth_provider() with the credentials from your OAuth app:\n\n\nCopynew_provider = await client.create_oauth_provider(\n    provider_id=\"{provider_id}\", # Provide any unique ID. Not formally tied to the provider.\n    name=\"{provider_display_name}\", # Provide any display name\n    client_id=\"{your_client_id}\",\n    client_secret=\"{your_client_secret}\",\n    auth_url=\"{auth_url_of_your_provider}\",\n    token_url=\"{token_url_of_your_provider}\",\n)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 6,
    "content": "‚Äã3. Authenticate from an agent\nThe client authenticate() API is used to get OAuth tokens from pre-configured providers. On the first call, it takes the caller through an OAuth 2.0 auth flow.\n‚ÄãIn LangGraph context\nBy default, tokens are scoped to the calling agent using the Assistant ID parameter.\nCopyauth_result = await client.authenticate(\n    provider=\"{provider_id}\",\n    scopes=[\"scopeA\"],\n    user_id=\"your_user_id\" # Any unique identifier to scope this token to the human caller\n)\n\n# Or if you'd like a token that can be used by any agent, set agent_scoped=False\nauth_result = await client.authenticate(\n    provider=\"{provider_id}\",\n    scopes=[\"scopeA\"],\n    user_id=\"your_user_id\",\n    agent_scoped=False\n)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 7,
    "content": "During execution, if authentication is required, the SDK will throw an interrupt. The agent execution pauses and presents the OAuth URL to the user:\n\nAfter the user completes OAuth authentication and we receive the callback from the provider, they will see the auth success page.\n\nThe agent then resumes execution from the point it left off at, and the token can be used for any API calls. We store and refresh OAuth tokens so that future uses of the service by either the user or agent do not require an OAuth flow.\nCopytoken = auth_result.token"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 8,
    "content": "‚ÄãOutside LangGraph context\nProvide the auth_url to the user for out-of-band OAuth flows.\nCopy# Default: user-scoped token (works for any agent under this user)\nauth_result = await client.authenticate(\n    provider=\"{provider_id}\",\n    scopes=[\"scopeA\"],\n    user_id=\"your_user_id\"\n)\n\nif auth_result.needs_auth:\n    print(f\"Complete OAuth at: {auth_result.auth_url}\")\n    # Wait for completion\n    completed_auth = await client.wait_for_completion(auth_result.auth_id)\n    token = completed_auth.token\nelse:\n    token = auth_result.token"
  },
  {
    "url": "https://docs.langchain.com/langsmith/agent-auth",
    "chunk_id": 9,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoDocument API authentication in OpenAPIPreviousHow to add custom lifespan eventsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#ux",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#ai-agent-adoption",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/customers",
    "chunk_id": 0,
    "content": "Customer Stories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upCustomers choose LangChain to build reliable agents\n\n\nTrusted byFEATURED STORIESHear from engineers who trust LangChain products.\n\nHow Morningstar saves 30% of analysts' time with Mo, its AI Research AssistantRead the story"
  },
  {
    "url": "https://www.langchain.com/customers",
    "chunk_id": 1,
    "content": "How Morningstar saves 30% of analysts' time with Mo, its AI Research AssistantRead the story\n\n\n\n\nHow Outshift by Cisco boosted productivity 10x with their AI Platform EngineerRead the story\n\n\n\n\nHow Rakuten speeds up time-to-market for business operationsRead the story\n\n\n\n\nHow Modern Treasury built a financial operations AI agentRead the story\n\n\n\n\nHow Pigment built their AI business planning platformRead the story\n\n\n\n\nHow City of Hope saved clinicians 1000+ hours with HopeLLMRead the story\n\n\nMore agent engineer storiesCustomer StoriesAllFinancial ServicesAI / MLB2B SaaSHealthcareReal EstateInsuranceTelecomTransportationCybersecurityOtherThank you! Your submission has been received!Oops! Something went wrong while submitting the form.Financial Services"
  },
  {
    "url": "https://www.langchain.com/customers",
    "chunk_id": 2,
    "content": "Klarna's AI Assistant speeds up customer resolution with LangSmith & LangGraphCybersecurity\n\n\nTrellix cuts log parsing time from days to minutes with LangSmith & LangGraphAI / ML\n\n\nHow Replit uses LangSmith for improved agent performanceTransportation\n\n\nHow C.H. Robinson transformed logistics shipments with LangSmith & LangGraphB2B SaaSCybersecurity\n\n\nElastic's AI Assistant leverages LangChain ecosystem to detect security threatsB2B SaaS\n\n\nHow Podium reduced engineering intervention by 90% with LangSmithHealthcare\n\n\nVizient provides reliable insights for healthcare providers with LangGraph & LangSmithReal Estate\n\n\nAppFolio's copilot saves property managers 10+ hours a weekAI / ML\n\n\nUnify launches agents for account qualification with LangGraph & LangSmithAI / ML"
  },
  {
    "url": "https://www.langchain.com/customers",
    "chunk_id": 3,
    "content": "Unify launches agents for account qualification with LangGraph & LangSmithAI / ML\n\n\nLovable scales its agent to $25M ARR in 4 months, gaining visibility with LangSmithTelecom\n\n\nVodafone transforms data operations with AI monitoringFinancial Services"
  },
  {
    "url": "https://www.langchain.com/customers",
    "chunk_id": 4,
    "content": "Vodafone transforms data operations with AI monitoringFinancial Services\n\n\nDun & Bradstreet uses LangSmith to empower clients with business dataReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#cognitive-architecture",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 0,
    "content": "Deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangSmith DeploymentDeploy agents fastLangSmith Deployment provides purpose-built infrastructure for deploying and managing long-running agent workloads.Sign UpGet a demoHelping top teams ship reliable agentsDeploy agents without the infrastructure complexityDeploy agents with 1-click. Get robust APIs for custom checkpointing, memory, and conversation threads ‚Äî no need to build custom state stores or endpoints. Instantly roll back to any previous version if needed.See how to deploy an agent"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 2,
    "content": "Handle production workloads at scaleOpinionated task queues enable horizontal scaling to handle enterprise-scale traffic, bursty workloads, and long-running agents without performance bottlenecks.Learn about the task queues\n\n\nStandardize your agent developmentWith LangSmith Deployment, you can also connect tools, configure agents, and collaborate across teams to move from development to production.Agent StudioDebug agents visually with LangSmith Studio, our agent IDE. Interact with agents in real-time to speed up development and quickly spot issues.Try out LangSmith Studio\n\n\nAgent AuthControl what your agent can access with granular authentication. Manage tool permissions and data access to meet enterprise requirements.Try out Agent Authorization beta"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 3,
    "content": "Assistants APIReuse existing agents for new use cases by modifying agent configurations and prompts -- no need to rebuild from scatch.Modify agents with Assistants API\n\n\nAgent registryCentrally manage all agents via the agent registry, allowing teammates to discover and share agents across the organization.Watch an overview of the agent registry \n\n\nReady to deploy your agent to production?Deploy your agent with production-ready infrastructure. Get started in minutes with 1-click deployments, built-in APIs, and autoscaling to handle enterprise-scale traffic.Sign up for freeBook a demoResources for LangSmith DeploymentBLOGWhy agent infrastructure matters DOCSAgent deployment optionsblogLangGraph PlatformGAFAQs for LangSmith DeploymentWas LangSmith Deployment renamed?"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 4,
    "content": "Yes, LangGraph Platform has been renamed to LangSmith Deployment as of October 2025.How is LangSmith Deployment different from LangGraph?\n\nLangGraph is the open-source framework for building stateful, orchestrated agent workflows. LangSmith Deployment is the managed service for running those agents at scale in production.Do I have to use LangGraph to deploy my agents?\n\nNo ‚Äî you can deploy agents built with any framework. Check out the docs to get started.What are my options if I want to deploy my agent?"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 5,
    "content": "You can deploy your LangGraph application using LangSmith Deployment, available in the LangSmith Plus and Enterprise plans. which has the following agent deployment options: Cloud:¬†Fully managed and hosted as part of LangSmith (our unified observability¬†& evals platform).¬†Deploy quickly, with automatic updates and zero maintenance. ‚ÄçHybrid (SaaS control plane, self-hosted data plane). No data leaves your VPC. Provisioning and scaling is managed as a service.‚ÄçFully Self-Hosted: Deploy LangGraph entirely on your own infrastructure.‚ÄçProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay"
  },
  {
    "url": "http://langchain.com/langsmith/deployment",
    "chunk_id": 6,
    "content": "StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 0,
    "content": "Deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangSmith DeploymentDeploy agents fastLangSmith Deployment provides purpose-built infrastructure for deploying and managing long-running agent workloads.Sign UpGet a demoHelping top teams ship reliable agentsDeploy agents without the infrastructure complexityDeploy agents with 1-click. Get robust APIs for custom checkpointing, memory, and conversation threads ‚Äî no need to build custom state stores or endpoints. Instantly roll back to any previous version if needed.See how to deploy an agent"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 2,
    "content": "Handle production workloads at scaleOpinionated task queues enable horizontal scaling to handle enterprise-scale traffic, bursty workloads, and long-running agents without performance bottlenecks.Learn about the task queues\n\n\nStandardize your agent developmentWith LangSmith Deployment, you can also connect tools, configure agents, and collaborate across teams to move from development to production.Agent StudioDebug agents visually with LangSmith Studio, our agent IDE. Interact with agents in real-time to speed up development and quickly spot issues.Try out LangSmith Studio\n\n\nAgent AuthControl what your agent can access with granular authentication. Manage tool permissions and data access to meet enterprise requirements.Try out Agent Authorization beta"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 3,
    "content": "Assistants APIReuse existing agents for new use cases by modifying agent configurations and prompts -- no need to rebuild from scatch.Modify agents with Assistants API\n\n\nAgent registryCentrally manage all agents via the agent registry, allowing teammates to discover and share agents across the organization.Watch an overview of the agent registry \n\n\nReady to deploy your agent to production?Deploy your agent with production-ready infrastructure. Get started in minutes with 1-click deployments, built-in APIs, and autoscaling to handle enterprise-scale traffic.Sign up for freeBook a demoResources for LangSmith DeploymentBLOGWhy agent infrastructure matters DOCSAgent deployment optionsblogLangGraph PlatformGAFAQs for LangSmith DeploymentWas LangSmith Deployment renamed?"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 4,
    "content": "Yes, LangGraph Platform has been renamed to LangSmith Deployment as of October 2025.How is LangSmith Deployment different from LangGraph?\n\nLangGraph is the open-source framework for building stateful, orchestrated agent workflows. LangSmith Deployment is the managed service for running those agents at scale in production.Do I have to use LangGraph to deploy my agents?\n\nNo ‚Äî you can deploy agents built with any framework. Check out the docs to get started.What are my options if I want to deploy my agent?"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 5,
    "content": "You can deploy your LangGraph application using LangSmith Deployment, available in the LangSmith Plus and Enterprise plans. which has the following agent deployment options: Cloud:¬†Fully managed and hosted as part of LangSmith (our unified observability¬†& evals platform).¬†Deploy quickly, with automatic updates and zero maintenance. ‚ÄçHybrid (SaaS control plane, self-hosted data plane). No data leaves your VPC. Provisioning and scaling is managed as a service.‚ÄçFully Self-Hosted: Deploy LangGraph entirely on your own infrastructure.‚ÄçProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay"
  },
  {
    "url": "https://www.langchain.com/langsmith/deployment",
    "chunk_id": 6,
    "content": "StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 0,
    "content": "LangChain Overview - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain OverviewLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingAdvanced usageMiddlewareGuardrailsStructured outputRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 1,
    "content": "memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy pageLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 2,
    "content": "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency."
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 3,
    "content": "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n‚Äã Install\npipuvCopypip install -U langchain"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 4,
    "content": "‚Äã Create an agent\nCopy# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 5,
    "content": "‚Äã Core benefits"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 6,
    "content": "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 7,
    "content": "LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/concepts/#agents",
    "chunk_id": 8,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoWhat's new in v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEngineer reliable agentsShip agents to production with LangChain's comprehensive platform for agent engineering.Request a demoSign Up"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 1,
    "content": "LangChain products power top engineering teams, from AI startups to global enterprisesVisibility &¬†controlSee exactly what's happening at every step of your agent. Steer your agent to accomplish critical tasks the way you intended.Fast iterationRapidly move through build, test, deploy, learn, repeat with workflows across the entire agent engineering lifecycle.Durable performanceShip at scale with agent infrastructure designed for long-running workloads and human oversight.Model neutralSwap models, tools, and databases without rewriting your app. Future-proof your stack as AI advances with no vendor lock-in.Your agent engineering stackOpen Source FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 2,
    "content": "FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph puts you in control with low-level primitives to build custom agent workflows.Agent Engineering PlatformLangSmithObservabilityEvaluationDeploymentObservabilityEvaluationDeploymentSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 3,
    "content": "Improve agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 4,
    "content": "Build agents your way, with templates or custom controlBring your own frameworkLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK¬†to gain visibility into your agent interactions, whether you use LangChain's frameworks or not.Open Source Frameworks¬†Bring your ownAgent Engineering PlatformLangSmithObservabilityEvaluationDeploymentOpen Source FrameworksBuild agents your way, with templates or custom controlLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraph puts you in control with low-level primitives to build custom agent workflows.LangSmith Agent Engineering PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 5,
    "content": "PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 6,
    "content": "LangSmith Agent Engineering PlatformImprove agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nLangSmith Agent Engineering PlatformDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 7,
    "content": "CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access to information and tools in a compliant manner so they can perform their best.Customer SupportImprove the speed and efficiency of support teams that handle customer requests.ResearchSynthesize data, summarize sources, and uncover insights faster for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way."
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 8,
    "content": "Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\n\n\nFinancial ServicesKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\n\n\nB2B SaaSElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\n\n\nAI/MLReplit's AI Agent serves 30+ million developers. Their AI engineers rely on LangSmith to debug complex traces."
  },
  {
    "url": "https://www.langchain.com",
    "chunk_id": 9,
    "content": "Learn alongside the 1 million+ practitioners who are pushing the industry forward90MMonthly downloads100k+GitHub stars#1Downloaded agent framework1000IntegrationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 0,
    "content": "Providers | ü¶úÔ∏èüîó LangChain"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 1,
    "content": "Skip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreProvidersAbsoAcreomActiveloop Deep LakeADS4GPTsAgentQLAI21 LabsAimAI/ML API LLMAINetworkAirbyteAirtableAlchemyAleph AlphaAlibaba CloudAnalyticDBAnchor BrowserAnnoyAnthropicAnyscaleApache Software FoundationApache DorisApifyAppleArangoDBArceeArcGISArgillaArizeArthurArxivAscendAskNewsAssemblyAIAstra DBAtlasAwaDBAWSAZLyricsAzure AIBAAIBagelBagelDBBaichuanBaiduBananaBasetenBeamBeautiful SoupBibTeXBiliBiliBittensorBlackboardbookend.aiBoxBrave SearchBreebs"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 2,
    "content": "AIBAAIBagelBagelDBBaichuanBaiduBananaBasetenBeamBeautiful SoupBibTeXBiliBiliBittensorBlackboardbookend.aiBoxBrave SearchBreebs (Open Knowledge)Bright DataBrowserbaseBrowserlessByteDanceCassandraCerebrasCerebriumAIChaindeskChromaClarifaiClearMLClickHouseClickUpCloudflareClovaCnosDBCogneeCogniSwitchCohereCollege ConfidentialCometConfident AIConfluenceConneryContextContextual AICouchbaseCozeCrateDBC TransformersCTranslate2CubeDappierDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODataheraldDedocDeepInfraDeeplakeDeepSeekDeepSparseDellDiffbotDingoDBDiscordDiscord (community loader)DocArrayDoclingDoctranDocugamiDocusaurusDriaDropboxDuckDBDuckDuckGo SearchE2BEden AIElasticsearchElevenLabsEmbedchainEpsillaEtherscanEverly AIEverNoteExaFacebook - MetaFalkorDBFaunaFeatherless"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 3,
    "content": "SearchE2BEden AIElasticsearchElevenLabsEmbedchainEpsillaEtherscanEverly AIEverNoteExaFacebook - MetaFalkorDBFaunaFeatherless AIFiddlerFigmaFireCrawlFireworks AIFlyteFMP Data (Financial Data Prep)Forefront AIFriendli AISmabblerGelGeopandasGitGitBookGitHubGitLabGOATGoldenGoodfireBigtableGoogleSerper - Google Search APIGooseAIGPT4AllGradientDigitalOcean GradientGraph RAGGraphsignalGreenNodeGrobidGroqGutenbergHacker NewsHazy ResearchHeliconeHologresHTML to textHuaweiHugging FaceHyperbrowserIBMIEIT SystemsiFixitiFlytekIMSDbInfinispan VSInfinityInfinoIntelIuguJaguarJavelin AI GatewayJenkinsJina AIJohnsnowlabsJoplinKDB.AIKineticaKoboldAIKonkoKoNLPYK√πzuLabel StudiolakeFSLanceDBOracleAI Vector SearchLangChain Decorators ‚ú®LangFair: Use-Case Level LLM Bias and Fairness AssessmentsLangfuse"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 4,
    "content": "StudiolakeFSLanceDBOracleAI Vector SearchLangChain Decorators ‚ú®LangFair: Use-Case Level LLM Bias and Fairness AssessmentsLangfuse ü™¢LanternLindormLinkupLiteLLMLlamaIndexLlama.cppLlamaEdgellamafileLLMonitorLocalAILog10MariaDBMariTalkMarqoMediaWikiDumpMeilisearchMemcachedMemgraphMetalMicrosoftMilvusMindsDBMinimaxMistralAIMLflow AI Gateway for LLMsMLflowMLXModalModelScopeModern TreasuryMomentoMongoDBMongoDB AtlasMotherduckMot√∂rheadMyScaleNAVERNebiusNeo4jNetmindNimbleNLPCloudNomicNotion DBNucliaNVIDIAObsidianOceanBaseOracle Cloud Infrastructure (OCI)OctoAIOllamaOntotext GraphDBOpenAIOpenGradientOpenLLMOpenSearchOpenWeatherMapOutlineOutlinesOxylabsPandasPaymanAIPebbloPermitPerplexityPetalsPostgres EmbeddingPGVectorPineconePipelineAIPipeshiftPortkeyPredibasePrediction"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 5,
    "content": "EmbeddingPGVectorPineconePipelineAIPipeshiftPortkeyPredibasePrediction GuardPremAISWI-PrologPromptLayerPsychicPubMedPullMd LoaderPygmalionAIPyMuPDF4LLMQdrantRAGatouillerank_bm25Ray ServeRebuffRecallioRedditRedisRemembrallReplicateRoamSema4 (fka Robocorp)RocksetRunhouseRunpodRWKV-4SalesforceSambaNovaSAPScrapeGraph AIScrapelessScraperAPISearchApiSearxNG Search APISemaDBSerpAPIShale Protocollangchain-siliconflowSingleStore Integrationscikit-learnSlackSnowflakespaCySparkSparkLLMSpreedlySQLiteStack ExchangeStarRocksStochasticAIStreamlitStripeSupabase (Postgres)SuperlinkedSurrealDBNebulaTableauTaigaTairTavilyTelegramTencentTensorFlow DatasetsTensorlakeTiDBTigerGraphTigrisTiloresTimbrTogether AI2MarkdownMCP"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 6,
    "content": "DatasetsTensorlakeTiDBTigerGraphTigrisTiloresTimbrTogether AI2MarkdownMCP ToolboxTranswarpTrelloTrubricsTrueFoundryTruLensTwitterTypesenseUnstructuredUpstageupstashUpTrainUSearchValtheraValyu Deep SearchVDMSVearchVectaraVectorizeVespavliteVoyageAIWeights & BiasesWeights & Biases tracingWeights & Biases trackingWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriter, Inc.xAIXataXorbits Inference (Xinference)YahooYandexYDBYeager.aiYellowbrick01.AIYouYouTubeZenRowsZepzeusdbZhipu AIZillizZoteroComponentsChat modelsChat modelsAbsoAI21 LabsAI/ML APIAlibaba Cloud PAI EASAnthropic[Deprecated] Experimental Anthropic Tools WrapperAnyscaleAzureAIChatCompletionsModelAzure OpenAIAzure ML EndpointBaichuan ChatBaidu QianfanAWS BedrockCerebrasCloudflareWorkersAICohereContextualAICoze ChatDappier"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 7,
    "content": "OpenAIAzure ML EndpointBaichuan ChatBaidu QianfanAWS BedrockCerebrasCloudflareWorkersAICohereContextualAICoze ChatDappier AIDatabricksDeepInfraDeepSeekEden AIErnie Bot ChatEverlyAIFeatherless AIFireworksChatFriendliGoodfireGoogle GeminiGoogle Cloud Vertex AIGPTRouterDigitalOcean GradientGreenNodeGroqChatHuggingFaceIBM watsonx.aiJinaChatKineticaKonkoLiteLLMLlama 2 ChatLlama APILlamaEdgeLlama.cppmaritalkMiniMaxMistralAIMLXModelScopeMoonshotNaverNebiusNetmindNVIDIA AI EndpointsChatOCIModelDeploymentOCIGenAIChatOctoAIOllamaOpenAIOutlinesPerplexityPipeshiftChatPredictionGuardPremAIPromptLayer ChatOpenAIQwenQwen QwQRekaRunPod Chat ModelSambaNovaCloudSambaStudioChatSeekrFlowSnowflake CortexsolarSparkLLM ChatNebula (Symbl.ai)Tencent HunyuanTogetherTongyi QwenUpstagevectaravLLM ChatVolc Engine"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 8,
    "content": "CortexsolarSparkLLM ChatNebula (Symbl.ai)Tencent HunyuanTogetherTongyi QwenUpstagevectaravLLM ChatVolc Engine MaasChat WriterxAIXinferenceYandexGPTChatYIYuan2.0ZHIPU AIRetrieversRetrieversActiveloop Deep MemoryAmazon KendraArceeArxivAskNewsAzure AI SearchBedrock (Knowledge Bases)BM25BoxBREEBS (Open Knowledge)ChaindeskChatGPT pluginCogneeCohere rerankerCohere RAGContextual AI RerankerDappierDocArrayDriaElasticSearch BM25ElasticsearchEmbedchainFlashRank rerankerFleet AI ContextGalaxiaGoogle DriveGoogle Vertex AI SearchGraph RAGGreenNodeIBM watsonx.aiJaguarDB Vector DatabaseKay.aiKinetica Vectorstore based RetrieverkNNLinkupSearchRetrieverLLMLingua Document CompressorLOTR (Merger Retriever)MetalNanoPQ (Product Quantization)NebiusneedleNimbleOutlinePermitPinecone Hybrid SearchPinecone"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 9,
    "content": "Document CompressorLOTR (Merger Retriever)MetalNanoPQ (Product Quantization)NebiusneedleNimbleOutlinePermitPinecone Hybrid SearchPinecone RerankPubMedQdrant Sparse VectorRAGatouilleRePhraseQueryRememberizerSEC filingSelf-querying retrieversSuperlinkedRetrieverSuperlinkedRetriever ExamplesSVMTavilySearchAPITF-IDF**NeuralDB**ValyuContextVectorizeVespaWikipediaYou.comZep CloudZep Open SourceZilliz Cloud PipelineZoteroTools/ToolkitsToolsADS4GPTsAgentQLAINetwork ToolkitAlpha VantageAmadeus ToolkitAnchor BrowserApify ActorArXivAskNewsAWS LambdaAzure AI Services ToolkitAzure Cognitive Services ToolkitAzure Container Apps dynamic sessionsShell (bash)Bearly Code InterpreterBing SearchBrave SearchBrightDataWebScraperAPIBrightDataSERPBrightDataUnlockerCassandra Database ToolkitCDPChatGPT"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 10,
    "content": "(bash)Bearly Code InterpreterBing SearchBrave SearchBrightDataWebScraperAPIBrightDataSERPBrightDataUnlockerCassandra Database ToolkitCDPChatGPT PluginsClickUp ToolkitCogniswitch ToolkitCompass DeFi ToolkitConnery Toolkit and ToolsDall-E Image GeneratorDappierDatabricks Unity Catalog (UC)DataForSEODataheraldDuckDuckGo SearchDiscordE2B Data AnalysisEden AIElevenLabs Text2SpeechExa SearchFile SystemFinancialDatasets ToolkitFMP DataGithub ToolkitGitlab ToolkitGmail ToolkitGOATGolden QueryGoogle BooksGoogle Calendar ToolkitGoogle Cloud Text-to-SpeechGoogle DriveGoogle FinanceGoogle ImagenGoogle JobsGoogle LensGoogle PlacesGoogle ScholarGoogle SearchGoogle SerperGoogle TrendsGradioGraphQLHuggingFace Hub ToolsHuman as a toolHyperbrowser Browser Agent ToolsHyperbrowser Web Scraping ToolsIBM"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 11,
    "content": "SearchGoogle SerperGoogle TrendsGradioGraphQLHuggingFace Hub ToolsHuman as a toolHyperbrowser Browser Agent ToolsHyperbrowser Web Scraping ToolsIBM watsonx.aiIFTTT WebHooksInfobipIonic Shopping ToolJenkinsJina SearchJira ToolkitJSON ToolkitLemon AgentLinkupSearchToolMemgraphMemorizeMojeek SearchMultiOn ToolkitNASA ToolkitNaver SearchNuclia UnderstandingNVIDIA Riva: ASR and TTSOffice365 ToolkitOpenAPI ToolkitNatural Language API ToolkitsOpenGradientOpenWeatherMapOracle AI Vector Search: Generate SummaryOxylabsPandas DataframePassio NutritionAIPaymanAIPermitPlayWright Browser ToolkitPolygon IO Toolkit and ToolsPowerBI ToolkitPrologPubMedPython REPLReddit SearchRequests ToolkitRiza Code InterpreterRobocorp ToolkitSalesforceSceneXplainScrapeGraphScrapelessScrapelessScrapelessLangChain ‚Äì"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 12,
    "content": "REPLReddit SearchRequests ToolkitRiza Code InterpreterRobocorp ToolkitSalesforceSceneXplainScrapeGraphScrapelessScrapelessScrapelessLangChain ‚Äì ScraperAPISearchApiSearxNG SearchSemantic Scholar API ToolSerpAPISlack ToolkitSpark SQL ToolkitSQLDatabase ToolkitStackExchangeSteam ToolkitStripeTableauTaigaTavily ExtractTavily SearchTilorestimbrMCP Toolbox for DatabasesTwilioUpstageValtheraValyuContextVectaraWikidataWikipediaWolfram AlphaWriter ToolsYahoo Finance NewsYou.com SearchYouTubeZapier Natural Language ActionsZenGuard AIZenRowsUniversalScraperDocument loadersDocument loadersacreomAgentQLLoaderAirbyteLoaderAirbyte CDK (Deprecated)Airbyte Gong (Deprecated)Airbyte Hubspot (Deprecated)Airbyte JSON (Deprecated)Airbyte Salesforce (Deprecated)Airbyte Shopify (Deprecated)Airbyte Stripe"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 13,
    "content": "Gong (Deprecated)Airbyte Hubspot (Deprecated)Airbyte JSON (Deprecated)Airbyte Salesforce (Deprecated)Airbyte Shopify (Deprecated)Airbyte Stripe (Deprecated)Airbyte Typeform (Deprecated)Airbyte Zendesk Support (Deprecated)AirtableAlibaba Cloud MaxComputeAmazon TextractApify DatasetArcGISArxivLoaderAssemblyAI Audio TranscriptsAstraDBAsync ChromiumAsyncHtmlAthenaAWS S3 DirectoryAWS S3 FileAZLyricsAzure AI DataAzure Blob Storage ContainerAzure Blob Storage FileAzure AI Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBoxBrave SearchBrowserbaseBrowserlessBSHTMLLoaderCassandraChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCouchbaseCSVCube Semantic LayerDatadog"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 14,
    "content": "DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCouchbaseCSVCube Semantic LayerDatadog LogsDedocDiffbotDiscordDoclingDocugamiDocusaurusDropboxDuckDBEmailEPubEtherscanEverNoteexample_dataFacebook ChatFaunaFigmaFireCrawlGeopandasGitGitBookGitHubGlue CatalogGoogle AlloyDB for PostgreSQLGoogle BigQueryGoogle BigtableGoogle Cloud SQL for SQL serverGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle Firestore in Datastore ModeGoogle DriveGoogle El Carro for Oracle WorkloadsGoogle Firestore (Native Mode)Google Memorystore for RedisGoogle SpannerGoogle Speech-to-Text Audio TranscriptsGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetHyperbrowserLoaderiFixitImagesImage"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 15,
    "content": "Speech-to-Text Audio TranscriptsGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetHyperbrowserLoaderiFixitImagesImage captionsIMSDbIuguJoplinJSONLoaderJupyter NotebookKineticalakeFSLangSmithLarkSuite (FeiShu)LLM SherpaMastodonMathPixPDFLoaderMediaWiki DumpMerge Documents LoadermhtmlMicrosoft ExcelMicrosoft OneDriveMicrosoft OneNoteMicrosoft PowerPointMicrosoft SharePointMicrosoft WordNear BlockchainModern TreasuryMongoDBNeedle Document LoaderNews URLNotion DB 2/2NucliaObsidianOpen Document Format (ODT)Open City DataOracle Autonomous DatabaseOracle AI Vector Search: Document ProcessingOrg-modeOutline Document LoaderOxylabsPandas DataFrameparsersPDFMinerLoaderPDFPlumberPebblo Safe DocumentLoaderPolars DataFrameDell PowerScale Document"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 16,
    "content": "Document LoaderOxylabsPandas DataFrameparsersPDFMinerLoaderPDFPlumberPebblo Safe DocumentLoaderPolars DataFrameDell PowerScale Document LoaderPsychicPubMedPullMdLoaderPyMuPDFLoaderPyMuPDF4LLMPyPDFDirectoryLoaderPyPDFium2LoaderPyPDFLoaderPySparkQuipReadTheDocs DocumentationRecursive URLRedditRoamRocksetrspaceRSS FeedsRSTscrapflyScrapingAntSingleStoreSitemapSlackSnowflakeSource CodeSpiderSpreedlyStripeSubtitleSurrealDBTelegramTencent COS DirectoryTencent COS FileTensorFlow DatasetsTiDB2MarkdownTOMLTrelloTSVTwitterUnstructuredUnstructuredMarkdownLoaderUnstructuredPDFLoaderUpstageURLVsdxWeatherWebBaseLoaderWhatsApp ChatWikipediaUnstructuredXMLLoaderXorbits Pandas DataFrameYouTube audioYouTube transcriptsYoutubeLoaderDLYuqueZeroxPDFLoaderVector storesVector storesActiveloop Deep LakeAlibaba"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 17,
    "content": "Pandas DataFrameYouTube audioYouTube transcriptsYoutubeLoaderDLYuqueZeroxPDFLoaderVector storesVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyApache DorisApertureDBAstra DB Vector StoreAtlasAwaDBAzure Cosmos DB Mongo vCoreAzure Cosmos DB No SQLAzure AI SearchBagelBagelDBBaidu Cloud ElasticSearch VectorSearchBaidu VectorDBGoogle BigtableApache CassandraChromaClarifaiClickHouseCloudflareVectorizeCouchbaseDashVectorDatabricksIBM Db2 Vector Store and Vector SearchDingoDBDocArray HnswSearchDocArray InMemorySearchAmazon Document DBDuckDBChina Mobile ECloud ElasticSearch VectorSearchElasticsearchEpsillaFaissFaiss (Async)FalkorDBVectorStoreGelGoogle AlloyDB for PostgreSQLGoogle BigQuery Vector SearchGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLFirestoreGoogle"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 18,
    "content": "AlloyDB for PostgreSQLGoogle BigQuery Vector SearchGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLFirestoreGoogle Memorystore for RedisGoogle SpannerGoogle Vertex AI Feature StoreGoogle Vertex AI Vector SearchHippoHologresInfinispanJaguar Vector DatabaseKDB.AIKineticaLanceDBLanternLindormLLMRailsManticoreSearch VectorStoreMariaDBMarqoMeilisearchAmazon MemoryDBMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOceanbaseopenGaussOpenSearchOracle AI Vector Search: Vector StorePathwayPostgres EmbeddingPGVecto.rsPGVectorPGVectorStorePineconePinecone (sparse)QdrantRedisRelytRocksetSAP HANA Cloud Vector EngineScaNNSemaDBSingleStorescikit-learnSQLiteVecSQLite-VSSSQLServerStarRocksSupabase (Postgres)SurrealDBVectorStoreTablestoreTairTencent Cloud"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 19,
    "content": "EngineScaNNSemaDBSingleStorescikit-learnSQLiteVecSQLite-VSSSQLServerStarRocksSupabase (Postgres)SurrealDBVectorStoreTablestoreTairTencent Cloud VectorDBThirdAI NeuralDBTiDB VectorTigrisTileDBTimescale Vector (Postgres)TypesenseUpstash VectorUSearchValdVDMSVearchVectaraVespaviking DBvliteWeaviateXataYDBYellowbrickYugabyteDBZepZep Cloud‚ö° ZeusDB Vector StoreZillizEmbedding modelsEmbedding modelsAI/ML API EmbeddingsAleph AlphaAnyscaleascendAwaDBAzureOpenAIBaichuan Text EmbeddingsBaidu QianfanBedrockBGE on Hugging FaceBookend AIClarifaiCloudflare Workers AIClova EmbeddingsCohereDashScopeDatabricksDeepInfraEDEN AIElasticsearchEmbaasERNIEFake EmbeddingsFastEmbed by QdrantFireworksGoogle GeminiGoogle Vertex AIGPT4AllGradientGreenNodeHugging FaceIBM watsonx.aiInfinityInstruct Embeddings on Hugging"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 20,
    "content": "by QdrantFireworksGoogle GeminiGoogle Vertex AIGPT4AllGradientGreenNodeHugging FaceIBM watsonx.aiInfinityInstruct Embeddings on Hugging FaceIPEX-LLM: Local BGE Embeddings on Intel CPUIPEX-LLM: Local BGE Embeddings on Intel GPUIntel¬Æ Extension for Transformers Quantized Text EmbeddingsJinaJohn Snow LabsLASER Language-Agnostic SEntence Representations Embeddings by Meta AILindormLlama.cppllamafileLLMRailsLocalAIMiniMaxMistralAImodel2vecModelScopeMosaicMLNaverNebiusNetmindNLP CloudNomicNVIDIA NIMsOracle Cloud Infrastructure Generative AIOllamaOpenClipOpenAIOpenVINOEmbedding Documents using Optimized and Quantized EmbeddersOracle AI Vector Search: Generate EmbeddingsOVHcloudPinecone EmbeddingsPredictionGuardEmbeddingsPremAISageMakerSambaNovaCloudSambaStudioSelf HostedSentence Transformers on"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 21,
    "content": "Generate EmbeddingsOVHcloudPinecone EmbeddingsPredictionGuardEmbeddingsPremAISageMakerSambaNovaCloudSambaStudioSelf HostedSentence Transformers on Hugging FaceSolarSpaCySparkLLM Text EmbeddingsTensorFlow HubText Embeddings InferenceTextEmbed - Embedding Inference ServerTitan TakeoffTogether AIUpstageVolc EngineVoyage AIXorbits inference (Xinference)YandexGPTZhipuAIOtherProvidersOn this pageProviders"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 22,
    "content": "infoIf you'd like to write your own integration, see Extending LangChain.\nIf you'd like to contribute an integration, see Contributing integrations.\nIntegration Packages‚Äã\nThese providers have standalone langchain-{provider} packages for improved versioning, dependency management and testing."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 23,
    "content": "ProviderPackageDownloadsLatestJSOpenAIlangchain-openai‚úÖGoogle VertexAIlangchain-google-vertexai‚úÖAWSlangchain-aws‚úÖAnthropiclangchain-anthropic‚úÖGoogle Generative AIlangchain-google-genai‚úÖGoogle Communitylangchain-google-community‚ùåOllamalangchain-ollama‚úÖGroqlangchain-groq‚úÖChromalangchain-chroma‚úÖCoherelangchain-cohere‚úÖHuggingfacelangchain-huggingface‚úÖMistralAIlangchain-mistralai‚úÖPostgreslangchain-postgres‚ùåPineconelangchain-pinecone‚úÖDeepseeklangchain-deepseek‚úÖPerplexitylangchain-perplexity‚úÖNvidia AI Endpointslangchain-nvidia-ai-endpoints‚ùåIbmlangchain-ibm‚úÖMilvuslangchain-milvus‚ùåMongoDBlangchain-mongodb‚úÖDatabricksdatabricks-langchain‚ùåQdrantlangchain-qdrant‚úÖFireworkslangchain-fireworks‚úÖTavilylangchain-tavily‚úÖElasticsearchlangchain-elasticsearch‚úÖUnstructuredlangchain-unstructured‚ùåDataStax Astra"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 24,
    "content": "Astra DBlangchain-astradb‚úÖNeo4Jlangchain-neo4j‚úÖTogetherlangchain-together‚úÖRedislangchain-redis‚úÖXAIlangchain-xai‚úÖSambanovalangchain-sambanova‚ùåGraph RAGlangchain-graph-retriever‚ùåAzure AIlangchain-azure-ai‚úÖWeaviatelangchain-weaviate‚úÖVoyageAIlangchain-voyageai‚ùåCerebraslangchain-cerebras‚úÖLitellmlangchain-litellm‚ùåDoclinglangchain-docling‚ùåUpstagelangchain-upstage‚ùåNomiclangchain-nomic‚úÖAzure Dynamic"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 25,
    "content": "Dynamic Sessionslangchain-azure-dynamic-sessions‚úÖPymupdf4Llmlangchain-pymupdf4llm‚ùåAI21langchain-ai21‚ùåExalangchain-exa‚úÖPredictionguardlangchain-predictionguard‚ùåWriterlangchain-writer‚ùåMemgraphlangchain-memgraph‚ùåQwqlangchain-qwq‚ùåDB2langchain-db2‚ùåPromptylangchain-prompty‚ùåSqlserverlangchain-sqlserver‚ùåHyperbrowserlangchain-hyperbrowser‚ùåApifylangchain-apify‚ùåLangFairlangfair‚ùåVDMSlangchain-vdms‚ùåNaverlangchain-naver‚ùåMariaDBlangchain-mariadb‚ùåSAP HANA Cloudlangchain-hana‚ùåScrapegraphlangchain-scrapegraph‚ùåSema4langchain-sema4‚ùåSnowflakelangchain-snowflake‚ùåCouchbaselangchain-couchbase‚ùåLinkuplangchain-linkup‚ùåADS4GPTsads4gpts-langchain‚ùåLocalAIlangchain-localai‚ùåSalesforcelangchain-salesforce‚ùåDeeplakelangchain-deeplake‚ùåProloglangchain-prolog‚ùåTableaulangchain-tableau‚ùåContextual"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 26,
    "content": "AIlangchain-contextual‚ùåCloudflarelangchain-cloudflare‚úÖYDBlangchain-ydb‚ùåValyulangchain-valyu‚ùåBoxlangchain-box‚ùåTilorestilores-langchain‚ùåRecalliolangchain-recallio‚ùåSurrealDBlangchain-surrealdb‚ùåKuzulangchain-kuzu‚ùåNaverlangchain-naver-community‚ùåBrightdatalangchain-brightdata‚ùåDigitalOcean Gradientlangchain-gradient‚ùåAgentqllangchain-agentql‚ùåNebiuslangchain-nebius‚ùåNimblelangchain-nimble‚ùåDappierlangchain-dappier‚ùåRunPodlangchain-runpod‚ùåGoodfirelangchain-goodfire‚ùåTaigalangchain-taiga‚ùåXinferencelangchain-xinference‚ùåJenkinslangchain-jenkins‚ùåGOAT SDKgoat-sdk-adapter-langchain‚ùåOxylabslangchain-oxylabs‚ùåCrateDBlangchain-cratedb‚ùåCogneelangchain-cognee‚ùåAbsolangchain-abso‚ùåValtheralangchain-valthera‚ùåFeatherless AIlangchain-featherless-ai‚ùåOceanbaselangchain-oceanbase‚ùåPull Mdlangchain-pull-md‚ùåGalaxia"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 27,
    "content": "AIlangchain-featherless-ai‚ùåOceanbaselangchain-oceanbase‚ùåPull Mdlangchain-pull-md‚ùåGalaxia Retrieverlangchain-galaxia-retriever‚ùåGellangchain-gel‚ùåVectaralangchain-vectara‚ùåOpengradientlangchain-opengradient‚ùåPermitlangchain-permit‚ùåFalkorDBlangchain-falkordb‚ùåDiscord (Shikenso)langchain-discord-shikenso‚ùåSingleStorelangchain-singlestore‚ùåGreennodelangchain-greennode‚ùåModelscopelangchain-modelscope‚ùåPipeshiftlangchain-pipeshift‚ùåFmp Datalangchain-fmp-data‚ùåNetmindlangchain-netmind‚ùåZoterolangchain-zotero-retriever‚ùåPowerScale RAG Connectorpowerscale-rag-connector‚ùåLindormlangchain-lindorm-integration‚ùåPayman Toollangchain-payman-tool‚ùåTensorlakelangchain-tensorlake‚ùåAnchor Browserlangchain-anchorbrowser‚ùåMCP"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 28,
    "content": "Toollangchain-payman-tool‚ùåTensorlakelangchain-tensorlake‚ùåAnchor Browserlangchain-anchorbrowser‚ùåMCP Toolboxtoolbox-langchain‚ùåAImlapilangchain-aimlapi‚ùåScrapelesslangchain-scrapeless‚ùåBigtablelangchain-google-bigtable‚ùåOracle Cloud Infrastructure (OCI)langchain-oci‚ùåTimbrlangchain-timbr‚ùåZenrowslangchain-zenrows‚ùåZeusDBlangchain-zeusdb‚ùåScraperapilangchain-scraperapi‚ùå"
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "chunk_id": 29,
    "content": "All Providers‚Äã\nClick here to see all providers. Or search for a\nprovider using the Search field in the top-right corner of the screen.Edit this pageNextAnthropicIntegration PackagesAll ProvidersCommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#prompt-engineering",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/",
    "chunk_id": 0,
    "content": "Redirecting...\n\n\n\n\n\n\nRedirecting..."
  },
  {
    "url": "https://www.langchain.com/about",
    "chunk_id": 0,
    "content": "About\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upWe help developers get their agents to productionLangChain provides the agent engineering platform and open source frameworks developers need to ship reliable agents fast."
  },
  {
    "url": "https://www.langchain.com/about",
    "chunk_id": 1,
    "content": "MissionWe‚Äôre on a mission to make agents as reliable as databases and APIs. To get there, we‚Äôre building the leading platform for agent engineering. Our open source frameworks, LangChain and LangGraph, give developers speed and granular control. Our commercial platform, LangSmith delivers observability, evaluation, and deployment for rapid iteration.Together, our stack helps AI teams create LLM systems that you can depend on in production. Millions of developers trust LangChain products to engineer reliable agents.Our small and mighty team is growing fastCome join usWe‚Äôre hiring across many teams. Explore our open positions or read more about¬†LangChain on our Careers page.Ready to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step"
  },
  {
    "url": "https://www.langchain.com/about",
    "chunk_id": 2,
    "content": "on our Careers page.Ready to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 0,
    "content": "How Trellix cut log parsing time from days to minutes with LangGraph Studio and LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Trellix cut log parsing time from days to minutes with LangGraph Studio and LangSmith\nSee how cybersecurity company Trellix used LangGraph Studio to visualize and debug agent interactions, plus LangSmith for agent evaluations\n\nCase Studies\n4 min read\nApr 21, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 1,
    "content": "Trellix is a leading cybersecurity firm with 40,000+ customers that prevents organizations from cybersecurity attacks and threats. To address challenges faced by customers, the Trellix Professional Services Team used LangSmith and LangGraph ‚Äì including the visual LangGraph Studio ‚Äì to develop Sidekick, their internal application that democratizes knowledge and automates tedious processes.Problem: Customer request backlog and log parsingTrellix faced significant challenges with a growing backlog of requests for cybersecurity integrations and log parsing. Each request often required a developer to spend 2 to 3 days deciphering logs, coding integrations, and managing customer communications. This lengthy process frustrated customers and led to delays, as support tickets would bounce back and"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 2,
    "content": "and managing customer communications. This lengthy process frustrated customers and led to delays, as support tickets would bounce back and forth between customers and engineers.To improve customer experience, Trellix decided to build Sidekick, an agentic platform to automate tasks for engineering teams at Trellix, including parsing and script writing. Specifically, they created a structured approach to intake and parse syslog data. Sidekick can automatically generate parsers for unknown log formats, reducing the time required for manual parsing from days to minutes. Additionally, they built agents that can speed up the development of plugins and integrations for their SaaS products. Traditionally, this required an engineer to read through 3rd-party API documentation and generate"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 3,
    "content": "plugins and integrations for their SaaS products. Traditionally, this required an engineer to read through 3rd-party API documentation and generate boilerplate code for each new plugin. Handing off this work to agents meant plugins, traditionally being written during the course of multiple days, could now be written during the better part of an afternoon. This quicker turn around time enabled engineers to make a dent in the integration backlog and increased customer satisfaction.LangGraph‚Äôs advantages as a libraryLangGraph provided the low-level tools and enhanced abstraction techniques needed for the Trellix AI engineering team to make the required customizations for their use cases. Specifically,¬† map-reduce style graphs using the Send API and subgraph calling are used throughout the"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 4,
    "content": "required customizations for their use cases. Specifically,¬† map-reduce style graphs using the Send API and subgraph calling are used throughout the Sidekick Agents. These features encouraged modularity and abstraction. The Trellix team started by making several smaller subgraphs, many of which relied on the Send API and other lower-level LangGraph techniques to work efficiently and at scale. Once multiple subgraphs could perform their individual roles successfully, larger graphs were made to call the original graphs as modules.The Trellix team noted the ease of use; it was not that LangGraph had fundamentally reimagined how to develop agents. Instead, LangGraph offered several out-of-the-box features that made their lives as developers easier. Rather than spending their time figuring out"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 5,
    "content": "Instead, LangGraph offered several out-of-the-box features that made their lives as developers easier. Rather than spending their time figuring out the best way to create agents in code, their time was spent tweaking, refining, and combining a small assembly of easily-built agents.LangGraph‚Äôs human-in-the-loop capabilities also provided reassurance that engineers could step in to approve or rewind the agent‚Äôs actions as needed. Having the ability to pause execution during development testing or restart a certain step with slightly different input without waiting for a whole new run led to efficiency gains. This was a big deal to the engineering team who has stressed that waiting for model responses to test code can become quite tedious.Using Studio to visualize agent workflows for"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 6,
    "content": "team who has stressed that waiting for model responses to test code can become quite tedious.Using Studio to visualize agent workflows for business stakeholdersNot only did the open source libraries offer advantages, but LangChain tools were particularly useful. LangGraph Studio played a crucial role in the development of Sidekick by providing a framework to visualize and optimize the workflows involved in log parsing and integration tasks. The engineering team used LangGraph Studio to map out the manual processes and transition them into an agentic workflow.¬†The benefits of LangGraph Studio did not stop with development. Agent visualization was especially helpful for presenting the thought process and reasoning behind AI models to both technical and non-technical stakeholders, such as"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 7,
    "content": "was especially helpful for presenting the thought process and reasoning behind AI models to both technical and non-technical stakeholders, such as executives and business leaders at Trellix. The engineering team behind Sidekick found that getting buy-in and inter-team understanding drastically improved once LangGraph Studio came into use. It became a great way to show that agents are not a ‚Äúblack box‚Äù but are instead carefully engineered programs.Trellix's LangGraph Studio workflowMonitoring agent performance over time with LangSmith¬†To make data-driven decisions and to assess agent performance, Trellix used LangSmith for experimentation and to action upon performance metrics. The team was able to first design different architectures of their agent with LangGraph, then test multiple"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 8,
    "content": "and to action upon performance metrics. The team was able to first design different architectures of their agent with LangGraph, then test multiple architectures of their Sidekick agents in LangSmith in order to see what performed best.¬†¬†Using datasets and experiments in LangSmith was especially powerful, as the Trellix team could quickly compare performance across app versions. In particular, they monitored key metrics such as recursion rate (i.e., how often the agent has to restart or go back to a previous step) and the ‚Äúmust include‚Äù rate (i.e., how often the agent retrieves helpful additional documents). Having this data and seeing improvements grounded in data helped Trellix build confidence before shipping to production.Trellix's Experiments view in LangSmithIn addition to their use"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 9,
    "content": "grounded in data helped Trellix build confidence before shipping to production.Trellix's Experiments view in LangSmithIn addition to their use of experiments and datasets, the engineers at Trellix found the traces to be especially useful for debugging both when in production and during development. The intuitive structuring of trace data into inputs and outputs of each node made debugging significantly easier than drudging through AWS logs. This led to quicker development and bug fixes which increased satisfaction from internal users.Impact & what‚Äôs next¬†With Sidekick, Trellix has amplified time savings for both engineers on the team and customers. They have:¬†Reduced log parsing time from days to minutes, drastically improving engineering efficiency.Accelerated customer request"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 10,
    "content": "and customers. They have:¬†Reduced log parsing time from days to minutes, drastically improving engineering efficiency.Accelerated customer request resolution, reducing backlog and improving time-to-value (TTV).Improved AI agent performance by testing multiple architectures and tracking key metrics in LangSmith.Boosted stakeholder confidence by providing clear, visual explanations of AI reasoning to non-technical leaders.Looking ahead, Trellix plans to expand the capabilities of Sidekick to external partners, further democratizing access to AI-driven solutions in cybersecurity. The positive impact of LangSmith and LangGraph has set the stage for continued innovation in Trellix's service delivery, with goals to extend automated parsing and cloud connectors to all customers in the next"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 11,
    "content": "for continued innovation in Trellix's service delivery, with goals to extend automated parsing and cloud connectors to all customers in the next quarter.¬†ConclusionTrellix has successfully implemented generative AI to address operational challenges in the cybersecurity realm, including servicing customer needs. By using LangSmith, LangGraph, and LangGraph Studio to develop Sidekick, Trellix has not only improved internal efficiencies but also enhanced customer satisfaction ‚Äì paving the way for future advancements in AI-driven cybersecurity solutions."
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 12,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-trellix/",
    "chunk_id": 13,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#insights",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 0,
    "content": "Join the Community\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 1,
    "content": "AboutCareersPricingGet a demoSign upJoin theLangChain Community SlackOur community is filled with smart and helpful individuals who share our commitment to advancing the field of generative AI. We‚Äôre dedicated to maintaining the spirit of this community and creating a welcoming environment for learning.These guidelines have been created to help everyone understand how to best engage and contribute. We appreciate your support in building a community we can all be proud of!Our Slack Community is designed for open discussion, sharing events and job opportunities, and showcasing your agents. It‚Äôs not intended for product support. If you're looking for product help, check out the LangChain Forum instead.Expectations for all MembersRule 1: Respect All MembersWe strive to bring a positive and"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 2,
    "content": "for product help, check out the LangChain Forum instead.Expectations for all MembersRule 1: Respect All MembersWe strive to bring a positive and rewarding experience for everyone in this community. Therefore, we have zero tolerance for any form of disrespectful behavior. Treat others as you would like to be treated and assume best intentions. We want to create a space where people lift each other up and encourage each other‚Äôs work. We have all seen dark corners of the internet riddled with pessimism and harsh critique. While feedback is welcome and solicited regularly in the community, we want to be defined by collective optimism and support.We ask everyone to use real identities because we want you to be mindful of your tone and discourse in all forums. When in doubt, pay extra"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 3,
    "content": "support.We ask everyone to use real identities because we want you to be mindful of your tone and discourse in all forums. When in doubt, pay extra consideration if your words are constructive and supportive, and contribute to making the space a place one in which people can share and create freely.All participants in LangChain Community spaces ‚Äì including Slack, code repositories, and meetups ‚Äì must adhere to the Community Code of Conduct. If you cannot comply with these guidelines, we ask that you refrain from participating in the community. We will remove people from the community who do not follow our conduct guidelines.Rule 2: Practice Proper Messaging EtiquetteIn the Forum, post help-related questions, product feedback, and feature requests. If you are posting a question, ensure"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 4,
    "content": "Proper Messaging EtiquetteIn the Forum, post help-related questions, product feedback, and feature requests. If you are posting a question, ensure your questions are carefully considered, verify that the question has not already been asked, and make sure you are posting in the right category with the right tags. If you are responding to a question, respond with an actual answer; do not link to an external link (unless it is the LangChain Docs), as external articles may be incorrect and are impossible for us to update.Use Slack for broader conversations, events, hearing about content, and showcasing your agents‚Äînot for asking questions. In Slack, send your posts in a single message, use threads, and post in the appropriate channels. If you are sharing a large snippet of code, put it in the"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 5,
    "content": "send your posts in a single message, use threads, and post in the appropriate channels. If you are sharing a large snippet of code, put it in the thread of your message. Do not seek extra attention by tagging individuals, double-posting, or bumping your message by sending to the channel.Report bugs to support[at]langchain.dev.Rule 3: Default to Using Public ChannelsBy default, keep interactions public in LangChain Community spaces. If someone is directly messaging you in a way that doesn‚Äôt uphold the values of the community or is against the Code of Conduct, you are encouraged to report behavior to community@langchain.dev.Rule 4: Do Not SolicitThis community is designed for developers to share their work, ideas, and learnings. It is not meant for lead generation by vendors or recruiters,"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 6,
    "content": "community is designed for developers to share their work, ideas, and learnings. It is not meant for lead generation by vendors or recruiters, nor is it a place for competitors to advertise their products or services. Any vendors, recruiters, or competitors soliciting members will be permanently banned from the community.Vendor Expectations"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 7,
    "content": "Vendors typically include individuals from companies offering products or services aimed at our LangChain Community. This category also covers recruiters, investors, open-source maintainers (whether they have a paid offering or not), consultants, and freelancers. When in doubt, it's best to be cautious.As a vendor, you are part of this community, and we encourage your full participation. We‚Äôve observed that those who engage with the intention of sharing, rather than pitching, build great user relationships for their products. Conversely, community members can easily sense when they are being treated as an audience or resource for monetization, which often leads to a negative response.Vendors must keep promotional content within designated areas.The LangChain community is primarily a"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 8,
    "content": "which often leads to a negative response.Vendors must keep promotional content within designated areas.The LangChain community is primarily a non-commercial space. Unsolicited DMs will not be tolerated. For community members interested in staying updated with the AI industry, the Community Slack provides several spaces for vendors to share promotional material:#vendor-content#events#vendor- specific channels#survey-cornerRecruiters may also post in #jobs but may not solicit applications in DMs.The definition of \"vendor content\" can sometimes be unclear, and we rely our members' judgment in these cases. Generally, if the content is hosted on a site controlled by the company or its employees (including platforms like Substack and Medium) or includes a call-to-action such as signing up for a"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 9,
    "content": "site controlled by the company or its employees (including platforms like Substack and Medium) or includes a call-to-action such as signing up for a mailing list or trial account, it will likely be considered promotional.Sign up for our Slack CommunityFirst NameLast NameTitle at WorkCompany NameWork EmailCity, CountryI accept the Community Code of Conduct and understand the expectations for all membersThank you! Your submission has been received!Oops! Something went wrong while submitting the form.LangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups"
  },
  {
    "url": "https://www.langchain.com/join-community",
    "chunk_id": 10,
    "content": "Education\n\n\nEnquire about startup pricing\n\n\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\n\n\nEducation\n\n\nProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 0,
    "content": "Interrupts - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesInterruptsLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangGraph v1.0Release notesGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelAdd and manage memorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 1,
    "content": "APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating human inputRules of interruptsDo not wrap interrupt calls in try/exceptDo not reorder interrupt calls within a nodeDo not return complex values in interrupt callsSide effects called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with interruptsUsing LangGraph StudioCapabilitiesInterruptsCopy pageCopy pageLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 2,
    "content": "Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\nInterrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you‚Äôre ready to continue, you resume execution by re-invoking the graph using Command, which then becomes the return value of the interrupt() call from inside the node."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 3,
    "content": "Unlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic‚Äîthey can be placed anywhere in your code and can be conditional based on your application logic."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 4,
    "content": "Checkpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state.\nthread_id is your pointer: set config={\"configurable\": {\"thread_id\": ...}} to tell the checkpointer which state to load.\nInterrupt payloads surface as __interrupt__: the values you pass to interrupt() return to the caller in the __interrupt__ field so you know what the graph is waiting on."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 5,
    "content": "The thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\n‚ÄãPause using interrupt\nThe interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\nTo use interrupt, you need:\n\nA checkpointer to persist the graph state (use a durable checkpointer in production)\nA thread ID in your config so the runtime knows which state to resume from\nTo call interrupt() where you want to pause (payload must be JSON-serializable)\n\nCopyfrom langgraph.types import interrupt"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 6,
    "content": "Copyfrom langgraph.types import interrupt\n\ndef approval_node(state: State):\n    # Pause and ask for approval\n    approved = interrupt(\"Do you approve this action?\")\n\n    # When you resume, Command(resume=...) returns that value here\n    return {\"approved\": approved}\n\nWhen you call interrupt, here‚Äôs what happens:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 7,
    "content": "# When you resume, Command(resume=...) returns that value here\n    return {\"approved\": approved}\n\nWhen you call interrupt, here‚Äôs what happens:\n\nGraph execution gets suspended at the exact point where interrupt is called\nState is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\nValue is returned to the caller under __interrupt__; it can be any JSON-serializable value (string, object, array, etc.)\nGraph waits indefinitely until you resume execution with a response\nResponse is passed back into the node when you resume, becoming the return value of the interrupt() call"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 8,
    "content": "‚ÄãResuming interrupts\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input.\nCopyfrom langgraph.types import Command\n\n# Initial run - hits the interrupt and pauses\n# thread_id is the persistent pointer (stores a stable ID in production)\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\nresult = graph.invoke({\"input\": \"data\"}, config=config)\n\n# Check what was interrupted\n# __interrupt__ contains the payload that was passed to interrupt()\nprint(result[\"__interrupt__\"])\n# > [Interrupt(value='Do you approve this action?')]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 9,
    "content": "# Resume with the human's response\n# The resume payload becomes the return value of interrupt() inside the node\ngraph.invoke(Command(resume=True), config=config)\n\nKey points about resuming:\n\nYou must use the same thread ID when resuming that was used when the interrupt occurred\nThe value passed to Command(resume=...) becomes the return value of the interrupt call\nThe node restarts from the beginning of the node where the interrupt was called when resumed, so any code before the interrupt runs again\nYou can pass any JSON-serializable value as the resume value\n\n‚ÄãCommon patterns\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 10,
    "content": "Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)\n Review and edit: Let humans review and modify LLM outputs or tool calls before continuing\n Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution\n Validating human input: Pause before proceeding to the next step to validate human input\n\n‚ÄãApprove or reject\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\nCopyfrom typing import Literal\nfrom langgraph.types import interrupt, Command"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 11,
    "content": "def approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\n    is_approved = interrupt({\n        \"question\": \"Do you want to proceed with this action?\",\n        \"details\": state[\"action_details\"]\n    })\n\n    # Route based on the response\n    if is_approved:\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\n    else:\n        return Command(goto=\"cancel\")\n\nWhen you resume the graph, pass true to approve or false to reject:\nCopy# To approve\ngraph.invoke(Command(resume=True), config=config)\n\n# To reject\ngraph.invoke(Command(resume=False), config=config)\n\nFull exampleCopyimport sqlite3\nfrom typing import Literal, Optional, TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 12,
    "content": "# To reject\ngraph.invoke(Command(resume=False), config=config)\n\nFull exampleCopyimport sqlite3\nfrom typing import Literal, Optional, TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass ApprovalState(TypedDict):\n    action_details: str\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\n\n\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\n    # Expose details so the caller can render them in a UI\n    decision = interrupt({\n        \"question\": \"Approve this action?\",\n        \"details\": state[\"action_details\"],\n    })\n\n    # Route to the appropriate node after resume\n    return Command(goto=\"proceed\" if decision else \"cancel\")"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 13,
    "content": "# Route to the appropriate node after resume\n    return Command(goto=\"proceed\" if decision else \"cancel\")\n\n\ndef proceed_node(state: ApprovalState):\n    return {\"status\": \"approved\"}\n\n\ndef cancel_node(state: ApprovalState):\n    return {\"status\": \"rejected\"}\n\n\nbuilder = StateGraph(ApprovalState)\nbuilder.add_node(\"approval\", approval_node)\nbuilder.add_node(\"proceed\", proceed_node)\nbuilder.add_node(\"cancel\", cancel_node)\nbuilder.add_edge(START, \"approval\")\nbuilder.add_edge(\"approval\", \"proceed\")\nbuilder.add_edge(\"approval\", \"cancel\")\nbuilder.add_edge(\"proceed\", END)\nbuilder.add_edge(\"cancel\", END)\n\n# Use a more durable checkpointer in production\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 14,
    "content": "# Use a more durable checkpointer in production\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\ninitial = graph.invoke(\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\n    config=config,\n)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'question': ..., 'details': ...})]\n\n# Resume with the decision; True routes to proceed, False to cancel\nresumed = graph.invoke(Command(resume=True), config=config)\nprint(resumed[\"status\"])  # -> \"approved\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 15,
    "content": "‚ÄãReview and edit state\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\nCopyfrom langgraph.types import interrupt\n\ndef review_node(state: State):\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\n    edited_content = interrupt({\n        \"instruction\": \"Review and edit this content\",\n        \"content\": state[\"generated_text\"]\n    })\n\n    # Update the state with the edited version\n    return {\"generated_text\": edited_content}\n\nWhen resuming, provide the edited content:\nCopygraph.invoke(\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\n    config=config\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 16,
    "content": "Full exampleCopyimport sqlite3\nfrom typing import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass ReviewState(TypedDict):\n    generated_text: str\n\n\ndef review_node(state: ReviewState):\n    # Ask a reviewer to edit the generated content\n    updated = interrupt({\n        \"instruction\": \"Review and edit this content\",\n        \"content\": state[\"generated_text\"],\n    })\n    return {\"generated_text\": updated}\n\n\nbuilder = StateGraph(ReviewState)\nbuilder.add_node(\"review\", review_node)\nbuilder.add_edge(START, \"review\")\nbuilder.add_edge(\"review\", END)\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 17,
    "content": "checkpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\ninitial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]\n\n# Resume with the edited text from the reviewer\nfinal_state = graph.invoke(\n    Command(resume=\"Improved draft after review\"),\n    config=config,\n)\nprint(final_state[\"generated_text\"])  # -> \"Improved draft after review\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 18,
    "content": "‚ÄãInterrupts in tools\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it‚Äôs called, and allows for human review and editing of the tool call before it is executed.\nFirst, define a tool that uses interrupt:\nCopyfrom langchain.tools import tool\nfrom langgraph.types import interrupt\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\"\n    })"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 19,
    "content": "if response.get(\"action\") == \"approve\":\n        # Resume value can override inputs before executing\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n        return f\"Email sent to {final_to} with subject '{final_subject}'\"\n    return \"Email cancelled by user\"\n\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\nFull exampleCopyimport sqlite3\nfrom typing import TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 20,
    "content": "from langchain.tools import tool\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass AgentState(TypedDict):\n    messages: list[dict]\n\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\",\n    })"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 21,
    "content": "if response.get(\"action\") == \"approve\":\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n\n        # Actually send the email (your implementation here)\n        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\n        return f\"Email sent to {final_to}\"\n\n    return \"Email cancelled by user\"\n\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5\").bind_tools([send_email])\n\n\ndef agent_node(state: AgentState):\n    # LLM may decide to call the tool; interrupt pauses before sending\n    result = model.invoke(state[\"messages\"])\n    return {\"messages\": state[\"messages\"] + [result]}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 22,
    "content": "builder = StateGraph(AgentState)\nbuilder.add_node(\"agent\", agent_node)\nbuilder.add_edge(START, \"agent\")\nbuilder.add_edge(\"agent\", END)\n\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\ninitial = graph.invoke(\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\n        ]\n    },\n    config=config,\n)\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={'action': 'send_email', ...})]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 23,
    "content": "# Resume with approval and optionally edited arguments\nresumed = graph.invoke(\n    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\n    config=config,\n)\nprint(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\n\n‚ÄãValidating human input\nSometimes you need to validate input from humans and ask again if it‚Äôs invalid. You can do this using multiple interrupt calls in a loop.\nCopyfrom langgraph.types import interrupt\n\ndef get_age_node(state: State):\n    prompt = \"What is your age?\"\n\n    while True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 24,
    "content": "while True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\n        # Validate the input\n        if isinstance(answer, int) and answer > 0:\n            # Valid input - continue\n            break\n        else:\n            # Invalid input - ask again with a more specific prompt\n            prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\n    return {\"age\": answer}\n\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\nFull exampleCopyimport sqlite3\nfrom typing import TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 25,
    "content": "from langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command, interrupt\n\n\nclass FormState(TypedDict):\n    age: int | None\n\n\ndef get_age_node(state: FormState):\n    prompt = \"What is your age?\"\n\n    while True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\n        if isinstance(answer, int) and answer > 0:\n            return {\"age\": answer}\n\n        prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\n\nbuilder = StateGraph(FormState)\nbuilder.add_node(\"collect_age\", get_age_node)\nbuilder.add_edge(START, \"collect_age\")\nbuilder.add_edge(\"collect_age\", END)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 26,
    "content": "checkpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\nfirst = graph.invoke({\"age\": None}, config=config)\nprint(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n\n# Provide invalid data; the node re-prompts\nretry = graph.invoke(Command(resume=\"thirty\"), config=config)\nprint(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n\n# Provide valid data; loop exits and state updates\nfinal = graph.invoke(Command(resume=30), config=config)\nprint(final[\"age\"])  # -> 30"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 27,
    "content": "‚ÄãRules of interrupts\nWhen you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning‚Äîit does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there‚Äôs a few important rules to follow when working with interrupts to ensure they behave as expected.\n‚ÄãDo not wrap interrupt calls in try/except"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 28,
    "content": "‚ÄãDo not wrap interrupt calls in try/except\nThe way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 29,
    "content": "‚úÖ Separate interrupt calls from error-prone code\n‚úÖ Use specific exception types in try/except blocks\n\nSeparating logicExplicit exception handlingCopydef node_a(state: State):\n    # ‚úÖ Good: interrupting first, then handling\n    # error conditions separately\n    interrupt(\"What's your name?\")\n    try:\n        fetch_data()  # This can fail\n    except Exception as e:\n        print(e)\n    return state\n\n\nüî¥ Do not wrap interrupt calls in bare try/except blocks\n\nCopydef node_a(state: State):\n    # ‚ùå Bad: wrapping interrupt in bare try/except\n    # will catch the interrupt exception\n    try:\n        interrupt(\"What's your name?\")\n    except Exception as e:\n        print(e)\n    return state"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 30,
    "content": "‚ÄãDo not reorder interrupt calls within a node\nIt‚Äôs common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task‚Äôs resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important.\n\n‚úÖ Keep interrupt calls consistent across node executions"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 31,
    "content": "‚úÖ Keep interrupt calls consistent across node executions\n\nCopydef node_a(state: State):\n    # ‚úÖ Good: interrupt calls happen in the same order every time\n    name = interrupt(\"What's your name?\")\n    age = interrupt(\"What's your age?\")\n    city = interrupt(\"What's your city?\")\n\n    return {\n        \"name\": name,\n        \"age\": age,\n        \"city\": city\n    }\n\n\nüî¥ Do not conditionally skip interrupt calls within a node\nüî¥ Do not loop interrupt calls using logic that isn‚Äôt deterministic across executions\n\nSkipping interruptsLooping interruptsCopydef node_a(state: State):\n    # ‚ùå Bad: conditionally skipping interrupts changes the order\n    name = interrupt(\"What's your name?\")"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 32,
    "content": "# On first run, this might skip the interrupt\n    # On resume, it might not skip it - causing index mismatch\n    if state.get(\"needs_age\"):\n        age = interrupt(\"What's your age?\")\n\n    city = interrupt(\"What's your city?\")\n\n    return {\"name\": name, \"city\": city}\n\n‚ÄãDo not return complex values in interrupt calls\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can‚Äôt serialize a function). To make your graphs adaptable to any deployment, it‚Äôs best practice to only use values that can be reasonably serialized.\n\n‚úÖ Pass simple, JSON-serializable types to interrupt\n‚úÖ Pass dictionaries/objects with simple values"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 33,
    "content": "‚úÖ Pass simple, JSON-serializable types to interrupt\n‚úÖ Pass dictionaries/objects with simple values\n\nSimple valuesStructured dataCopydef node_a(state: State):\n    # ‚úÖ Good: passing simple types that are serializable\n    name = interrupt(\"What's your name?\")\n    count = interrupt(42)\n    approved = interrupt(True)\n\n    return {\"name\": name, \"count\": count, \"approved\": approved}\n\n\nüî¥ Do not pass functions, class instances, or other complex objects to interrupt\n\nFunctionsClass instancesCopydef validate_input(value):\n    return len(value) > 0"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 34,
    "content": "FunctionsClass instancesCopydef validate_input(value):\n    return len(value) > 0\n\ndef node_a(state: State):\n    # ‚ùå Bad: passing a function to interrupt\n    # The function cannot be serialized\n    response = interrupt({\n        \"question\": \"What's your name?\",\n        \"validator\": validate_input  # This will fail\n    })\n    return {\"name\": response}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 35,
    "content": "‚ÄãSide effects called before interrupt must be idempotent\nBecause interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\nAs an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n\n‚úÖ Use idempotent operations before interrupt\n‚úÖ Place side effects after interrupt calls\n‚úÖ Separate side effects into separate nodes when possible"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 36,
    "content": "‚úÖ Use idempotent operations before interrupt\n‚úÖ Place side effects after interrupt calls\n‚úÖ Separate side effects into separate nodes when possible\n\nIdempotent operationsSide effects after interruptSeparating into different nodesCopydef node_a(state: State):\n    # ‚úÖ Good: using upsert operation which is idempotent\n    # Running this multiple times will have the same result\n    db.upsert_user(\n        user_id=state[\"user_id\"],\n        status=\"pending_approval\"\n    )\n\n    approved = interrupt(\"Approve this change?\")\n\n    return {\"approved\": approved}\n\n\nüî¥ Do not perform non-idempotent operations before interrupt\nüî¥ Do not create new records without checking if they exist"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 37,
    "content": "üî¥ Do not perform non-idempotent operations before interrupt\nüî¥ Do not create new records without checking if they exist\n\nCreating recordsAppending to listsCopydef node_a(state: State):\n    # ‚ùå Bad: creating a new record before interrupt\n    # This will create duplicate records on each resume\n    audit_id = db.create_audit_log({\n        \"user_id\": state[\"user_id\"],\n        \"action\": \"pending_approval\",\n        \"timestamp\": datetime.now()\n    })\n\n    approved = interrupt(\"Approve this change?\")\n\n    return {\"approved\": approved, \"audit_id\": audit_id}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 38,
    "content": "approved = interrupt(\"Approve this change?\")\n\n    return {\"approved\": approved, \"audit_id\": audit_id}\n\n‚ÄãUsing with subgraphs called as functions\nWhen invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called.\nCopydef node_in_parent_graph(state: State):\n    some_code()  # <-- This will re-execute when resumed\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 39,
    "content": "async function node_in_subgraph(state: State) {\n    someOtherCode(); # <-- This will also re-execute when resumed\n    result = interrupt(\"What's your name?\")\n    ...\n}\n\n‚ÄãDebugging with interrupts\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph.\nStatic interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.\n At compile time At run timeCopygraph = builder.compile(\n    interrupt_before=[\"node_a\"],  \n    interrupt_after=[\"node_b\", \"node_c\"],  \n    checkpointer=checkpointer,\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 40,
    "content": "# Pass a thread ID to the graph\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=config)  \n\n# Resume the graph\ngraph.invoke(None, config=config)  \n\nThe breakpoints are set during compile time.\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\nA checkpointer is required to enable breakpoints.\nThe graph is run until the first breakpoint is hit.\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/interrupts",
    "chunk_id": 41,
    "content": "‚ÄãUsing LangGraph Studio\nYou can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoStreamingPreviousUse time-travelNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 0,
    "content": "Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangSmith EvaluationContinuously improve agent qualityRun evals before and after shipping, gather expert feedback on real performance, and iterate on prompts with your team.Sign UpGet a demoHelping top teams ship reliable agentsEvaluate your agent‚Äôs performanceTest with offline evaluations on datasets, or run online evaluations on production traffic. Score performance with automated evaluators ‚Äî LLM-as-judge, code-based, or any custom logic‚Äî across criteria that matter to your business.Learn how to run an eval"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 2,
    "content": "Iterate & collaborate on promptsExperiment with models and prompts in the Playground, and compare outputs across different prompt versions or providers. Use the Prompt Canvas UI to auto improve prompts and compare results.Create and test a prompt\n\n\nGather expert human feedbackSet up annotation queues, so subject-matter experts can assess response relevance, correctness, and other custom criteria. Automatically assign runs for review, and annotate any part of your agent workflow to capture precise feedback on quality.Streamline feedback with annotation queues"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 3,
    "content": "Ready to build better agents through continuous evaluation?LangSmith works with any framework to help you test and iterate faster. Run automated evals, gather expert feedback, and collaborate on improvements -- all without leaving your workflow.Sign up for freeBook a demoResources for LangSmith EvaluationguidebookEvals guidebookDOCSEvals conceptsblogWhy Evals Matter video (first in a series)FAQs for LangSmith EvaluationWhat kind of evaluators does LangSmith support?\n\nHuman: Use annotation queues or inline review.Heuristic: Rule-based checks like ‚Äúis the response empty?‚Äù or ‚Äúdoes the code compile?‚ÄùLLM-as-judge: Use an LLM to score outputs against criteria you define.Pairwise: Compare two outputs to see which one is better.Can I run both offline and online evaluations?"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 4,
    "content": "Yes. Offline evals run on datasets (great for benchmarking or regression testing). Online evals run on real production traffic in near real time and can be used to monitor deployed agents or LLM apps. How does human feedback work?\n\nLangSmith makes it easy to collect expert feedback through annotation queues. You can flag runs for review, assign them to SMEs (Small and medium-sized enterprises), and use that feedback to improve prompts or evaluators or augment datasets.Can I use LangSmith Evaluation without LangSmith Observability?\n\nYes. You can use LangSmith Evaluation with or without Observability. For all plan types, you'll get access to both and only pay for what you use.I can‚Äôt have data leave my environment. Can I self-host LangSmith?"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 5,
    "content": "Yes, we allow customers to self-host LangSmith on our enterprise plan. We deliver the software to run on your Kubernetes cluster, and data will not leave your environment. For more information, check out our documentation.Where is my data stored?\n\nWhen using LangSmith hosted at smith.langchain.com, data is stored in GCP us-central-1. If you‚Äôre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment. For more information, check out our documentation.Will LangSmith add latency to my application?"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 6,
    "content": "No, LangSmith does not add any latency to your application. In the LangSmith SDK, there‚Äôs a callback handler that sends traces to a LangSmith trace collector which runs as an async, distributed process. Additionally, if LangSmith experiences an incident, your application performance will not be disrupted.Will you train on the data that I send LangSmith?\n\nWe will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information.How much does LangSmith cost?"
  },
  {
    "url": "https://www.langchain.com/langsmith/evaluation",
    "chunk_id": 7,
    "content": "See our pricing page for more information, and find a plan that works for you.ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 0,
    "content": "Monitor projects with dashboards - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationMonitoring & alertingMonitor projects with dashboardsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 1,
    "content": "(SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pagePrebuilt dashboardsDashboard sectionsGroup byCustom DashboardsCreating a new dashboardAdding charts to your dashboardChart configurationSelect tracing projects and filter runsPick a metricSplit the dataPick a chart typeSave and manage chartsLinking to a dashboard from a tracing projectExample: user-journey monitoringVideo guideMonitoring"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 2,
    "content": "the dataPick a chart typeSave and manage chartsLinking to a dashboard from a tracing projectExample: user-journey monitoringVideo guideMonitoring & alertingMonitor projects with dashboardsCopy pageCopy pageDashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the Monitoring tab in the left sidebar."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 3,
    "content": "LangSmith offers two dashboard types:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 4,
    "content": "Prebuilt dashboards: Automatically generated for every tracing project.\nCustom dashboards: Fully configurable collections of charts tailored to your needs.\n\n‚ÄãPrebuilt dashboards\nPrebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the Dashboard button on the top right of the tracing project page."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 5,
    "content": "You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.\n‚ÄãDashboard sections\nPrebuilt dashboards are broken down into the following sections:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 6,
    "content": "SectionWhat it showsTracesTrace count, latency and error rates. A trace is a collection of runs related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace.LLM CallsLLM call count and latency. Includes all runs where run type is ‚Äúllm‚Äù.Cost & TokensTotal and per-trace token counts and costs, broken down by token type. Costs are measured using LangSmith‚Äôs cost tracking.ToolsRun counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is ‚Äútool‚Äù. Limits to top 5 most frequently occurring tools.Run TypesRun counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 7,
    "content": "error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table.Feedback ScoresAggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 8,
    "content": "For example, for the following trace, the following runs have a depth of 1:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 9,
    "content": "‚ÄãGroup by\nGroup by run tag or metadata can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won‚Äôt take effect; the global group by will apply to all other charts.\nWhen adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.\n‚ÄãCustom Dashboards"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 10,
    "content": "‚ÄãCustom Dashboards\nCreate tailored collections of charts for tracking metrics that matter most for your application.\n‚ÄãCreating a new dashboard"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 11,
    "content": "Navigate to the Monitor tab in the left sidebar.\nClick on the + New Dashboard button.\nGive your dashboard a name and a description.\nClick on Create.\n\n‚ÄãAdding charts to your dashboard\n\nWithin a dashboard, click on the + New Chart button to open up the chart creation pane.\nGive your chart a name and a description.\nConfigure the chart.\n\n‚ÄãChart configuration\n‚ÄãSelect tracing projects and filter runs\n\nSelect one or more tracing projects to track metrics for.\nUse the Chart filters section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on filtering traces in application.\n\n‚ÄãPick a metric"
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 12,
    "content": "‚ÄãPick a metric\n\nChoose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you‚Äôll see a preview of your chart and the matching runs.\nFor certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.\n\n\n‚ÄãSplit the data\nThere are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):\n\n\nGroup by: Group runs by run tag or metadata, run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 13,
    "content": "Data series: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.\n\n\n\n‚ÄãPick a chart type\n\nChoose between a line chart and a bar chart for visualizing\n\n‚ÄãSave and manage charts\n\nClick Save to save your chart to the dashboard.\nEdit or delete a chart by clicking the triple dot button in the top right of the chart.\nClone a chart by clicking the triple line button in the top right of the chart and selecting + Clone. This will open a new chart creation pane with the same configurations as the original."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 14,
    "content": "‚ÄãLinking to a dashboard from a tracing project\nYou can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:\n\nIn your tracing project, click the three dots next to the Dashboard button.\nChoose a dashboard to set as the new default.\n\n\n‚ÄãExample: user-journey monitoring\nUse monitoring charts for mapping the decisions made by an agent at a particular node.\nConsider an email assistant agent. At a particular node it makes a decision about an email to:\n\nsend an email back\nnotify the user\nno response needed\n\nWe can create a chart to track and visualize the breakdown of these decisions.\nCreating the chart\n\n\nMetric Selection: Select the metric Run count."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 15,
    "content": "We can create a chart to track and visualize the breakdown of these decisions.\nCreating the chart\n\n\nMetric Selection: Select the metric Run count.\n\n\nChart Filters: Add a tree filter to include all of the traces with name triage_input. This means we only include traces that hit the triage_input node. Also add a chart filter for Is Root is true, so our count is not inflated by the number of nodes in the trace.\n\n\n\nData Series: Create a data series for each decision made at the triage_input node. The output of the decision is stored in the triage.response field of the output object, and the value of the decision is either no, email, or notify. Each of these decisions generates a separate data series in the chart."
  },
  {
    "url": "https://docs.langchain.com/langsmith/dashboards",
    "chunk_id": 16,
    "content": "Now we can visualize the decisions made at the triage_input node over time.\n‚ÄãVideo guide\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoSet up online evaluatorsPreviousAlerts in LangSmithNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/careers",
    "chunk_id": 0,
    "content": "Careers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upCareersWe are a small team of builders making an outsized impact in our industry."
  },
  {
    "url": "https://www.langchain.com/careers",
    "chunk_id": 1,
    "content": "We‚Äôre on a mission to make software smarter. Come¬†join us.Our valuesWork on what mattersWe ruthlessly prioritize and focus on only the most important, making sure we do a few things extremely well.Trust and accountabilityWe trust that everyone we hire can make a big impact, and we hold each other accountable to deliver great outcomes.Ship fast, ship robustlyWe learn the most by shipping. In service of the large LangChain community, we¬†aim to keep the bar high in everything we do.Push the industry forwardWe are in a super early, very fast-moving industry. Our users trust us to be their guide and partner ‚Äì we strive to stay ahead of the curve and innovate.Backed by the best in¬†the businessExplore our job opportunities\nOur Awards"
  },
  {
    "url": "https://www.langchain.com/careers",
    "chunk_id": 2,
    "content": "001Enterprise Tech¬†302024Databricks Partner of the Year2024Forbes  AI 502024GitHub‚ÄçAwards2023\n\n\nJoin us in San Francisco!Don‚Äôt see an open role, but believe you can make a difference ‚Ä®on¬†the¬†team? Send us a note telling us how.Contact UsAbout UsProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 0,
    "content": "Pushing LangSmith to new limits with Replit Agent's complex workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPushing LangSmith to new limits with Replit Agent's complex workflows\nSee how Replit built their agents atop LangGraph and integrated LangSmith to pinpoint issues, improve the performance of their agents, and enable human-in-the-loop workflows.\n\nCase Studies\n3 min read\nSep 26, 2024"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 1,
    "content": "Replit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues.¬†The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs."
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 2,
    "content": "LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context.¬†Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 3,
    "content": "context.¬†Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what‚Äôs happening, including at the inputs and outputs of each step, in order to understand the agent‚Äôs decision-making.¬†Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions ‚Äì including planning, creating dev environments, installing dependencies, and deploying applications for users.¬†As a result, Replit‚Äôs traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 4,
    "content": "involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith‚Äôs frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent‚Äôs traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 5,
    "content": "traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern ‚Äì searching within traces ‚Äì was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit‚Äôs time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 6,
    "content": "agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,¬† Replit‚Äôs agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces.¬†To solve this, LangSmith‚Äôs thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 7,
    "content": "across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial.¬†ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith‚Äôs powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development."
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 8,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-replit/",
    "chunk_id": 9,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#barriers-and-challenges",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#methodology",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://python.langchain.com/",
    "chunk_id": 0,
    "content": "ü¶úÔ∏èüîó LangChain"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#conclusion",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 0,
    "content": "Terms of Service\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upTerms of Service"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 1,
    "content": "LangChain Terms of ServiceOur Terms of Service was last updated on¬†October 14, 2025.‚ÄçThese Terms of Service (together with any applicable Order Forms, exhibits, and incorporated attachments, the ‚ÄúAgreement‚Äù) govern access to and use of the Licensed Platform and are entered into by and between LangChain Inc., a Delaware corporation (‚ÄúLangChain‚Äù), and the individual or entity (‚ÄúCustomer‚Äù defined below) that:¬† (a) enters into an Order Form that expressly incorporates this Agreement by reference; or (b) accesses or uses the Licensed Platform (including via a Free Access Subscription, as defined below). If the individual accepting this Agreement is accepting on behalf of a company or other legal entity, such individual represents that they have the authority to bind Customer to the terms and"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 2,
    "content": "is accepting on behalf of a company or other legal entity, such individual represents that they have the authority to bind Customer to the terms and conditions of this Agreement.¬† If the individual accepting this Agreement does not have such authority or does not agree with the terms of the Agreement, such individual must not accept this Agreement and may not use the LangChain Platform. Capitalized terms shall have the meaning outlined in Section 1 (Definitions) and others are defined contextually in this Agreement.‚Äç1. Definitions¬†‚ÄúAffiliate‚Äù means an entity that owns or controls, is owned or controlled by, or is under common control or ownership with a party, where ‚Äúcontrol‚Äù is the possession, directly or indirectly, of the power to direct or cause the direction of the management and"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 3,
    "content": "with a party, where ‚Äúcontrol‚Äù is the possession, directly or indirectly, of the power to direct or cause the direction of the management and policies of an entity, whether through ownership of voting securities, by contract or otherwise.‚Äç‚ÄúAgent Run‚Äù means a complete invocation of a LangGraph agent whether initiated within a LangGraph Application or through the Ancillary Software, consisting of execution from the start Node to the end Node.¬†‚Äç\"AI Content\" means any inputs or outputs generated by AI Services.‚Äç\"AI Services\" means any machine learning, generative AI functionality, or other artificial intelligence systems utilized for the generation of AI Content.‚Äç\"Ancillary Software\" means LangChain's Software Developer Kit (SDK) and/or APIs, including any Updates, that LangChain distributes"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 4,
    "content": "of AI Content.‚Äç\"Ancillary Software\" means LangChain's Software Developer Kit (SDK) and/or APIs, including any Updates, that LangChain distributes as part of the Licensed Platform (as applicable).‚Äç‚ÄúCloud Deployment‚Äù means a version of the Licensed Platform hosted and operated by LangChain in LangChain's cloud environment.‚Äç\"Customer Data\" means electronic data and information submitted or generated by or for Customer in connection with its use of the Licensed Platform, including all inputs and outputs, including AI Content, observed or executed by the Licensed Platform.‚Äç\"Customer Infrastructure\" means Customer-managed cloud infrastructure or equipment needed to host, access, or otherwise use the Licensed Platform, including as outlined in the Documentation.‚Äç\"Documentation\" means the"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 5,
    "content": "or equipment needed to host, access, or otherwise use the Licensed Platform, including as outlined in the Documentation.‚Äç\"Documentation\" means the electronic, online help files, technical documentation, and user manuals made available by LangChain for the Licensed Platform.‚Äç\"Free Access Subscriptions\" means limited access to the LangSmith Platform that LangChain makes available to Customer free of charge.¬† Free Access Subscriptions exclude access to the Licensed Platform offered via Beta Releases and purchased according to an Order Form.‚Äç\"High-Risk Activities\" means activities where the use or failure of the Licensed Platform could lead to death, personal injury, or environmental damage.¬†¬†‚Äç\"Hybrid Deployment\" means a version of the Licensed Platform that partially resides on Customer"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 6,
    "content": "to death, personal injury, or environmental damage.¬†¬†‚Äç\"Hybrid Deployment\" means a version of the Licensed Platform that partially resides on Customer Infrastructure (e.g. a virtual private cloud) and partially resides in LangChain's cloud.¬† In the Hybrid Deployment, no Customer Data, including AI Content, will pass through LangChain networks and systems. Only Operational Metadata and Subscription Metrics will be provided to LangChain networks and systems according to Section 4.3 below.‚Äç\"LangGraph Application\" means an application built by Customer utilizing the LangGraph framework as further described at https://langchain-ai.github.io/langgraph/ or https://github.com/langchain-ai/langgraphjs, as may be updated by LangChain from time to time.‚Äç\"LangSmith Platform\" means the platform for"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 7,
    "content": "or https://github.com/langchain-ai/langgraphjs, as may be updated by LangChain from time to time.‚Äç\"LangSmith Platform\" means the platform for monitoring, testing, and debugging as well as deploying and managing large language model applications in connection with the Licensed Platform, granted to Customer according to the Subscription Metrics outlined in an Order Form executed hereunder.‚Äç\"Licensed Platform\" means the LangSmith Platform, including any Ancillary Software, and made available to Customer via Cloud Deployment, Self-Hosted Deployment, or Hybrid Deployment, according to the Subscription Metrics set forth in an applicable Order Form executed hereunder.‚Äç\"Node\" means an action in the graphical representation of the LangGraph Application.‚Äç\"Nodes Executed\" means the sum of Nodes"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 8,
    "content": "Form executed hereunder.‚Äç\"Node\" means an action in the graphical representation of the LangGraph Application.‚Äç\"Nodes Executed\" means the sum of Nodes invoked by the LangGraph Application.‚Äç\"Operational Metadata\" means usage and diagnostic information generated by the Licensed Platform and collected by LangChain to support, maintain, and optimize the performance of the Licensed Platform. Operational Metadata may include information such as type and version of the LangChain software, operating system version, uptime and availability, system error logs and health metrics, product feature usage, general integrity and security of the Licensed Platform. For the avoidance of doubt, Operational Metadata will never include Customer Data, AI Content, Customer Confidential Information, Personal Data"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 9,
    "content": "For the avoidance of doubt, Operational Metadata will never include Customer Data, AI Content, Customer Confidential Information, Personal Data or protected health information.‚Äç\"Order Form\" means any LangChain online sign-up, order form, statement of work, service addendum, or other sign-up flow that references this Agreement and is hereby incorporated by reference.‚Äç\"Personal Data\" means Customer Data relating to an identified or identifiable natural person.‚Äç\"Reseller\" means a third party authorized by LangChain to promote, distribute, and/or resell the Licensed Platform.‚Äç‚ÄúSelf-Hosted Deployment‚Äù means a version of the Licensed Platform licensed for installation and operation within the Customer Infrastructure.‚Äç\"Subscription Metrics\" means the metrics used to determine Customer's access"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 10,
    "content": "for installation and operation within the Customer Infrastructure.‚Äç\"Subscription Metrics\" means the metrics used to determine Customer's access and use of the Licensed Platform, including but not limited to, the number of Traces, number of Users, number of Nodes Executed, number of Agent Runs or other mutually agreed criteria as set out in an Order Form.‚Äç\"Subscription Term\" means the period during which Customer is entitled to use the Licensed Platform as outlined in the applicable Order Form.¬†¬†‚Äç\"Third Party Products\" means any product not developed or provided by LangChain that Customer may, at its own discretion, opt to use with the Licensed Platform, often as an integration via the Ancillary Software, as further described in Section 2.8.¬†¬†‚Äç\"Trace\" means one complete invocation of an"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 11,
    "content": "Platform, often as an integration via the Ancillary Software, as further described in Section 2.8.¬†¬†‚Äç\"Trace\" means one complete invocation of an application chain or agent, evaluator run, or playground run.‚Äç\"Updates\" means all updates and enhancements that LangChain generally makes available at no additional charge to its customers of the version of the Licensed Platform licensed hereunder who are current in payment of applicable Fees (defined below).‚Äç\"User\" means Customer's or Customer‚Äôs Affiliates‚Äô employees, consultants, and third-party contractors accessing the Licensed Platform on Customer's behalf according to the terms of this Agreement. For purposes of clarity, Users are named individuals who have access to the Licensed Platform through a login and are typically software"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 12,
    "content": "Agreement. For purposes of clarity, Users are named individuals who have access to the Licensed Platform through a login and are typically software developers.‚Äç¬†2. Access¬†and Use of the Licensed Platform2.1\tAccess and Use of Licensed Platform. Subject to Customer‚Äôs compliance with the terms of this Agreement and timely payment of all applicable Fees, during the Subscription Term, LangChain will:‚Äç\t(A)\tFor Cloud Deployments.¬† Make the applicable LangSmith Platform available to Customer under an Order Form, solely for Customer‚Äôs internal business use, in accordance with the Subscription Metrics and order terms set forth in such Order Form.‚Äç\t(B)\tFor Self-Hosted Deployment Subscriptions. ¬† Grant Customer a limited, non-exclusive, non-transferable (except as set forth in Section 10.8 below),"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 13,
    "content": "Self-Hosted Deployment Subscriptions. ¬† Grant Customer a limited, non-exclusive, non-transferable (except as set forth in Section 10.8 below), non-sublicensable, license to install and use the Licensed Platform on Customer Infrastructure per applicable configuration parameters, as outlined in the Documentation, according to the Subscription Metrics and other terms of the Order Form.¬† Customer may make a reasonable number of copies of the Licensed Platform software and Documentation for environment or data residency separation, or as otherwise specified in the Order Form.¬†¬†‚Äç\t(C)\tProtection of Customer Data for Cloud Deployments.¬† LangChain will maintain administrative, technical, and physical safeguards designed to protect the security, confidentiality, and integrity of Customer Data."
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 14,
    "content": "will maintain administrative, technical, and physical safeguards designed to protect the security, confidentiality, and integrity of Customer Data.¬† Where Customer's use of the Licensed Platform for Cloud Deployments includes the processing of Personal Data subject to the applicable data protection laws, it will be governed by the Data Protection Addendum located at langchain.com/DPA (\"DPA\") that is incorporated into this Agreement by reference or, at Customer‚Äôs option, when countersigned by Customer and provided to LangChain at privacy@langchain.dev.¬†¬† ‚Äç2.2\tAccess to Users. Only authorized Users are permitted to access and use the Licensed Platform. Customer is solely responsible for (a) approving and maintaining access, identifying and authenticating Users, and controlling against"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 15,
    "content": "Platform. Customer is solely responsible for (a) approving and maintaining access, identifying and authenticating Users, and controlling against unauthorized access by Users including use or access that is inconsistent with the Subscription Metrics purchased according to an Order Form; (b) maintaining the confidentiality of usernames, passwords and account information (as applicable);ÔºàcÔºâall activities that occur under its Users‚Äô usernames, passwords or accounts as a result of Users‚Äô access to the Licensed Platform; and (d) ensuring Users‚Äô abide by all applicable local, state, national and foreign laws applicable to Customer‚Äôs use of the Licensed Platform.¬† Customer will notify LangChain immediately of any unauthorized use of, or access to, the Licensed Platform, and will use reasonable"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 16,
    "content": "Platform.¬† Customer will notify LangChain immediately of any unauthorized use of, or access to, the Licensed Platform, and will use reasonable efforts to promptly stop any unauthorized access to or use of the Licensed Platform.‚Äç2.3\tSelf-Hosted Deployment Platform License Keys.¬† Access to the Licensed Platform requires an authorized license key issued from LangChain.¬† The license key may impose limits on the use of the Licensed Platform in accordance with the Subscription Metrics and other terms of the applicable Order Form.¬† Customer shall not (or attempt to) destroy, disable, or circumvent in any way the license keys.¬† If LangChain issues a new license key, Customer will not use the previous license key to access and/or enable the Licensed Platform.¬† LangChain reserves the right to"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 17,
    "content": "a new license key, Customer will not use the previous license key to access and/or enable the Licensed Platform.¬† LangChain reserves the right to suspend access to the Licensed Platform in the event Customer or a User's use of the license keys is in breach of this Section 2.3.¬†¬†‚Äç2.4\tLicense Restrictions. Customer may not directly or indirectly and may not authorize any third party to: (a) decompile, disassemble, reverse engineer, or otherwise attempt to derive the source code, structure, ideas, algorithms, or associated know-how of, the Licensed Platform, Documentation, or reconstruct, or discover, any hidden or non-public elements of the Licensed Platform (except to the extent expressly permitted by applicable law notwithstanding this restriction); (b) translate, adapt, create derivative"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 18,
    "content": "Platform (except to the extent expressly permitted by applicable law notwithstanding this restriction); (b) translate, adapt, create derivative works from or modify the Licensed Platform, Documentation, or any portion of any of the foregoing; ÔºàcÔºâ sell, resell, license, sublicense, distribute, rent, or lease the Licensed Platform in a service bureau or outsourcing offering; (d) use the Licensed Platform to develop a similar or competing product or service; (e) publish benchmarks or performance information about the Licensed Platform; (f) transmit unlawful, infringing, harmful data or code (including, without limitation, ‚ÄúTrojan horses,‚Äù ‚Äúviruses,‚Äù ‚Äúworms,‚Äù ‚Äútime bombs,‚Äù ‚Äútime locks,‚Äù ‚Äúdevices,‚Äù ‚Äútraps,‚Äù ‚Äúaccess codes,‚Äù or ‚Äúdrop dead‚Äù or ‚Äútrap door‚Äù devices or any other harmful, malicious,"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 19,
    "content": "‚Äúworms,‚Äù ‚Äútime bombs,‚Äù ‚Äútime locks,‚Äù ‚Äúdevices,‚Äù ‚Äútraps,‚Äù ‚Äúaccess codes,‚Äù or ‚Äúdrop dead‚Äù or ‚Äútrap door‚Äù devices or any other harmful, malicious, or hidden procedures, routines or mechanisms that would cause the Licensed Platform to cease functioning or to damage or corrupt data, storage media, programs, equipment, or communications, or otherwise interfere with operations) either to or from the Licensed Platform; (g) alter or remove any trademarks or proprietary notices contained in or on the Licensed Platform or Documentation; (h) circumvent or otherwise interfere with the Licensed Platform's operation, access or use restrictions or conduct any security or vulnerability test (without proper written authorization from LangChain);¬† (i) use the Licensed Platform for High-Risk Activities, or"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 20,
    "content": "any security or vulnerability test (without proper written authorization from LangChain);¬† (i) use the Licensed Platform for High-Risk Activities, or (j) use the Licensed Platform in violation of this Agreement.¬†‚Äç2.5\tChanges to Licensed Platform.¬† Subject to Subsection 6.2(b) below, LangChain may issue new releases for the Licensed Platform during the Subscription Term which may include Updates, enhancements, or other modifications which will be included in the Fees set out in the Order Form.‚Äç2.6\tBeta Releases.¬† From time to time, LangChain may invite Customer and Users to discuss or evaluate certain pre-release or beta releases on a trial basis (collectively \"Beta Releases\") of the Licensed Platform. Customer may accept or decline any such evaluation or trial. Beta Releases designated by"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 21,
    "content": "(collectively \"Beta Releases\") of the Licensed Platform. Customer may accept or decline any such evaluation or trial. Beta Releases designated by LangChain ‚Äúbeta,‚Äù ‚Äúpilot,‚Äù ‚Äúnon-production evaluation‚Äù, \"design partner\" or other similar designations) are solely for Customer‚Äôs internal evaluation purposes. If Customer opts into Beta Releases, Customer agrees to participate in usage and other testing and provide Feedback (as defined below) about the Beta Releases, as reasonably requested by LangChain. Beta Releases are not considered the Licensed Platform under this Agreement, may not be supported, and may be subject to additional terms as outlined in an Order Form. LangChain may discontinue Beta Releases at any time and may never make Beta Releases generally available. Beta Releases are"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 22,
    "content": "in an Order Form. LangChain may discontinue Beta Releases at any time and may never make Beta Releases generally available. Beta Releases are provided ‚ÄúAs Is‚Äù without express or implied warranty or indemnity. LangChain will have no liability for, and Customer hereby releases LangChain from any liability or damage arising out of or in connection with any Beta Releases.‚Äç2.7\tFree Access Subscriptions.¬† LangChain may provide Customer with access to the Licensed Platform for free or on a trial basis.¬† LangChain makes no promises that any Free Access Subscriptions will be made available under the same commercial or other terms.¬† LangChain may terminate Customer‚Äôs right to use any Free Access Subscriptions at any time in LangChain‚Äôs sole discretion without liability; provided that LangChain will"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 23,
    "content": "Customer‚Äôs right to use any Free Access Subscriptions at any time in LangChain‚Äôs sole discretion without liability; provided that LangChain will use commercially reasonable efforts to provide Customer fifteen (15) days‚Äô written notice (email to suffice) if LangChain elects to institute a fee for Customer‚Äôs access to the Licensed Platform or terminate Customer‚Äôs Free Access Subscription without cause.¬† Any Free Access Subscriptions are provided by LangChain ‚ÄúAS-IS‚Äù and without any representations, warranties, performance, or¬† support obligations.‚Äç2.8\tThird-Party Products.¬† Third-Party Products may be available to Customer in connection with the Licensed Platform.¬† By opting to use these integrations to Third Party Products, Customer acknowledges that the Licensed Platform may transmit or"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 24,
    "content": "the Licensed Platform.¬† By opting to use these integrations to Third Party Products, Customer acknowledges that the Licensed Platform may transmit or exchange Customer Data with the Third-Party Products as authorized by Customer.¬† All exchanges will be visible in the Licensed Product as the Customer must supply proper authentication and authorization methods, often in the form of an access token.¬† Third-Party Products may be subject to the third-party provider's additional terms and may require an additional fee to such providers to use the Third-Party Products.¬† LangChain does not control and has no liability for Third Party Products, including their security, operation, functionality, or interoperability with the Licensed Platform.2.9 Affiliates. Any Affiliate of Customer will have the"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 25,
    "content": "their security, operation, functionality, or interoperability with the Licensed Platform.2.9 Affiliates. Any Affiliate of Customer will have the right to enter into an Order Form executed by such Affiliate and LangChain and this Agreement will apply to each such Order Form as if such Affiliate were a signatory to this Agreement. With respect to such Order Forms, such Affiliate becomes a party to this Agreement and references to Customer in this Agreement are deemed to be references to such Affiliate. Each Order Form is a separate obligation of the Customer entity that executes such Order Form, and no other Customer entity has any liability or obligation under such Order Form.¬†3. Fees and Payment3.1\tFees.¬† Customer will pay LangChain all fees as outlined in the applicable Order Form"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 26,
    "content": "or obligation under such Order Form.¬†3. Fees and Payment3.1\tFees.¬† Customer will pay LangChain all fees as outlined in the applicable Order Form (‚ÄúFees‚Äù). Except as outlined in Section 6.2, all payment obligations are non-cancelable, and Fees paid are non-refundable.¬†‚Äç3.2\tPayment Terms.¬† Except as otherwise outlined in the applicable Order Form, all Fees will be billed annually in advance. All invoices for Fees are due and payable within the time frame and in United States Dollars (\"USD\") outlined in the applicable Order Form, without deduction or setoff. Customer is responsible for providing complete and accurate billing and contact information to LangChain and notifying LangChain of any changes to such information.¬† If Customer fails to pay any undisputed portion of a past due invoice"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 27,
    "content": "to LangChain and notifying LangChain of any changes to such information.¬† If Customer fails to pay any undisputed portion of a past due invoice within ten (10) business days after receiving notice that its account is overdue, LangChain may, without limiting its other rights and remedies, suspend access to or use of the Licensed Platform until such amounts are paid in full (‚ÄúNon-Payment Suspension‚Äù). LangChain will not be obligated to continue to provide access or use of the Licensed Platform without payment of applicable Fees. LangChain will reinstate access to the Licensed Platform without undue delay once all past due payments have been made. LangChain will not suspend access to the Licensed Platform while Customer is disputing the applicable charges reasonably and in good faith and"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 28,
    "content": "made. LangChain will not suspend access to the Licensed Platform while Customer is disputing the applicable charges reasonably and in good faith and cooperating diligently to resolve the dispute.‚Äç3.3\tCredit Card Payment Terms.¬† If Customer elects to pay via credit card, then Customer is responsible for either (a) enabling auto-recharge on Customer‚Äôs payment instrument or (b) ensuring that Customer‚Äôs payment instrument has a sufficient positive balance to cover all Fees due.¬† If, for any reason, Customer has a negative balance on its payment account or its payment services provider (\"Payment Provider\") declines to authorize payment for any reason, then LangChain reserves the right to suspend access to the Licensed Platform until all Fees are paid in full.¬† Customer hereby authorizes"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 29,
    "content": "any reason, then LangChain reserves the right to suspend access to the Licensed Platform until all Fees are paid in full.¬† Customer hereby authorizes LangChain to charge Customer‚Äôs designated credit card account (or other means of payment) for the Fees outlined in the corresponding Order Form.¬† LangChain is not responsible for any handling, process, or related fees assessed by the Payment Provider.‚Äç3.4\tUse of Purchase Orders.¬† No additional or inconsistent terms of any purchase order, or other form provided by Customer, will modify or supplement this Agreement, regardless of any failure of LangChain to object to such terms, and any such additional or inconsistent terms in the purchase order will be void.¬†‚Äç3.5\tTaxes.¬† Customer is responsible for any sales, use, Goods and Services Tax"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 30,
    "content": "additional or inconsistent terms in the purchase order will be void.¬†‚Äç3.5\tTaxes.¬† Customer is responsible for any sales, use, Goods and Services Tax (GST), value-added, withholding, or similar taxes or levies that apply to its Orders Forms (identified or not), whether domestic or foreign (‚ÄúTaxes‚Äù), other than LangChain‚Äôs income tax. Fees and expenses are exclusive of Taxes.¬†‚Äç3.6\tSelf-Hosted Deployment Record Keeping.¬† For Self-Hosted Deployments, Customer shall provide a written verification of its actual usage of the Licensed Platform (\"Usage Report\") no more than once per year, either (a) within thirty (30) days before the end of the Initial Term (and each renewal term) or (b) upon at least thirty (30) days written notice from LangChain.¬† The Usage Report may be generated by running a"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 31,
    "content": "Term (and each renewal term) or (b) upon at least thirty (30) days written notice from LangChain.¬† The Usage Report may be generated by running a LangChain-provided script or through automated telemetry data collected by LangChain's online license key, using available reporting capabilities within the Licensed Platform, or by other mutually agreed method.¬† If the Usage Report shows Customer has exceeded its Subscription Metrics as outlined in the Order Form, LangChain shall invoice Customer and Customer shall pay such usage overage under Section 3.1 above.‚Äç3.7\tPurchases Through a Reseller. If Customer purchases a subscription to the Licensed Platform through a Reseller, the pricing and payment terms are between Customer and Reseller (‚ÄúReseller Terms‚Äù). Customer acknowledges: (a) all"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 32,
    "content": "Platform through a Reseller, the pricing and payment terms are between Customer and Reseller (‚ÄúReseller Terms‚Äù). Customer acknowledges: (a) all payments for the Licensed Platform procured via a Reseller will be made directly to the Reseller and per the Reseller Terms, and (b) if a Reseller notifies LangChain of its right to terminate or suspend access or use of the Licensed Platform, LangChain may terminate or suspend such access or use. LangChain will not be liable to Customer or any third party for any liabilities, claims, or expenses arising from or relating to any applicable Reseller Terms.¬†4. Proprietary¬†Rights and License4.1\tOwnership; Reservation of Rights.¬† As between LangChain and Customer, all rights, title, and interest in and to all intellectual property rights in the Licensed"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 33,
    "content": "Reservation of Rights.¬† As between LangChain and Customer, all rights, title, and interest in and to all intellectual property rights in the Licensed Platform and LangChain‚Äôs Confidential Information are and will remain owned exclusively by LangChain and its licensors. Ownership in all Updates, derivatives, modifications, new functionalities, enhancements, and customization related to the Licensed Platform created by or on behalf of LangChain will immediately vest in LangChain upon creation. Nothing in this Agreement will preclude or restrict LangChain from using or exploiting any concepts, ideas, techniques, or know-how of or related to the Licensed Platform.¬† Other than as expressly outlined in this Agreement, no license or other rights in or to the Licensed Platform or other LangChain"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 34,
    "content": "Licensed Platform.¬† Other than as expressly outlined in this Agreement, no license or other rights in or to the Licensed Platform or other LangChain intellectual property rights are granted to Customer, and all such rights are expressly reserved to LangChain and its licensors.¬†‚Äç4.2\tCustomer Data. ¬† As between Customer and LangChain, Customer Data and Customer Confidential Information are and will remain owned exclusively by Customer,¬† the User, or their licensors, as applicable. Customer warrants that it has all rights necessary to provide any information, data, or other materials that it provides hereunder, and Customer hereby grants LangChain a worldwide, limited-term license to utilize Customer Data as necessary for LangChain to access and use Customer Data to provide the Licensed"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 35,
    "content": "a worldwide, limited-term license to utilize Customer Data as necessary for LangChain to access and use Customer Data to provide the Licensed Platform in accordance with this Agreement and each Order Form executed hereunder.¬† LangChain agrees that it will not use Customer Data and/or AI Content to train on, develop, or otherwise improve its products.‚Äç4.3\tOperational Metadata.¬† Customer agrees that LangChain may collect and use Operational Metadata to operate, maintain, improve, and support the Licensed Platform, including for diagnostics, analytics, system performance, and reporting purposes.¬† LangChain will only disclose Operational Metadata externally if such data is (a) aggregated or anonymized with data across other customers, and (b) does not disclose the identity of Customer,"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 36,
    "content": "externally if such data is (a) aggregated or anonymized with data across other customers, and (b) does not disclose the identity of Customer, Personal Data of its Users, or any Customer Confidential Information.‚Äç4.4\tFeedback.¬† To the extent that Customer or its Users provide any recommendations, suggestions, proposals, ideas, improvements, or other feedback regarding the Licensed Platform or Documentation (‚ÄúFeedback‚Äù), Customer hereby grants LangChain an irrevocable, perpetual, royalty-free license to use, incorporate, and further develop such Feedback without any restrictions or attribution, provided that Customer is not identified as the source of such Feedback. LangChain acknowledges any Feedback is provided ‚Äúas is‚Äù without warranties of any kind.‚Äç5. Confidentiality5.1\tDefinition of"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 37,
    "content": "source of such Feedback. LangChain acknowledges any Feedback is provided ‚Äúas is‚Äù without warranties of any kind.‚Äç5. Confidentiality5.1\tDefinition of Confidential Information.¬† ‚ÄúConfidential Information‚Äù means all information disclosed by a party (‚ÄúDisclosing Party‚Äù) to the other party (‚ÄúReceiving Party‚Äù), whether orally or in writing, that is designated as confidential or that reasonably should be understood to be confidential given the nature of the information and the circumstances of disclosure. Confidential Information of Customer includes Customer Data, AI Content; Confidential Information of LangChain includes the Licensed Platform including any discussions or information related to any Beta Releases; and Confidential Information of each party includes the terms of this Agreement"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 38,
    "content": "any discussions or information related to any Beta Releases; and Confidential Information of each party includes the terms of this Agreement and all Order Forms (including pricing), as well as business and marketing plans, technology and technical information, product plans and designs, and business processes disclosed by such party. However, Confidential Information does not include any information that the Receiving Party can demonstrate (a) is or becomes generally known to the public without breach of any obligation owed to the Disclosing Party; (b) was known to the Receiving Party before its disclosure by the Disclosing Party without breach of any obligation owed to the Disclosing Party; ÔºàcÔºâis received from a third party without breach of any obligation owed to the Disclosing Party;"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 39,
    "content": "of any obligation owed to the Disclosing Party; ÔºàcÔºâis received from a third party without breach of any obligation owed to the Disclosing Party; or (d) was independently developed by the Receiving Party.‚Äç5.2\tProtection of Confidential Information.¬† The Receiving Party will (a) use the same degree of care that it uses to protect the confidentiality of its own confidential information of like kind (but not less than reasonable care); (b) not use any Confidential Information for any purpose outside the scope of this Agreement; and except as otherwise expressly consented to by an authorized representative of the Disclosing Party, limit access to Confidential Information to those of its and its Affiliates‚Äô employees and contractors who need that access for purposes consistent with this"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 40,
    "content": "to Confidential Information to those of its and its Affiliates‚Äô employees and contractors who need that access for purposes consistent with this Agreement and¬† who are bound by obligations of confidentiality at least as restrictive as those set forth in this Agreement. Neither party will disclose the terms of this Agreement or any Order Form to any third party other than its Affiliates, legal counsel, and accountants without the other party‚Äôs prior written consent, on condition that a party that makes any such disclosure to its Affiliate, legal counsel, or accountants will remain responsible for such Affiliate‚Äôs, legal counsel‚Äôs, and accountants‚Äô compliance with this ‚ÄúConfidentiality‚Äù Section. Upon the written request of the Disclosing Party, the Receiving Party will return or destroy all"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 41,
    "content": "compliance with this ‚ÄúConfidentiality‚Äù Section. Upon the written request of the Disclosing Party, the Receiving Party will return or destroy all Confidential Information without undue delay, except for Confidential Information stored in routine back-up media and not accessible in the ordinary course of business or is otherwise required to be retained for Receiving Party‚Äôs legal, compliance, tax, or document retention purposes.‚Äç5.3\tCompelled Disclosure. The Receiving Party may disclose Confidential Information to the extent compelled by law to do so, on condition that the Receiving Party gives the Disclosing Party prior notice of the compelled disclosure (to the extent legally permitted) and reasonable assistance, at the Disclosing Party‚Äôs cost, if the Disclosing Party wishes to seek a"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 42,
    "content": "disclosure (to the extent legally permitted) and reasonable assistance, at the Disclosing Party‚Äôs cost, if the Disclosing Party wishes to seek a protective order or other appropriate remedy. If such protective order or other remedy is not obtained (or the Disclosing Party does not seek such protective order or other remedy), the Receiving Party will use commercially reasonable efforts to furnish only that portion of the Confidential Information which it is advised by counsel is legally required to be disclosed, and will request that such disclosed Confidential Information be treated confidentially. Receiving Party‚Äôs obligations under this Section 5 shall survive for as long as Disclosing Party‚Äôs Confidential Information remains in Receiving Party‚Äôs possession.6. Representation,"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 43,
    "content": "this Section 5 shall survive for as long as Disclosing Party‚Äôs Confidential Information remains in Receiving Party‚Äôs possession.6. Representation, Warranties, Exclusive Remedies, Disclaimers6.1 \tGeneral Warranty.¬† Each party represents and warrants (a) that it has validly entered into this Agreement and has the legal power to do so; (b) no authorization or approval from any third party is required in connection with such party‚Äôs execution, delivery or performance of this Agreement, and ÔºàcÔºâ the execution, delivery and performance of this Agreement does not violate the terms or conditions of any other agreement to which it is a party or by which it is otherwise bound.‚Äç6.2 \tLangChain Limited Warranties.¬† LangChain warrants that (a) the Licensed Platform will perform materially in accordance"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 44,
    "content": "it is otherwise bound.‚Äç6.2 \tLangChain Limited Warranties.¬† LangChain warrants that (a) the Licensed Platform will perform materially in accordance with the applicable Documentation; (b) LangChain will not materially decrease the overall functionality of the Licensed Platform during the current Subscription Term.¬† Customer‚Äôs exclusive remedy and LangChain‚Äôs entire liability for a breach of the above warranties will be the correction of the deficient service that caused the breach of warranty, provision of comparable functionality, or, if LangChain cannot accomplish the foregoing in a commercially reasonable manner, as determined in its reasonable discretion, LangChain may terminate the deficient service and refund Customer any prepaid Fees related to the Licensed Platform prorated for the"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 45,
    "content": "discretion, LangChain may terminate the deficient service and refund Customer any prepaid Fees related to the Licensed Platform prorated for the remainder of the Subscription Term following notice of the breach of warranty.¬†‚Äç6.3\tDisclaimers. EXCEPT AS EXPRESSLY PROVIDED HEREIN, NEITHER PARTY OR ITS LICENSORS MAKES ANY WARRANTY OF ANY KIND, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, AND EACH PARTY AND ITS LICENSORS SPECIFICALLY DISCLAIM ALL IMPLIED WARRANTIES, INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, OR NON-INFRINGEMENT, TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW. LANGCHAIN DOES NOT WARRANT THAT LICENSED PLATFORM WILL BE ERROR-FREE OR UNINTERRUPTED, WILL MEET CUSTOMER‚ÄôS REQUIREMENTS OR EXPECTATIONS, OR THAT ITS SECURITY"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 46,
    "content": "DOES NOT WARRANT THAT LICENSED PLATFORM WILL BE ERROR-FREE OR UNINTERRUPTED, WILL MEET CUSTOMER‚ÄôS REQUIREMENTS OR EXPECTATIONS, OR THAT ITS SECURITY MEASURES WILL BE SUFFICIENT TO PREVENT THIRD-PARTY ACCESS TO CUSTOMER DATA.¬†7. Indemnification7.1\tIndemnification by LangChain. LangChain will defend Customer from and against any third-party claim to the extent alleging that the Licensed Platform, when used by Customer as authorized in this Agreement, infringes a third party‚Äôs patent, copyright, trademark, or trade secret rights, and will indemnify and hold harmless Customer against any damages or costs awarded against Customer (including reasonable attorneys‚Äô fees) or agreed to in settlement by LangChain resulting from the claim.¬†‚Äç7.2\tIndemnification by Customer. Customer will defend"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 47,
    "content": "reasonable attorneys‚Äô fees) or agreed to in settlement by LangChain resulting from the claim.¬†‚Äç7.2\tIndemnification by Customer. Customer will defend LangChain from and against any third-party claim to the extent resulting from Customer Data (when used by LangChain as authorized in this Agreement) including allegations that Customer Data infringes a third party‚Äôs patent, copyright, trademark or trade secret rights, and will indemnify and hold harmless LangChain against any damages or costs awarded against LangChain (including reasonable attorneys‚Äô fees) or agreed to in settlement by Customer resulting from the claim.‚Äç7.3\tProcedures. The indemnifying party‚Äôs obligations in this Section 7 are subject to receiving (a) prompt notice of the claim, (b) the exclusive right to control and direct"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 48,
    "content": "party‚Äôs obligations in this Section 7 are subject to receiving (a) prompt notice of the claim, (b) the exclusive right to control and direct the investigation, defense, and settlement of the claim, and ÔºàcÔºâall reasonably necessary cooperation of the indemnified party, at the indemnifying party‚Äôs expense. The indemnifying party may not settle any claim without the indemnified party‚Äôs prior consent if the settlement would require the indemnified party to admit fault or take or refrain from taking any action (other than relating to use of the Licensed Platform, when LangChain is the indemnifying party). The indemnified party may participate in a claim with its own counsel at its own expense.‚Äç7.4\tMitigation and Exceptions. In response to an actual or potential infringement claim, if required"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 49,
    "content": "a claim with its own counsel at its own expense.‚Äç7.4\tMitigation and Exceptions. In response to an actual or potential infringement claim, if required by settlement or injunction or as LangChain determines necessary to avoid material liability, LangChain may at its option: (a) procure rights for Customer‚Äôs continued use of the Licensed Platform, (b) replace or modify the allegedly infringing portion of the Licensed Platform to avoid infringement without reducing the Licensed Platform's overall functionality orÔºàcÔºâ ¬†terminate the affected Order Form and refund to Customer any pre-paid, unused fees for the terminated portion of the Subscription Term. LangChain‚Äôs obligations in Section 7.1 do not apply (1) to infringement resulting from Customer‚Äôs unauthorized modification of the Licensed"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 50,
    "content": "Term. LangChain‚Äôs obligations in Section 7.1 do not apply (1) to infringement resulting from Customer‚Äôs unauthorized modification of the Licensed Platform or use of the Licensed Platform in combination with items not specified in the Documentation or provided by LangChain (including Third-Party Products), (2) to infringement resulting from Licensed Platform other than the most recent release (for Self-Hosted Deployments), (3) to unauthorized use of the Licensed Platform, (4) if Customer settles or makes any admissions about a claim without LangChain‚Äôs prior consent, (5) if Customer continues to use the Licensed Platform (or any element thereof) after being notified of allegedly infringing activity or informed of modifications that would have avoided the alleged infringement, or (6) to"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 51,
    "content": "after being notified of allegedly infringing activity or informed of modifications that would have avoided the alleged infringement, or (6) to Beta Releases and/or Free Access Subscriptions. This Section 7 sets out Customer‚Äôs exclusive remedy and LangChain‚Äôs entire liability regarding infringement of third-party intellectual property rights.‚ÄçCustomer‚Äôs obligations in Section 7.2 do not apply (1) to infringement resulting from LangChain‚Äôs unauthorized modification of Customer Data, (2) to use of the Customer Data by LangChain not in compliance with this Agreement, (3) if LangChain settles or makes any admissions about a claim without Customer‚Äôs prior consent, (4) if LangChain continues to use the Customer Data (or any element thereof) after being notified of allegedly infringing activity"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 52,
    "content": "prior consent, (4) if LangChain continues to use the Customer Data (or any element thereof) after being notified of allegedly infringing activity or informed of modifications that would have avoided the alleged infringement. This Section 7 sets out LangChain‚Äôs exclusive remedy and Customer‚Äôs entire liability regarding infringement of third-party intellectual property rights.‚Äç¬†8. Limitation¬†of Liability8.1\tLimitation of Liability.¬†¬†TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, EXCEPT WITH REGARD TO LIABILITY (A) FOR EITHER INTENTIONAL MISUSE OF THE OTHER PARTY‚ÄôS CONFIDENTIAL INFORMATION, (B) FOR EITHER PARTY‚ÄôS INDEMNIFICATION OBLIGATIONS UNDER SECTION 7, AND (C) ARISING FROM EITHER PARTY‚ÄôS GROSS NEGLIGENCE, FRAUD OR WILLFUL MISCONDUCT, EACH PARTY‚ÄôS TOTAL CUMULATIVE LIABILITY TO THE"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 53,
    "content": "UNDER SECTION 7, AND (C) ARISING FROM EITHER PARTY‚ÄôS GROSS NEGLIGENCE, FRAUD OR WILLFUL MISCONDUCT, EACH PARTY‚ÄôS TOTAL CUMULATIVE LIABILITY TO THE OTHER PARTY WITH RESPECT TO ANY CLAIM RELATING TO OR ARISING OUT OF THE SERVICES OR THE AGREEMENT WILL NOT EXCEED THE FEES PAID OR PAYABLE TO LANGCHAIN UNDER THIS AGREEMENT DURING THE TWELVE (12) MONTHS IMMEDIATELY PRECEDING THE FIRST EVENT GIVING RISE TO SUCH CLAIM. EACH PARTY AGREES THAT THE FOREGOING IS AN AGREED ALLOCATION OF RISK AND IS A REFLECTION OF THE RIGHTS AND OBLIGATIONS AGREED UPON BY CUSTOMER AND LANGCHAIN IN THIS AGREEMENT. THIS LIMITATION APPLIES REGARDLESS OF THE NATURE OF THE CLAIM, WHETHER UNDER CONTRACT, TORT (INCLUDING NEGLIGENCE), STATUTE OR ANY OTHER LEGAL THEORY.‚Äç8.2\tDamages Exclusion. IN NO EVENT WILL EITHER PARTY OR"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 54,
    "content": "WHETHER UNDER CONTRACT, TORT (INCLUDING NEGLIGENCE), STATUTE OR ANY OTHER LEGAL THEORY.‚Äç8.2\tDamages Exclusion. IN NO EVENT WILL EITHER PARTY OR ITS AFFILIATES HAVE ANY LIABILITY ARISING OUT OF OR RELATED TO THIS AGREEMENT FOR ANY LOST PROFITS, REVENUES, GOODWILL, OR INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, COST OF COVER, BUSINESS INTERRUPTION, OR PUNITIVE DAMAGES, WHETHER AN ACTION IS IN CONTRACT OR TORT AND REGARDLESS OF THE THEORY OF LIABILITY, EVEN IF A PARTY OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES OR IF A PARTY‚ÄôS OR ITS AFFILIATES‚Äô REMEDY OTHERWISE FAILS OF ITS ESSENTIAL PURPOSE. THE FOREGOING DISCLAIMER WILL NOT APPLY TO THE EXTENT PROHIBITED BY LAW.‚Äç‚Äç9. Term and Termination9.1 \tTerm of Agreement. This Agreement will commence on the Effective Date"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 55,
    "content": "WILL NOT APPLY TO THE EXTENT PROHIBITED BY LAW.‚Äç‚Äç9. Term and Termination9.1 \tTerm of Agreement. This Agreement will commence on the Effective Date and continue until terminated as permitted herein (the ‚ÄúTerm‚Äù). If there are no active Order Forms, this Agreement shall automatically expire ninety (90) days following the termination or expiration of all Order Forms.¬†‚Äç9.2¬† \tSubscription Term.¬† The initial Subscription Term and any applicable renewal Subscription Term will commence and expire following the start date and end date outlined in the Order Form. Unless otherwise specified in an Order Form, a Subscription Term will automatically renew for one (1) year on the same terms, unless either party gives the other party written notice (email is sufficient) of non-renewal at least thirty (30)"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 56,
    "content": "one (1) year on the same terms, unless either party gives the other party written notice (email is sufficient) of non-renewal at least thirty (30) days before the end of the relevant Subscription Term.¬†‚Äç9.3 \tSuspension.¬† LangChain may¬† disable the Customer‚Äôs or any User‚Äôs access if, in LangChain‚Äôs reasonable determination, the Customer or any User (i)¬† poses a security risk to the Licensed Platform, (ii) may adversely impact LangChain, the Licensed Platform or the networks or data of any other LangChain customer, business partner or service provider, or (iii) are in violation of applicable law or the terms of this Agreement including but not limited to Section 2.4 (License Restrictions). LangChain will¬† provide as much notice as is reasonably practicable under the circumstances, and to"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 57,
    "content": "limited to Section 2.4 (License Restrictions). LangChain will¬† provide as much notice as is reasonably practicable under the circumstances, and to reinstate the Customer‚Äôs and/or any User‚Äôs access to the Licensed Platform as soon as reasonably practicable following resolution of the issue.‚Äç9.4\tFree Access Subscriptions.¬† If Customer is accessing the Licensed Platform via a Free Access Subscription and has not otherwise agreed to purchase any other support or services, each party may terminate this Agreement upon written notice to the other party.¬†‚Äç9.5¬† \tTermination.¬† Either party may terminate this Agreement or any Order Form by written notice if the other party is in material breach of this Agreement, where such material breach is not cured within thirty (30) days after written notice of"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 58,
    "content": "if the other party is in material breach of this Agreement, where such material breach is not cured within thirty (30) days after written notice of the breach from the non-breaching party, or with immediate effect where such material breach cannot be cured. For the avoidance of doubt, Customer‚Äôs noncompliance with Section 2,4 (License Restrictions) is deemed a material breach of this Agreement.¬† This Agreement may be terminated by either party with immediate effect if the other party becomes the subject of a petition in bankruptcy or other proceeding relating to insolvency, receivership, liquidation, or assignment for the benefit of creditors, and such petition or proceeding is not dismissed within forty-five (45) days.¬†‚Äç9.6\tData Export & Deletion For Cloud Deployments.¬†¬†‚Äç\t(A)\tDuring a"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 59,
    "content": "and such petition or proceeding is not dismissed within forty-five (45) days.¬†‚Äç9.6\tData Export & Deletion For Cloud Deployments.¬†¬†‚Äç\t(A)\tDuring a Subscription Term or within thirty (30) days thereafter upon Customer's written request, Customer may export Customer Data from the Cloud Deployments as may be described in the Documentation.¬†¬†\t(B)\tAfter termination or expiration of this Agreement, within thirty (30) days of the Customer‚Äôs written request, LangChain shall delete all Customer Data in its custody and control.‚Äç9.7\tEffect of Termination.¬† Upon the termination of this Agreement for any reason: (a) all outstanding Order Forms and access to the Licensed Platform will automatically terminate; (b) Customer and its Users will immediately cease access and use of the Licensed Platform, other"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 60,
    "content": "the Licensed Platform will automatically terminate; (b) Customer and its Users will immediately cease access and use of the Licensed Platform, other than for retrieval purposes provided in above Section 9.6(A), as applicable;ÔºàcÔºâ all outstanding payment obligations of Customer will become due and payable immediately. ¬† Except where an exclusive remedy is provided herein, exercising a remedy under this Agreement, including termination, does not limit other remedies a party may have.‚Äç9.8¬† \tSurviving Provisions.¬† The Sections titled ‚ÄúFees and Payment,‚Äù ‚ÄúProprietary Rights and Licenses,‚Äù ‚ÄúConfidentiality,‚Äù ‚ÄúRepresentation, Warranties, Exclusive Remedies, Disclaimers‚Äù, ‚ÄúTerm and Termination,‚Äù ‚ÄúIndemnification,‚Äù ‚ÄúLimitation of Liability,‚Äù and ‚ÄúGeneral Provisions‚Äù will survive any termination of"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 61,
    "content": "Disclaimers‚Äù, ‚ÄúTerm and Termination,‚Äù ‚ÄúIndemnification,‚Äù ‚ÄúLimitation of Liability,‚Äù and ‚ÄúGeneral Provisions‚Äù will survive any termination of this Agreement.‚Äç10. General Provision10.1 \tPublicity and Reference.¬† Except as otherwise outlined in the Order Form, neither party may publicly announce this Agreement except with the other party's prior written consent or as required by applicable laws.¬† However, LangChain may include Customer and its trademarks in LangChain's customer lists and promotional materials but will cease this use at Customer's written request.¬†¬†‚Äç10.2 Export Control.¬† Each party will comply with all applicable Export Control and Sanctions Laws and Regulations in connection with providing and using the Licensed Platform. Without limiting the foregoing, (a) each party"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 62,
    "content": "and Sanctions Laws and Regulations in connection with providing and using the Licensed Platform. Without limiting the foregoing, (a) each party represents that it is not listed on any list of entities or individuals who are restricted from receiving U.S. services or items subject to U.S. jurisdiction (including but not limited to the Specially Designated Nationals and Blocked Persons List and the Entity List) nor is it owned or controlled by any such listed entity; (b) Customer will not, and will ensure that Users do not, violate any Export Control and Sanctions Laws and Regulations, or cause any such violation to occur; and (c) Customer will not use or cause any person to use the Licensed Platform to store, retrieve, or transmit technical data controlled under the U.S. International"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 63,
    "content": "will not use or cause any person to use the Licensed Platform to store, retrieve, or transmit technical data controlled under the U.S. International Traffic in Arms Regulations.¬†‚Äç10.3 Anti-Corruption. ¬† Neither party has promised, made, or received any bribe, kickback, or other similar payment or transfer of value from or to any director, officer, employee, agent, or other representative of the other party in connection with this Agreement. Reasonable gifts, entertainment, sponsorships, and donations do not violate the above restriction.¬†‚Äç10.4 U.S. Government Rights.¬† If Customer, or any User, is a branch, agency, or instrumentality of the United States Government, the following provision applies: The Licensed Platform and Documentation comprise ‚Äúcommercial computer software‚Äù and"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 64,
    "content": "of the United States Government, the following provision applies: The Licensed Platform and Documentation comprise ‚Äúcommercial computer software‚Äù and ‚Äúcommercial computer software documentation‚Äù as such terms are used in 48 C.F.R. 12.212 and are provided to the Government (a) for acquisition by or on behalf of civilian agencies, consistent with the policy in 48 C.F.R. 12.212; or (b) for acquisition by or on behalf of units of the Department of Defense, consistent with the policies in 48 C.F.R. 227.7202-1 and 22.7202-3.¬†‚Äç10.5\tGoverning Law & Dispute Resolution.¬† This Agreement will be governed by and construed under the laws of the State of Delaware without reference to conflict of laws principles.¬† The provisions of the United Nations Convention of Contracts for the International Sale of"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 65,
    "content": "without reference to conflict of laws principles.¬† The provisions of the United Nations Convention of Contracts for the International Sale of Goods and the Uniform Computer Information Transactions Acts will not apply to this Agreement in any manner whatsoever. The parties will be subject to the exclusive jurisdiction of the state and federal courts located in the State of Delaware, and the parties agree and consent to the exclusive jurisdiction and venue of such courts.‚Äç10.6 Notices. ¬† Notices must be in English, in writing and will be deemed given upon receipt, after being sent using a method that provides for confirmation of delivery (including through an automated receipt or by electronic log) to the postal address(es) or email address provided by a party. Email notices shall be"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 66,
    "content": "(including through an automated receipt or by electronic log) to the postal address(es) or email address provided by a party. Email notices shall be deemed received the day after being sent in the absence of a message invalidating delivery. All notices will be given using the contact information with respect to each party set forth in the applicable Order Form or such other contact information as may be designated by a party by giving written notice to the other party pursuant to this Section. Billing communications will be addressed to dbt Labs at ar@langchain.dev or to Customer at the billing contact designated on each Order Form. Notices to LangChain must be copied to Legal@langchain.dev to be effective.‚Äç10.7 Force Majeure. ¬† Neither party will be liable nor responsible to the other"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 67,
    "content": "to LangChain must be copied to Legal@langchain.dev to be effective.‚Äç10.7 Force Majeure. ¬† Neither party will be liable nor responsible to the other party, nor be deemed to have defaulted under or breached this Agreement, for any failure or delay in its performance under this Agreement due to any cause beyond its reasonable control, including without limitation elements of nature or acts of God, war, riots, civil disorders, rebellions, revolutions, pandemics or epidemics (or similar regional health crisis), actions or decrees of governmental bodies, acts or threats of terrorism, strikes, labor disputes, failure of utilities or telecommunications, or other causes beyond the reasonable control of the affected Party (each a ‚ÄúForce Majeure Event‚Äù). The party suffering a Force Majeure Event"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 68,
    "content": "or other causes beyond the reasonable control of the affected Party (each a ‚ÄúForce Majeure Event‚Äù). The party suffering a Force Majeure Event will use reasonable efforts to mitigate against the effects of such a Force Majeure Event.¬†‚Äç10.8 Assignment.¬† Neither party may assign this Agreement, in whole or part, without the prior written consent of the other party, which will not be unreasonably withheld; however, either party may assign this Agreement without consent (a) to an Affiliate, or (b) in the event of a merger, corporate reorganization, or to a purchaser of a party‚Äôs business entity in the event of a sale of all or substantially all of its business or assets. Subject to the foregoing, this Agreement will be binding upon and inure to the benefit of the parties and their respective"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 69,
    "content": "of its business or assets. Subject to the foregoing, this Agreement will be binding upon and inure to the benefit of the parties and their respective successors and permitted assigns. Any other attempt to transfer a party‚Äôs rights or obligations under this Agreement is void.‚Äç10.9\tAttorneys‚Äô Fees and Costs.¬† The prevailing party in any action to enforce this Agreement will be entitled to recover its reasonable attorneys‚Äô fees and costs in connection with such action.‚Äç10.10\tRelationship of the Parties.¬† The parties are independent contractors. This Agreement does not create a partnership, franchise, joint venture, agency, fiduciary, or employment relationship between the Parties. Each party will be solely responsible for payment of all compensation owed to its employees, as well as all"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 70,
    "content": "relationship between the Parties. Each party will be solely responsible for payment of all compensation owed to its employees, as well as all employment-related taxes.‚Äç10.11\tEntire Agreement; Order of Precedence.¬† This Agreement (together with any Order Forms, and linked terms) contains the entire agreement of the parties concerning the subject matter of this Agreement and supersedes all prior communications, representations, agreements, and understandings, either oral or written, between the parties concerning its subject matter. In the event of any conflict or inconsistency among the following documents, the order of precedence will be (1) the DPA, (2) the applicable Order Form, (3) this Agreement, and (4) any links provided herein.¬†‚Äç10.12\tModifications.¬† Any amendment or modification"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 71,
    "content": "the DPA, (2) the applicable Order Form, (3) this Agreement, and (4) any links provided herein.¬†‚Äç10.12\tModifications.¬† Any amendment or modification to this Agreement or any Order Form must be in writing and executed by both parties to be effective.¬† Except for the Order Form, no terms or conditions set forth in any confirmation, acceptance, written or oral communication, purchase order, or any other similar document will modify or supplement this Agreement or have any force or effect whatsoever unless signed by both parties.‚Äç10.13 Special Terms for Free Access Subscriptions and Plus Plans. Notwithstanding any contrary terms in Section 10.12, and only with respect to Customers on a Free Access Subscription or the Developer, Plus, Startup plan, or any plan commonly known as a ‚Äúself-service"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 72,
    "content": "and only with respect to Customers on a Free Access Subscription or the Developer, Plus, Startup plan, or any plan commonly known as a ‚Äúself-service plan‚Äù, LangChain may modify this Agreement at any time by posting such modification in the Licensed Platform or by providing Customer notice of such update, and any such modification shall automatically go into effect thirty (30) days after notice is given or it is so posted. If Customer does not agree to the terms of any such modification, Customer's sole remedy is to provide LangChain with written notice during such thirty (30)-day period of Customer's objection and desire to terminate the Agreement, in which case the Agreement will terminate on the last day of such thirty (30)-day period. By continuing to use the Licensed Platform after"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 73,
    "content": "in which case the Agreement will terminate on the last day of such thirty (30)-day period. By continuing to use the Licensed Platform after any such modification goes into effect, Customer agrees to the terms of any such modification; for clarity, this Section 10.12 shall not apply to Customers purchasing any plan commonly known as an ‚Äúenterprise plan‚Äù.‚Äç10.14 Miscellaneous. If a provision of this Agreement is unenforceable or invalid, the provision will be revised to best accomplish the objectives of the parties as evidenced by this Agreement, and the remainder of this Agreement will continue in full force. This Agreement is in the English language only, which language is controlling in all respects, and all versions of this Agreement in any other language are for accommodation only and"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 74,
    "content": "language only, which language is controlling in all respects, and all versions of this Agreement in any other language are for accommodation only and will not be binding on the parties. Waiver of any term of this Agreement or forbearance to enforce any term by either party will not constitute a waiver as to any subsequent breach or failure of the same term, or a waiver of any other term of this Agreement. There are no third-party beneficiaries to this Agreement.‚Äç‚ÄçProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops!"
  },
  {
    "url": "https://www.langchain.com/terms-of-service",
    "chunk_id": 75,
    "content": "AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#conclusion",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=1",
    "chunk_id": 0,
    "content": "Resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upGuides\n\n\nFilters\n\nGuides & ReportsUse cases & inspirationMethodsThank you! Your submission has been received!Oops! Something went wrong while submitting the form.Use cases & inspirationUpcomingBuilt with LangGraphSee customer stories"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=1",
    "chunk_id": 1,
    "content": "Use cases & inspirationUpcomingBuilt with LangGraphSee customer stories\n\n\nGuides & ReportsUpcomingThe Definitive Guide to Testing LLM ApplicationsDownload now\n\n\nGuides & ReportsUpcomingThe Definitive Guide to Testing LLM ApplicationsDownload now\n\n\nUse cases & inspirationUpcomingBreakout Agentic AppsGet inspired\n\n\nUse cases & inspirationUpcomingBreakout Agentic AppsGet inspired\n\n\nGuides & ReportsUpcomingLangChain State of AI 2024 ReportSee product data\n\n\nGuides & ReportsUpcomingLangChain State of AI 2024 ReportSee product data\n\n\nGuides & ReportsUpcomingState of AI Agents Survey & ReportRead the report\n\n\nGuides & ReportsUpcomingState of AI Agents Survey & ReportRead the report\n\n\nMethodsUpcomingLLM EvaluationsLearn the method\n\n\nMethodsUpcomingLLM EvaluationsLearn the method"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=1",
    "chunk_id": 2,
    "content": "MethodsUpcomingLLM EvaluationsLearn the method\n\n\nMethodsUpcomingLLM EvaluationsLearn the method\n\n\nShow moreReady to start shipping ‚Ä®reliable, GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Contact UsSign UpProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://js.langchain.com/",
    "chunk_id": 0,
    "content": "ü¶úÔ∏èüîó Langchain"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 0,
    "content": "Introduction | ü¶úÔ∏èüîó Langchain"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 1,
    "content": "Skip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMorePeopleCommunityError referenceExternal guidesContributingv0.3v0.3v0.2v0.1ü¶úüîóLangSmithLangSmith DocsLangChain HubLangServePython DocsChatSearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to add memory to chatbotsHow to use"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 2,
    "content": "semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to add memory to chatbotsHow to use example selectorsInstallationHow to stream responses from an LLMHow to stream chat model responsesHow to embed text dataHow to use few shot examples in chat modelsHow to cache model responsesHow to cache chat model responsesRicher outputsHow to use few shot examplesHow to use output parsers to parse an LLM response into structured formatHow to return structured data from a modelHow to add ad-hoc tool calling capability to LLMs and Chat ModelsRicher outputsHow to do per-user retrievalHow to track token usageHow to track token usageHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to use legacy LangChain"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 3,
    "content": "token usageHow to track token usageHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to use legacy LangChain Agents (AgentExecutor)How to add values to a chain's stateHow to attach runtime configuration to a RunnableHow to cache embedding resultsHow to attach callbacks to a moduleHow to pass callbacks into a module constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to await callbacks in serverless environmentsHow to cancel executionHow to split by characterHow to init any model in one lineHow to do retrievalHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to write a custom retriever classHow to"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 4,
    "content": "with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to write a custom retriever classHow to create ToolsHow to debug your LLM appsHow to load CSV dataHow to write a custom document loaderHow to load data from a directoryHow to load HTMLHow to load MarkdownHow to load PDF filesHow to load JSON dataHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by similarityHow to use reference examplesHow to handle long textHow to do extraction without using function callingFallbacksFew Shot Prompt TemplatesHow to filter messagesHow to run custom functionsHow to build an LLM generated UIHow to construct knowledge graphsHow to map values to a databaseHow to"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 5,
    "content": "to filter messagesHow to run custom functionsHow to build an LLM generated UIHow to construct knowledge graphsHow to map values to a databaseHow to improve results with promptingHow to add a semantic layer over the databaseHow to reindex data to keep your vectorstore in-sync with the underlying data sourceLangChain Expression Language CheatsheetHow to get log probabilitiesHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to generate multiple embeddings per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to generate multiple queries to retrieve data forHow to try to fix errors in output parsingHow to parse JSON outputHow to parse XML outputHow to invoke runnables in"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 6,
    "content": "queries to retrieve data forHow to try to fix errors in output parsingHow to parse JSON outputHow to parse XML outputHow to invoke runnables in parallelHow to retrieve the whole document for a chunkHow to partially format prompt templatesHow to add chat historyHow to return citationsHow to return sourcesHow to stream from a question-answering chainHow to construct filtersHow to add examples to the promptHow to deal with high cardinality categorical variablesHow to handle multiple queriesHow to handle multiple retrieversHow to handle cases where no queries are generatedHow to recursively split text by charactersHow to reduce retrieval latencyHow to route execution within a chainHow to do \"self-querying\" retrievalHow to chain runnablesHow to split text by tokensHow to deal with large"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 7,
    "content": "to route execution within a chainHow to do \"self-querying\" retrievalHow to chain runnablesHow to split text by tokensHow to deal with large databasesHow to use prompting to improve resultsHow to do query validationHow to stream agent data to the clientHow to stream structured output to the clientHow to streamHow to create a time-weighted retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to call tools with multimodal dataHow to force tool calling behaviorHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to use LangChain toolsHow to handle tool errorsHow to use few-shot prompting with tool"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 8,
    "content": "to stream events from a toolHow to stream tool callsHow to use LangChain toolsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to trim messagesHow use a vector store to retrieve dataHow to create and query vector storesConceptual GuideAgentsArchitectureCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputstString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraph.jsVersionsv0.3v0.2Migrating from v0.0 memoryHow to"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 9,
    "content": "callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraph.jsVersionsv0.3v0.2Migrating from v0.0 memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryRelease PolicySecurityIntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by large language models (LLMs).LangChain simplifies every stage of the LLM application lifecycle:Development: Build your applications using LangChain's open-source building blocks, components, and third-party integrations."
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 10,
    "content": "Use LangGraph.js to build stateful agents with first-class streaming and human-in-the-loop support.Productionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.Concretely, the framework consists of the following open-source libraries:@langchain/core: Base abstractions and LangChain Expression Language.@langchain/community: Third party integrations.Partner packages (e.g. @langchain/openai, @langchain/anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on @langchain/core.langchain: Chains, agents, and retrieval strategies that make up an"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 11,
    "content": "split into their own lightweight packages that only depend on @langchain/core.langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.LangGraph.js: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.noteThese docs focus on the JavaScript LangChain library. Head here for docs on the Python LangChain library.Tutorials‚ÄãIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials."
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 12,
    "content": "This is the best place to get started.These are the best ones to get started with:Build a Simple LLM ApplicationBuild a ChatbotBuild an AgentLangGraph.js quickstartExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here.How-To Guides‚ÄãHere you'll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\nThese how-to guides don't cover topics in depth - you'll find that material in the Tutorials and the API Reference."
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 13,
    "content": "However, these guides will help you quickly accomplish common tasks.Check out LangGraph-specific how-tos here.Conceptual Guide‚ÄãIntroductions to all the key parts of LangChain you'll need to know! Here you'll find high level explanations of all LangChain concepts.For a deeper dive into LangGraph concepts, check out this page.API reference‚ÄãHead to the reference section for full documentation of all classes and methods in the LangChain JavaScript packages.Ecosystem‚Äãü¶úüõ†Ô∏è LangSmith‚ÄãTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.ü¶úüï∏Ô∏è LangGraph‚ÄãBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin,"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 14,
    "content": "with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.Additional resources‚ÄãSecurity‚ÄãRead up on our Security best practices to make sure you're developing safely with LangChain.Integrations‚ÄãLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.Contributing‚ÄãCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.Was this page helpful?You can also leave detailed feedback on GitHub.NextTutorialsTutorialsHow-To GuidesConceptual GuideAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional"
  },
  {
    "url": "https://js.langchain.com/docs/introduction/",
    "chunk_id": 15,
    "content": "also leave detailed feedback on GitHub.NextTutorialsTutorialsHow-To GuidesConceptual GuideAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesSecurityIntegrationsContributingCommunityLangChain ForumTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2025 LangChain, Inc."
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 0,
    "content": "Unify Launches Agents for Account Qualification using LangGraph and LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnify Launches Agents for Account Qualification using LangGraph and LangSmith\n\n6 min read\nOct 8, 2024"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 1,
    "content": "This is a guest blog post written by Sam and Connor at Unify. Unify is reinventing how go-to-market teams work using generative AI. As part of this transformation, they are launching a new agents feature today (powered by LangGraph and LangSmith). We had the pleasure of learning more about the engineering journey taken to launch this feature, and thought it would be a great story to share.Agents are a new feature we‚Äôre launching alongside a broader automation suite we call Plays. Agents are effectively research tools‚Äîthey can research companies or people by searching the web, visiting websites, navigating between pages, and performing standard LLM synthesis and reasoning to answer questions.For the initial launch, the target use case of Agents is account qualification, which is the task"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 2,
    "content": "LLM synthesis and reasoning to answer questions.For the initial launch, the target use case of Agents is account qualification, which is the task of deciding whether a company fits your ideal customer profile to sell to or not. Given a company and a set of questions and criteria, the agent performs some research and decides whether they are ‚Äúqualified‚Äù or not."
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 3,
    "content": "0:00\n\n                            /0:35\n\n\n1√ó"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 4,
    "content": "Example Research QuestionsHere are some examples of qualification questions different users might ask the agent to research and qualify based on:An HR software company ‚áíAre there any HR job postings on this company‚Äôs careers page?Do any of the job postings for HR roles mention a competitor‚Äôs software?An AI infra company ‚áíDoes this company mention using LLMs anywhere on their website?Are any of the company‚Äôs open ML roles looking for experience with transformer models for language or audio?Does the website or any job postings mention open source LLMs?Agent v0We used LangGraph as the framework for the agent state machine and LangSmith as the experimentation and tracing framework. Our starting point was a trivial agent that was about as barebones as you can imagine (not even a prompt):This"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 5,
    "content": "experimentation and tracing framework. Our starting point was a trivial agent that was about as barebones as you can imagine (not even a prompt):This actually worked reasonably well for a lot of simple tasks, but it also gets things wrong and produces inconsistent results. It was also difficult to analyze the reasoning behind answers.Agent v1Our next iteration was to build out a more complex agent structure with an initial plan step and a reflect step. The graph looks like this:The first step involves using a large model to generate a plan. In our testing, mainline models like gpt-4o did not construct particularly comprehensive plans without very specific prompting. The best results we obtained at this stage were with OpenAI‚Äôs o1-preview model. The plans generated by o1 stood out as being"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 6,
    "content": "very specific prompting. The best results we obtained at this stage were with OpenAI‚Äôs o1-preview model. The plans generated by o1 stood out as being difficult to replicate with other models because they had:Very detailed step-by-step instructionsPotential pitfalls and mistakes to avoid that are correct and usefulExpansions of what the user‚Äôs questions are asking, even when phrased poorlyThe main downside of o1 is its speed. It can take up to 30-45 seconds to respond, which significantly slows down the overall agent run. An active area of experimentation for us is replicating equivalent results with faster, lighter models.After planning, the agent then begins looping between a ‚Äúreflect‚Äù step and tool calling. For this, we experimented with several models. The ‚Äúmainline‚Äù models like GPT-4o"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 7,
    "content": "agent then begins looping between a ‚Äúreflect‚Äù step and tool calling. For this, we experimented with several models. The ‚Äúmainline‚Äù models like GPT-4o and 3.5 Sonnet work fairly well. One of the most important characteristics of the reflection model is honesty about what it does not yet know in order to appropriately choose the right next steps.Agent v2We‚Äôre still using the plan-reflect-tools state machine structure for our latest iteration. The areas we‚Äôve been most actively tuning and experimenting with are speed and user experience.SpeedThe main downside of this architecture (especially when using heavier planning models like o1-preview) is that it increases the overall runtime substantially. We‚Äôve found ways to deal with this by both speeding up the agent loop and by revising the UI/UX"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 8,
    "content": "that it increases the overall runtime substantially. We‚Äôve found ways to deal with this by both speeding up the agent loop and by revising the UI/UX around using agents in our product.One of the biggest speed boosts we achieved came from parallelizing tool calls. For example, we allowed the model to scrape multiple web pages at a time. This helps a lot since each webpage can take several seconds to load. It works well intuitively because humans often do the same thing‚Äîquickly opening multiple Google results at once in new tabs.User ExperienceThere‚Äôs ultimately a limit on how fast agents can be made without sacrificing accuracy and capabilities. We instead decided to rework the UI for building and testing agents in our product. The initial designs showed a spinner to the user while the"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 9,
    "content": "We instead decided to rework the UI for building and testing agents in our product. The initial designs showed a spinner to the user while the agent ran (which gets pretty painful after a few seconds). Our updated interface instead shows the actions and decision-making process of the agent as it runs in real time. (See video at the top of this article)Pulling this off also required some engineering changes. We originally had a simple prediction endpoint that would hold a request open while the agent ran. To deal with longer agent runtimes and accomplish the new step-by-step UI, we converted this to an ‚Äúasync‚Äù endpoint that starts the agent execution and returns an ID that can be used to poll for progress. We added hooks into the agent graph and tools to ‚Äúlog‚Äù progress to our database. The"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 10,
    "content": "and returns an ID that can be used to poll for progress. We added hooks into the agent graph and tools to ‚Äúlog‚Äù progress to our database. The frontend then polls for updates and displays newly completed steps as they are executed until the final result is obtained.Final LearningsEmbrace ExperimentationWorking with agents definitely required figuring new things out. The research space is super green at the moment and there‚Äôs no definitive SOTA agent architectures yet in the way we think of SOTA in other domains like vision or audio.Given this, we have to lean heavily into good old fashioned ML experimentation and evaluation cycles to make substantive progress. üí°Constructing an extensive and representative set of test cases, labeling them (oftentimes using the agent itself for the first"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 11,
    "content": "substantive progress. üí°Constructing an extensive and representative set of test cases, labeling them (oftentimes using the agent itself for the first pass and then correcting the labels), then running experiments is critical.We‚Äôve been really happy with LangSmith for this (we were also already using LangSmith for Smart Snippets, another LLM-powered feature in Unify). In particular, versioned datasets are exactly what we were hoping they would be. Running and comparing experiments is straightforward, and the tracing is also excellent. We‚Äôre able to run a new agent version on hundreds of examples and quickly compare it against previous versions on a given dataset with very little in house ML infra work.Think of agents as summer internsüí°Asking ‚ÄúHow would it work if this was a human being"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 12,
    "content": "on a given dataset with very little in house ML infra work.Think of agents as summer internsüí°Asking ‚ÄúHow would it work if this was a human being instead of an agent?‚Äù has been a useful exercise for us when developing the UI/UX around agents.For example, many tools with agent building functionality have a UX that revolves around writing a prompt, running it on some test cases, waiting for a black box agent to run, inspecting the results, and then guessing at how to modify the prompt to try and improve it.Now imagine the agent was instead a summer intern prone to oversights and mistakes. If you give the intern a task and they come back with the wrong answer, would you simply try to revise your instructions and then send them off on their own again? No ‚Äî you would ask them to show how they"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 13,
    "content": "wrong answer, would you simply try to revise your instructions and then send them off on their own again? No ‚Äî you would ask them to show how they accomplished the task so that you can identify what they‚Äôre doing wrong. Once you spot their mistake, it‚Äôs much easier to adjust your instructions to prevent the mistake from happening again.Bringing it back to agents, the UX we ended up at is one where users can clearly see what the agent is doing step-by-step to analyze its decision-making and figure out what additional guidance they need to provide.o1-preview is a solid model, but very slowDespite its slowness, OpenAI‚Äôs o1-preview model is a step up from other models we‚Äôve experimented with for plan formation. It has a tendency to be verbose, but that verbosity is often valuable content"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 14,
    "content": "a step up from other models we‚Äôve experimented with for plan formation. It has a tendency to be verbose, but that verbosity is often valuable content rather than just filler or boilerplate. It consistently returns results that we aren‚Äôt able to reproduce with other models (yet), but with a painfully long wait. We were able to work around the slowness with UX improvements, but as we scale this system o1 will likely become a bottleneck.Empower end users to experimentThe biggest challenge we see users face with LLM-powered features is figuring out how to iterate. Many users have little exposure to LLMs or prompting strategies. As a user, if I hit ‚Äúgenerate‚Äù and the results are only partially right, what do I do next? How do I iterate in a way that makes progress without regressions in the"
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 15,
    "content": "hit ‚Äúgenerate‚Äù and the results are only partially right, what do I do next? How do I iterate in a way that makes progress without regressions in the examples that were already correct?We see the intersection of UX and LLMs as ripe for disruption. While we‚Äôre excited about the UI we‚Äôve developed so far, making it easier for users to experiment and correct agents‚Äô mistakes will be one of our biggest focuses going forward."
  },
  {
    "url": "https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/",
    "chunk_id": 16,
    "content": "Join our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 0,
    "content": "LangChain Overview - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain OverviewLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingAdvanced usageMiddlewareGuardrailsStructured outputRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 1,
    "content": "memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy pageLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 2,
    "content": "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 3,
    "content": "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n‚Äã Install\npipuvCopypip install -U langchain"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 4,
    "content": "‚Äã Create an agent\nCopy# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 5,
    "content": "‚Äã Core benefits"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 6,
    "content": "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 7,
    "content": "LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "chunk_id": 8,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoWhat's new in v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 0,
    "content": "LangChain Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring your agents with authentication and authorization\nAgents can take action which makes proper authentication and authorization critical. Read on for how to implement and evolve agent auth.\n\n\n6 min read\n\n\n\n\nFeatured\n\n\n\n\n\n\n\n\n\n\nLangGraph Platform is now Generally Available: Deploy & manage long-running, stateful Agents\n\n\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Klarna's AI assistant redefined customer support at scale for 85 million active users\n\n\nCase Studies\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIs LangGraph Used In Production?\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 1,
    "content": "Case Studies\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIs LangGraph Used In Production?\n\n\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\n\nHarrison Chase\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot Another Workflow Builder\nBy Harrison Chase\n\nOne of the most common requests we‚Äôve gotten from day zero of LangChain has been a visual workflow builder. We never\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to turn Claude Code into a domain specific coding agent\nAuthored by: Aliyan Ishfaq\n\nCoding agents are great at writing code that uses popular libraries on which LLMs have been heavily trained on. But point\n\n\n10 min read"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 2,
    "content": "Coding agents are great at writing code that uses popular libraries on which LLMs have been heavily trained on. But point\n\n\n10 min read\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\nSee how Monte Carlo built its AI Troubleshooting Agent on LangGraph and debugged with LangSmith to help data teams resolve issues faster\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nAgent Middleware\nLangChain has had agent abstractions for nearly three years. There are now probably 100s of agent frameworks with the same core abstraction. They all suffer\n\n\n5 min read"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 3,
    "content": "5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding LangGraph: Designing an Agent Runtime from first principles\nIn this blog piece, you‚Äôll learn why and how we built LangGraph for production agents‚Äîfocusing on control, durability, and the core features needed to scale.\n\n\n15 min read\n\n\n\n\n\n\n\n\n\n\n\n\nStandard message content\nTLDR: We‚Äôve introduced a new view of message content that standardizes reasoning, citations, server-side tool calls, and other modern LLM features across providers. This\n\n\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain & LangGraph 1.0 alpha releases\nToday we are announcing alpha releases of v1.0 for langgraph and langchain, in both Python and JS. LangGraph is a low-level agent orchestration framework,\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 4,
    "content": "3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Open SWE: An Open-Source Asynchronous Coding Agent\nThe use of AI in software engineering has evolved over the past two years. It started as autocomplete, then went to a copilot in an\n\n\n7 min read\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Agents\nUsing an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are ‚Äúshallow‚Äù\n\n\nIn the Loop\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Align Evals: Streamlining LLM Application Evaluation\nAlign Evals is a new feature in LangSmith that helps you calibrate your evaluators to better match human preferences.\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.dev/",
    "chunk_id": 5,
    "content": "3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\nSee how one of the world‚Äôs biggest media companies leveraged LangGraph from its earliest days to build and deploy a multi-agent system to production that empowers creativity.\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nWhy agent infrastructure matters\nLearn why agent infrastructure is essential to handling stateful, long-running tasks ‚Äî and how LangGraph Platform provides the runtime support needed to build and scale reliable agents.\n\n\n4 min read\n\n\n\n\n\n\n\n            Page\n            1\n            of\n            27\n            \n\n\n\nLoad More\nSomething went wrong with loading more posts\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 0,
    "content": "LangSmith docs - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith docsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APILangGraph Server APIControl Plane API for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQLangSmith docsCopy pageCopy"
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 1,
    "content": "DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQLangSmith docsCopy pageCopy pageLangSmith provides tools for developing, debugging, and deploying LLM applications."
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 2,
    "content": "It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\nLangSmith is framework agnostic, so you can use it with or without LangChain‚Äôs open-source libraries\nlangchain and langgraph.\nPrototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n‚ÄãGet started\nCreate an accountSign up at smith.langchain.com (no credit card required).\nYou can log in with Google, GitHub, or email.Create an API keyGo to your Settings page ‚Üí API Keys ‚Üí Create API Key.\nCopy the key and save it securely.\nOnce your account and API key are ready, choose a quickstart to begin building with LangSmith:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 3,
    "content": "ObservabilityGain visibility into every step your application takes to debug faster and improve reliability.Start tracingEvaluationMeasure and track quality over time to ensure your AI applications are consistent and trustworthy.Evaluate your appDeploymentDeploy your agents as LangGraph Servers, ready to scale in production.Deploy your agentsPrompt TestingIterate on prompts with built-in versioning and collaboration to ship improvements faster.Test your promptsStudioUse a visual interface to design, test, and refine applications end-to-end.Develop with StudioHostingHost LangSmith in the cloud, in your environment, or hybrid to match your infrastructure and compliance needs.Choose your hosting mode\n‚ÄãWorkflow"
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 4,
    "content": "‚ÄãWorkflow\nLangSmith combines observability, evaluation, deployment, and hosting in one integrated workflow‚Äîfrom local development to production."
  },
  {
    "url": "https://docs.langchain.com/langsmith/home",
    "chunk_id": 5,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoPricing plansNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#evaluation",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 0,
    "content": "Agent architectures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Skip to content\n        \n\n\n\n\n\n\n\n            \n            \nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            LangGraph\n          \n\n\n\n            \n              Agent architectures\n            \n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n\n\n          \n  \n  \n    \n  \n  Get started\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Guides\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Reference\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Examples\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Additional resources\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    LangGraph\n  \n\n\n\n\n\n\n    GitHub"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 1,
    "content": "Additional resources\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    LangGraph\n  \n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n    Get started\n    \n  \n\n\n\n\n\n\n\n\n            Get started\n          \n\n\n\n\n\n    Quickstarts\n    \n  \n\n\n\n\n\n            Quickstarts\n          \n\n\n\n\n    Start with a prebuilt agent\n    \n  \n\n\n\n\n\n    Build a custom workflow\n    \n  \n\n\n\n\n\n\n    Run a local server\n    \n  \n\n\n\n\n\n\n\n\n\n    General concepts\n    \n  \n\n\n\n\n\n            General concepts\n          \n\n\n\n\n    Workflows & agents\n    \n  \n\n\n\n\n\n\n    Agent architectures\n    \n  \n\n\n\n\n    Agent architectures\n    \n  \n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Router\n    \n\n\n\n\n\n\n      Structured Output\n    \n\n\n\n\n\n\n\n\n      Tool-calling agent\n    \n\n\n\n\n\n\n      Tool calling\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Planning"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 2,
    "content": "Tool-calling agent\n    \n\n\n\n\n\n\n      Tool calling\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Planning\n    \n\n\n\n\n\n\n\n\n      Custom agent architectures\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Parallelization\n    \n\n\n\n\n\n      Subgraphs\n    \n\n\n\n\n\n      Reflection\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Guides\n    \n  \n\n\n\n\n\n\n    Reference\n    \n  \n\n\n\n\n\n\n    Examples\n    \n  \n\n\n\n\n\n\n    Additional resources\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Table of contents\n    \n\n\n\n\n      Router\n    \n\n\n\n\n\n\n      Structured Output\n    \n\n\n\n\n\n\n\n\n      Tool-calling agent\n    \n\n\n\n\n\n\n      Tool calling\n    \n\n\n\n\n\n      Memory\n    \n\n\n\n\n\n      Planning\n    \n\n\n\n\n\n\n\n\n      Custom agent architectures\n    \n\n\n\n\n\n\n      Human-in-the-loop\n    \n\n\n\n\n\n      Parallelization\n    \n\n\n\n\n\n      Subgraphs\n    \n\n\n\n\n\n      Reflection"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 3,
    "content": "Human-in-the-loop\n    \n\n\n\n\n\n      Parallelization\n    \n\n\n\n\n\n      Subgraphs\n    \n\n\n\n\n\n      Reflection\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgent architectures¬∂\nMany LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context. \nInstead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 4,
    "content": "An LLM can route between two potential paths\nAn LLM can decide which of many tools to call\nAn LLM can decide whether the generated answer is sufficient or more work is needed\n\nAs a result, there are many different types of agent architectures, which give an LLM varying levels of control."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 5,
    "content": "Router¬∂\nA router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.\nStructured Output¬∂\nStructured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 6,
    "content": "Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.\nOutput parsers: Using post-processing to extract structured data from LLM responses.\nTool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.\n\nStructured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about structured outputs in this how-to guide.\nTool-calling agent¬∂\nWhile a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 7,
    "content": "Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.\nTool access: The LLM can choose from and use a variety of tools to accomplish tasks.\n\nReAct is a popular general purpose agent architecture that combines these expansions, integrating three core concepts. \n\nTool calling: Allowing the LLM to select and use various tools as needed.\nMemory: Enabling the agent to retain and use information from previous steps.\nPlanning: Empowering the LLM to create and follow multi-step plans to achieve goals."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 8,
    "content": "This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. Unlike the original paper, today's agents rely on LLMs' tool calling capabilities and operate on a list of messages.\nIn LangGraph, you can use the prebuilt agent to get started with tool-calling agents.\nTool calling¬∂"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 9,
    "content": "In LangGraph, you can use the prebuilt agent to get started with tool-calling agents.\nTool calling¬∂\nTools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema. \nMany LLM providers support tool calling and tool calling interface in LangChain is simple: you can simply pass any Python function into ChatModel.bind_tools(function)."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 10,
    "content": "Memory¬∂\nMemory is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:\n\nShort-term memory: Allows the agent to access information acquired during earlier steps in a sequence.\nLong-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.\n\nLangGraph provides full control over memory implementation:\n\nState: User-defined schema specifying the exact structure of memory to retain.\nCheckpointer: Mechanism to store state at every step across different interactions within a session.\nStore: Mechanism to store user-specific or application-level data across sessions."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 11,
    "content": "This flexible approach allows you to tailor the memory system to your specific agent architecture needs. Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time. For a practical guide on adding and managing memory, see Memory.\nPlanning¬∂\nIn a tool-calling agent, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.\nCustom agent architectures¬∂"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 12,
    "content": "Custom agent architectures¬∂\nWhile routers and tool-calling agents (like ReAct) are common, customizing agent architectures often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:\nHuman-in-the-loop¬∂\nHuman involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 13,
    "content": "Approving specific actions\nProviding feedback to update the agent's state\nOffering guidance in complex decision-making processes\n\nHuman-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our human-in-the-loop guide.\nParallelization¬∂\nParallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its Send API, enabling:\n\nConcurrent processing of multiple states\nImplementation of map-reduce-like operations\nEfficient handling of independent subtasks\n\nFor practical implementation, see our map-reduce tutorial\nSubgraphs¬∂\nSubgraphs are essential for managing complex agent architectures, particularly in multi-agent systems. They allow:"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 14,
    "content": "Isolated state management for individual agents\nHierarchical organization of agent teams\nControlled communication between agents and the main system\n\nSubgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our subgraph how-to guide.\nReflection¬∂\nReflection mechanisms can significantly improve agent reliability by:\n\nEvaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 15,
    "content": "Evaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n\n\n\n                Previous\n              \n\n                Workflows & agents\n              \n\n\n\n\n\n                Next\n              \n\n                Guides"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop",
    "chunk_id": 16,
    "content": "Workflows & agents\n              \n\n\n\n\n\n                Next\n              \n\n                Guides\n              \n\n\n\n\n\n\n\n\n\n\n      Copyright ¬© 2025 LangChain, Inc | Consent Preferences\n\n  \n  \n    Made with\n    \n      Material for MkDocs"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 0,
    "content": "Evaluation Concepts - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation ConceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 1,
    "content": "performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageDatasetsExamplesDataset curationManually curated examplesHistorical tracesSynthetic dataSplitsVersionsEvaluatorsEvaluator inputsEvaluator outputsDefining evaluatorsEvaluation techniquesHumanHeuristicLLM-as-judgePairwiseExperimentExperiment configurationRepetitionsConcurrencyCachingAnnotation queuesOffline evaluationBenchmarkingUnit testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 2,
    "content": "testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy pageCopy pageLangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 3,
    "content": "Datasets: Collections of test inputs and reference outputs.\nEvaluators: Functions for scoring outputs. These can be online evaluators that run on traces in real time or offline evaluators that run on a dataset.\n\n‚ÄãDatasets\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.\n\n‚ÄãExamples\nEach example consists of:\n\nInputs: a dictionary of input variables to pass to your application.\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 4,
    "content": "‚ÄãDataset curation\nThere are various ways to build datasets for evaluation, including:\n‚ÄãManually curated examples\nThis is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what ‚Äúgood‚Äù responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.\n‚ÄãHistorical traces\nOnce you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they‚Äôre, well, the most realistic!"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 5,
    "content": "If you‚Äôre getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 6,
    "content": "User feedback: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.\nHeuristics: You can also use other heuristics to identify ‚Äúinteresting‚Äù datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.\nLLM feedback: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 7,
    "content": "‚ÄãSynthetic data\nOnce you have a few examples, you can try to artificially generate some more. It‚Äôs generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.\n‚ÄãSplits"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 8,
    "content": "‚ÄãSplits\nWhen setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.\nLearn how to create and manage dataset splits.\n‚ÄãVersions"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 9,
    "content": "Learn how to create and manage dataset splits.\n‚ÄãVersions\nDatasets are versioned such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset‚Äôs history.\nYou can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn‚Äôt accidentally break your CI pipelines.\n‚ÄãEvaluators\nEvaluators are functions that score how well your application performs on a particular example.\n‚ÄãEvaluator inputs\nEvaluators receive these inputs:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 10,
    "content": "Example: The example(s) from your Dataset. Contains inputs, (reference) outputs, and metadata.\nRun: The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.\n\n‚ÄãEvaluator outputs\nAn evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:\n\nkey: The name of the metric.\nscore | value: The value of the metric. Use score if it‚Äôs a numerical metric and value if it‚Äôs categorical.\ncomment (optional): The reasoning or additional string information justifying the score.\n\n‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 11,
    "content": "‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:\n\nCustom code: Define custom evaluators as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.\nBuilt-in evaluators: LangSmith has a number of built-in evaluators that you can configure and run via the UI."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 12,
    "content": "You can run evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring Rules to automatically run them on particular tracing projects or datasets.\n‚ÄãEvaluation techniques\nThere are a few high-level approaches to LLM evaluation:\n‚ÄãHuman\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\nLangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 13,
    "content": "LangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic\nHeuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot‚Äôs response isn‚Äôt empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.\n‚ÄãLLM-as-judge\nLLM-as-judge evaluators use LLMs to score the application‚Äôs output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference)."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 14,
    "content": "With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.\nLearn about how to define an LLM-as-a-judge evaluator.\n‚ÄãPairwise\nPairwise evaluators allow you to compare the outputs of two versions of an application. Think LMSYS Chatbot Arena - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic (‚Äúwhich response is longer‚Äù), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\nWhen should you use pairwise evaluation?"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 15,
    "content": "When should you use pairwise evaluation?\nPairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.\nLearn how run pairwise evaluations.\n‚ÄãExperiment\nEach time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 16,
    "content": "Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple experiments in a comparison view."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 17,
    "content": "‚ÄãExperiment configuration\nLangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.\n‚ÄãRepetitions\nRunning an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.\nRepetitions can be configured by passing the num_repetitions argument to evaluate / aevaluate (Python, TypeScript). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.\nTo learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 18,
    "content": "To learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency\nBy passing the max_concurrency argument to evaluate / aevaluate, you can specify the concurrency of your experiment. The max_concurrency argument has slightly different semantics depending on whether you are using evaluate or aevaluate.\nevaluate\nThe max_concurrency argument to evaluate specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.\naevaluate"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 19,
    "content": "aevaluate\nThe max_concurrency argument to aevaluate is fairly similar to evaluate, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. aevaluate works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The max_concurrency argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.\n‚ÄãCaching\nLastly, you can also cache the API calls made in your experiment by setting the LANGSMITH_TEST_CACHE to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 20,
    "content": "‚ÄãAnnotation queues\nHuman feedback is often the most valuable feedback you can gather on your application. With annotation queues you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a dataset for future evaluations. While you can always annotate runs inline, annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.\nLearn more about annotation queues and human feedback.\n‚ÄãOffline evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 21,
    "content": "Learn more about annotation queues and human feedback.\n‚ÄãOffline evaluation\nEvaluating an application on a dataset is what we call ‚Äúoffline‚Äù evaluation. It is offline because we‚Äôre evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application‚Äôs outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.\nYou can run offline evaluations client-side using the LangSmith SDK (Python and TypeScript). You can run them server-side via the Prompt Playground or by configuring automations to run certain evaluators on every new experiment against a specific dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 22,
    "content": "‚ÄãBenchmarking"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 23,
    "content": "Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 24,
    "content": "like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 25,
    "content": "‚ÄãUnit tests\nUnit tests are used in software development to verify the correctness of individual system components. Unit tests in the context of LLMs are often rule-based assertions on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.\nUnit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).\n‚ÄãRegression tests"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 26,
    "content": "‚ÄãRegression tests\nRegression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.\nLangSmith‚Äôs comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 27,
    "content": "‚ÄãBacktesting\nBacktesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\nThis is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.\n‚ÄãPairwise evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 28,
    "content": "‚ÄãPairwise evaluation\nFor some tasks it is easier for a human or LLM grader to determine if ‚Äúversion A is better than B‚Äù than to assign an absolute score to either A or B. Pairwise evaluations are just this ‚Äî¬†a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine ‚ÄúWhich of these two summaries is more clear and concise?‚Äù than to give an absolute score like ‚ÄúGive this summary a score of 1-10 in terms of clarity and concision.‚Äù\nLearn how run pairwise evaluations.\n‚ÄãOnline evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 29,
    "content": "Learn how run pairwise evaluations.\n‚ÄãOnline evaluation\nEvaluating a deployed application‚Äôs outputs in (roughly) realtime is what we call ‚Äúonline‚Äù evaluation. In this case there is no dataset involved and no possibility of reference outputs ‚Äî we‚Äôre running evaluators on real inputs and real outputs as they‚Äôre produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 30,
    "content": "Online evaluators are generally intended to be run server-side. LangSmith has built-in LLM-as-judge evaluators that you can configure, or you can define custom code evaluators that are also run within LangSmith."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 31,
    "content": "‚ÄãTesting\n‚ÄãEvaluations vs testing\nTesting and evaluation are very similar and overlapping concepts that often get confused.\nAn evaluation measures performance according to a metric(s). Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they‚Äôre often used to compare two systems against each other rather than to assert something about an individual system.\nTesting asserts correctness. A system can only be deployed if it passes all tests.\nEvaluation metrics can be turned into tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 32,
    "content": "It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.\nYou can also choose to write evaluations using standard software testing tools like pytest or vitest/jest out of convenience.\n‚ÄãUsing pytest and Vitest/Jest\nThe LangSmith SDKs come with integrations for pytest and Vitest/Jest. These make it easy to:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 33,
    "content": "Track test results in LangSmith\nWrite evaluations as tests"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 34,
    "content": "Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.\nWriting evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 35,
    "content": "Using testing tools is also helpful when you want to both evaluate your system‚Äôs outputs and assert some basic things about them."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts?_gl=1*169u5qa*_gcl_au*MTIwMjAyNzcxOS4xNzU1NTQ1MTcw*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NTg2NDU2MTIkbzUzMyRnMSR0MTc1ODY0NzM0NCRqMzckbDAkaDA.#regression-tests",
    "chunk_id": 36,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoEvaluation quickstartPreviousApplication-specific evaluation approachesNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability",
    "chunk_id": 0,
    "content": "LangSmith Observability - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith ObservabilityGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability",
    "chunk_id": 1,
    "content": "a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxLangSmith ObservabilityCopy pageCopy pageThe following sections help you set up and use tracing, monitoring, and observability features:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability",
    "chunk_id": 2,
    "content": "Set up tracingConfigure tracing with basic options, framework integrations, or advanced settings for full control.View tracesAccess and manage traces via UI or API with filtering, exporting, sharing, and comparison tools.Monitor performanceCreate dashboards and set alerts to track performance and get notified when issues arise.Configure automationsUse rules, webhooks, and online evaluations to streamline observability workflows.Collect feedbackGather and manage annotations on outputs using queues and inline annotation.Trace a RAG appFollow a step-by-step tutorial to trace a Retrieval-Augmented Generation application from start to finish.\nFor terminology definitions and core concepts, refer to Observability concepts."
  },
  {
    "url": "https://docs.langchain.com/langsmith/observability",
    "chunk_id": 3,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoTracing quickstartNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 0,
    "content": "Definitive Guide to Testing LLM Applications - LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upThe Definitive Guide to Testing LLM ApplicationsEngineering teams building and testing LLM applications face unique challenges. The non-deterministic nature of LLMs makes it difficult to review natural language responses for style and accuracy, requiring robust testing with new success metrics.‚ÄçThis guide will help you add rigor to your testing process, so you can iterate faster without risking embarrassing or harmful regressions.In this guide, you‚Äôll learn:\n\nTips for testing across the product lifecycle\n\nMethods for building a dataset & defining testing metrics"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 2,
    "content": "Tips for testing across the product lifecycle\n\nMethods for building a dataset & defining testing metrics\n\nTemplates for evaluating RAG and agents, with visual examples Download your copyFirst Name*Last Name*Work Email** ¬†personal emails will not be accepted.Company Name*Job Title*"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 3,
    "content": "Thanks for your interest!PDF file has been sent to your email inbox.You can also open your copy of \"The Definitive Guide to Testing LLM Applications\" in your browser by clicking the button below.Open PDF in your browserOops! Something went wrong while submitting the form.Hear from our customersWalker WardStaff Software Engineer Architect\"LangSmith has made it easier than ever to curate and maintain high-signal LLM testing suites. With LangSmith, we‚Äôve seen a 43% performance increase over production systems, bolstering executive confidence to invest millions in new opportunities.\"Varadarajan SrinivasanVP of Data Science, AI & ML Engineering\"LangSmith has been instrumental in accelerating our AI adoption and enhancing our ability to identify and resolve issues that impact application"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 4,
    "content": "has been instrumental in accelerating our AI adoption and enhancing our ability to identify and resolve issues that impact application reliability. With LangSmith, we can also create custom feedback loops, improving our AI application accuracy by 40% and reducing deployment time by 50%.\"Padarn WilsonHead of Engineering, ML Platform\"Before LangSmith, we didn't have a systematic way to improve the quality of our LLM applications. By integrating LangSmith into our application framework, we now have a cohesive approach to benchmark prompts and models for 200+ applications. This supports our data-driven culture at Grab and allows us to drive continuous refinement of our LLM-powered solutions.\"What can you expect?Testing Guide EbookTake a peek at what's in our testing guideGet the eBook"
  },
  {
    "url": "https://www.langchain.com/testing-guide-ebook",
    "chunk_id": 5,
    "content": "ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 0,
    "content": "Plans and Pricing - LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upPlans for teams of¬†any¬†sizeGet all LangSmith services -- pay for what you useDeveloperFor solo developers getting started.$0 / seat per monththen pay as you goFirst 5k traces included, starting at $0.50 per 1k base ¬†traces thereafter."
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 1,
    "content": "Start for freeGet started with:\n\nUp to 5k base traces / mo, then pay-as--you-go\n\nTracing to debug agent execution\n\nOnline and offline evals\n\nPrompt Hub, Playground, and Canvas for auto improving prompts\n\nAnnotation queues for human feedback\n\nMonitoring and alerting\n\nCommunity support\n\n1 seatPlusFor teams building and deploying agents.$39 / seat per monththen pay as you goFirst 10k traces included, starting at $0.50 per 1k base ¬†traces thereafter.\n\n\n\nSign upEverything in the Developer plan, and:\n\nUp to 10k base traces / mo, then pay-as-you-go\n\n1 dev-sized ¬†agent deployment included\n\nEmail support\n\nUp to 10 seats\n\nUp to 3 workspacesEnterpriseFor teams with advanced hosting, security, and support needs.¬†CustomContact salesEverything in the Plus plan, and:"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 2,
    "content": "Up to 3 workspacesEnterpriseFor teams with advanced hosting, security, and support needs.¬†CustomContact salesEverything in the Plus plan, and:\n\nAlternative hosting options, including hybrid and self-hosted so data doesn‚Äôt leave your VPC\n\nCustom SSO¬†and RBAC\n\nAcccess to deployed engineering team\n\nSupport SLA\n\nTeam trainings & architectural guidance\n\nCustom seats and workspacesLangSmith for Startups and Education.  Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.Reach out to learn about startup pricing for a period of time.Startups\n\n\nEducation\n\n\nEnquire about startup pricing\n\n\nLangSmith for Startups and EducationSeed stage startups and educational institutions, reach out for starter pricing and get shipping today.Startups\n\n\nEducation"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 3,
    "content": "Education\n\n\nCompare plansFeaturesDeveloperPlusEnterprisePricingUsersMaximum of 1 seat(free)Up to 10 seats$39¬†per¬†seat/monthCustomTrace volume (Observability¬†&¬†Evaluation)Prices for traces vary depending on the data retention period you've set. \n\n\n\n 5k base traces / mo included Pay as you go thereafter ‚ìò 10k base traces / mo included Pay as you go thereafter ‚ìòCustomMax trace ingestion and trace size Prices for traces vary depending on the data retention period you've set."
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 4,
    "content": "See hourly trace ingestion and trace event limits here.See hourly trace ingestion and trace event limits here.CustomNode execution cost (Deployment)N/A1 free Dev deployment with unlimited node executions included.For additional deployments:¬†$0.001/node executionCustomUptime cost (Deployment)N/A$0.0007 / min per Development deployment$0.0036 / min per Production deploymentCustomLangSmith Observability &¬†Evaluation:Debug, monitor, and improve your AI¬†applications with tracing and evalsTracingMonitoringInsights (beta)\n\nOnline and offline evalsDataset collectionAnnotation queue (human feedback)Prompt Hub and PlaygroundBulk data export"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 5,
    "content": "Online and offline evalsDataset collectionAnnotation queue (human feedback)Prompt Hub and PlaygroundBulk data export\n\nLangSmith Deployment:Deploy and manage long-running agents with our purpose-built infrastructureLangSmith StudioExpose agent as MCP serverReal-time streaming of intermediary steps and final outputAgent Authorization (beta)1-Click Deploy\n\nHorizontally-scalable service for production-sized deployments\n\nAssistants API\n\n30+¬†API¬†endpoints including state and memory\n\nCron scheduling\n\nAuthentication &¬†authorization for LangGraph APIs"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 6,
    "content": "Assistants API\n\n30+¬†API¬†endpoints including state and memory\n\nCron scheduling\n\nAuthentication &¬†authorization for LangGraph APIs\n\nHosting options:Choose where to host the LangSmith platformPlatform hosting option(s)CloudCloudCloud, Hybrid, or Self-HostedInfraN/AFully managed by LangChainCloud: Fully managed by LangChainHybrid: SaaS control plane, Self-hosted data plane¬†Self-Hosted: Fully self-managedData locationN/ALangChain's Cloud (US or EU)Cloud:¬†LangChain's Cloud (US¬†or EU)Hybrid:¬†LangChain's Cloud (US or EU)Self-Hosted:¬†Your VPCSecurity ControlsSSO\n\nGoogle, GitHubCustom SSORole-Based Access Control\n\n\n\nOrganization Roles (User and Admin)\n\nSupportCommunity SlackEmail Support\n\nTeam trainings\n\n\n\n Architectural guidance for your applications\n\n\n\nAccess to deployed engineers\n\n\n\nSLA"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 7,
    "content": "SupportCommunity SlackEmail Support\n\nTeam trainings\n\n\n\n Architectural guidance for your applications\n\n\n\nAccess to deployed engineers\n\n\n\nSLA\n\n\n\nProcurementBillingMonthly, self-serveMonthly, self-serveAnnual invoiceCustom Terms\n\n\n\nInfosec Review\n\n\n\nFAQsGeneral QuestionsWhich plan is right for me?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 8,
    "content": "Infosec Review\n\n\n\nFAQsGeneral QuestionsWhich plan is right for me?\n\nOur Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).¬†¬†‚ÄçThe Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangSmith Deployment service, with 1 free dev-sized deployment included.¬†‚ÄçThe Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.Do you offer a plan for startups?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 9,
    "content": "Yes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. You‚Äôll get discounted rates and generous free trace allotments to build with confidence from day one.‚Äç‚ÄçApply here to get started with startup pricing. Customers can stay on the Startup Plan for 1 year before graduating to the Plus Plan.When will I be billed?\n\nFor the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.Will you train on the data that I send to LangSmith?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 10,
    "content": "We will not train on your data, and you own all rights to your data. See our Terms of Service for more information.LangSmith Observability &¬†Evaluation QuestionsWhat is a trace? Can it contain multiple events?\n\nA trace represents a single execution of your application‚Äîwhether it‚Äôs an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here's an example of a single trace.What is the difference between a base trace and an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 11,
    "content": "Base traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can \"upgrade\" base traces to extended traces at $4.50 per 1k traces.Why might I want to upgrade a base trace to an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 12,
    "content": "Base traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They‚Äôre priced for volume and short-term utility.‚ÄçExtended traces, on the other hand, are retained for 400 days and often include valuable feedback‚Äîwhether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.‚ÄçLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 13,
    "content": "Base traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They‚Äôre priced for volume and short-term utility.‚ÄçExtended traces, on the other hand, are retained for 400 days and often include valuable feedback‚Äîwhether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.‚ÄçLangSmith lets you choose the right retention for each trace, helping you balance cost and value. I‚Äôve exhausted my free trace allocation. What can I do?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 14,
    "content": "If you‚Äôve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If you‚Äôve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangSmith Deployment QuestionsDoes LangSmith Deployment include any free deployments?\n\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, you‚Äôll be charged by usage (nodes executed and uptime).For my free Dev agent deployment for LangSmith Deployment, is there a cap on the number of nodes executed?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 15,
    "content": "If you‚Äôre on the Plus plan, you get 1 free dev-sized agentdeployment ‚Äì all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed?\n\nNodes Executed is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.What does uptime mean for LangSmith Deployment usage?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 16,
    "content": "Uptime is the duration your deployment‚Äôs database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev agent deployments are typically short-lived (used during iteration, then deleted) ‚Äì whereas Production agent deployments stay live and are updated via revisions (rather than being deleted).¬†When should I use a dev-sized deployment vs. a production-sized agent deployment?"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 17,
    "content": "We recommend using the production-sized deployment for any customer-facing agent. Dev-sized agent deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.Ready to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems"
  },
  {
    "url": "https://www.langchain.com/pricing",
    "chunk_id": 18,
    "content": "up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 0,
    "content": "How Klarna's AI assistant redefined customer support at scale for 85 million active users\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Klarna's AI assistant redefined customer support at scale for 85 million active users\nKlarna's AI assistant is revolutionizing the personal shopping experience, including customer service and productivity. See how they used LangGraph and LangSmith to achieve 80% faster customer resolution times.\n\nCase Studies\n2 min read\nFeb 12, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 1,
    "content": "Klarna has reshaped global commerce with its consumer-centric, AI-powered payment and shopping solutions. With over 85 million active users and 2.5 million daily transactions on its platform, Klarna is a fintech leader that simplifies shopping while empowering consumers with smarter, more flexible financial solutions.Klarna‚Äôs flagship AI Assistant is revolutionizing the shopping and payments experience. Built on LangGraph and powered by LangSmith, the AI Assistant handles tasks ranging from customer payments, to refunds, to other payment escalations.With 2.5 million conversations to date, the AI Assistant is more than just a chatbot; it‚Äôs a transformative agent that performs the work equivalent of 700 full-time staff, delivering results quickly and improving company efficiency.The"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 2,
    "content": "it‚Äôs a transformative agent that performs the work equivalent of 700 full-time staff, delivering results quickly and improving company efficiency.The Challenge: Scaling Customer Support & Handling EscalationsOvercoming escalation overloadKlarna faced growing challenges in managing multi-departmental escalations. To meet rising consumer expectations, Klarna needed a solution that could combine speed, accuracy, and accessibility while scaling across global markets.\"LangChain has been a great partner in helping us realize our vision for an AI-powered assistant, scaling support and delivering superior customer experiences across the globe.\"‚Äî Sebastian Siemiatkowski, CEO and Co-Founder, KlarnaThe Solution: Powered by LangGraph and LangSmithA partnership driving precision and performanceKlarna"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 3,
    "content": "Siemiatkowski, CEO and Co-Founder, KlarnaThe Solution: Powered by LangGraph and LangSmithA partnership driving precision and performanceKlarna turned to LangGraph and LangSmith to evolve their AI Assistant into a reliable, scalable multi-agent system. Key improvements included:Controllable agent architecture: Klarna‚Äôs AI assistant routed requests and handled different tasks using the LangGraph framework. ¬† This helped decrease latency, improve reliability, and cut operational costs.Context-aware intelligence: By dynamically tailoring prompts to specific scenarios, Klarna ensured that their AI assistant consistently delivered relevant, context-aware responses while reducing token costs and latency.¬†Test-driven development: With LangSmith, Klarna could pinpoint what issues arose by seeing"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 4,
    "content": "responses while reducing token costs and latency.¬†Test-driven development: With LangSmith, Klarna could pinpoint what issues arose by seeing step-by-step how their AI assistant behaved. Leveraging LangSmith, Klarna rigorously tested critical use cases for their AI assistant, then validated and refined agent performance with LLM evaluations and prompt iteration.¬†Prompt optimization: Klarna‚Äôs insights in turn improved LangSmith‚Äôs prompt engineering features ‚Äì notably, Klarna helped inspire and design advanced capabilities like meta-prompting. Meta-prompting allows users to suggest specific improvements to the prompts, by prompting them and seeing how the optimized prompt impacted response quality.The ImpactBuilt with LangGraph and refined with LangSmith, Klarna‚Äôs AI assistant has empowered"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 5,
    "content": "how the optimized prompt impacted response quality.The ImpactBuilt with LangGraph and refined with LangSmith, Klarna‚Äôs AI assistant has empowered their teams to handle customer escalations more effectively. They‚Äôve achieved the following results in the past 9 months:Faster resolutions: Reduced average customer query resolution time by 80%, enabling faster responses to user queries and saving analysts and engineers hours a week of investigation time.Increased AI automation for chat handling: Automated ~70% of repetitive support tasks, freeing up customer service agents to handle complex, high-value interactions¬†Improved accuracy: Improved root cause identification for rejection, leading to a significant reduction in customer escalations."
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 6,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-klarna/",
    "chunk_id": 7,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://smith.langchain.com/",
    "chunk_id": 0,
    "content": "LangSmith"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#cognitive-architecture",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#evaluation",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 0,
    "content": "How we built Automatic Import, Attack Discovery, and Elastic AI Assistant using LangChain | Elastic BlogSkip to main contentTable of ContentsTable of contentsHow we built Automatic Import, Attack Discovery, and Elastic AI Assistant using LangChainIntegrating LangChainExample workflow: ES|QL query generationExample workflow: Automatic ImportAdditional benefits of using LangChainCloseHow we built Automatic Import, Attack Discovery, and Elastic AI Assistant using LangChainByDhrumil Patel,James Spiteri,Linda  Ye (LangChain)August 8, 2024Share on TwitterShare on TwitterShare on LinkedInShare on LinkedInShare on FacebookShare on FacebookShare by EmailShare by EmailPrint this pagePrintElastic Security is delivering innovative AI features for security teams, accelerating migration from legacy SIEM"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 1,
    "content": "by EmailPrint this pagePrintElastic Security is delivering innovative AI features for security teams, accelerating migration from legacy SIEM to AI-driven security analytics. We‚Äôve released three capabilities ‚Äî¬†Automatic Import, Attack Discovery, and Elastic AI Assistant ‚Äî that apply generative AI to expedite labor-intensive SecOps tasks, and we‚Äôre just getting started.Using Elastic's Search AI Platform and generative AI, Elastic partners with LangChain, the de facto generative AI orchestration library, to build and deliver these features. This technical blog shares the engineering underpinnings of this work.Integrating LangChainElastic Security‚Äôs integration with LangChain leverages two key components:LangChain and LangGraph open source provide the necessary tools for building"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 2,
    "content": "Security‚Äôs integration with LangChain leverages two key components:LangChain and LangGraph open source provide the necessary tools for building applications that require context-aware reasoning, such as:Enhancing Elastic AI Assistant‚Äôs ability to understand and react to complex security scenarios and generate queriesAttack Discovery‚Äôs ability to identify and describe attacksAutomatic Import‚Äôs ability to craft an accurate data integration based on sample data¬†LangSmith offers detailed tracing and a complete breakdown of requests to large language models (LLMs), enabling the Elastic Security team to debug issues, track performance, and estimate costs.¬†\"Working with Elastic has been amazing in so many ways. Elastic AI Assistant for Security, powered by LangChain's standard LLM interfaces and"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 3,
    "content": "costs.¬†\"Working with Elastic has been amazing in so many ways. Elastic AI Assistant for Security, powered by LangChain's standard LLM interfaces and instrumented using LangSmith, has successfully deployed to production, reaching over 350 users,‚Äù said Erick Friis, founding engineer at LangChain. ‚ÄúElastic is also using LangGraph to build more controllable agents. It's been inspiring to see how our shared users have embraced similar retrieval workflows on their own Elastic deployments!\"Example workflow: ES|QL query generationES|QL, our new piped query language, added a whole new set of threat hunting and detection capabilities for Elastic Security users. Without AI, adopting it required learning the details of the query syntax and its functions. We wanted to simplify this process by enabling"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 4,
    "content": "users. Without AI, adopting it required learning the details of the query syntax and its functions. We wanted to simplify this process by enabling Elastic AI Assistant to generate ES|QL queries from natural language questions.Facilitating ES|QL generation was one of the first core use cases for the integration of LangChain. Elastic AI Assistant generates ES|QL leveraging retrieval augmented generation (RAG) to provide rich context to the chosen LLM, enabling generation of a query based on the user‚Äôs input. LangGraph, a controllable agent orchestration framework, powers the end-to-end generation workflow.Applying a modified version of the native Elasticsearch LangChain vector store component as part of the ES|QL generation graph allowed the team to use the Search AI Platform to retrieve"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 5,
    "content": "Elasticsearch LangChain vector store component as part of the ES|QL generation graph allowed the team to use the Search AI Platform to retrieve the vectorized content necessary to formulate the query.Below is a screenshot of LangSmith visualizing a trace of the resulting ES|QL generation LangGraph.Example workflow: Automatic ImportAutomatic Import leverages LangGraph to generate the resulting integration package. With the combination of LangGraph and LLMs, users are now able to simply and quickly build stateful workflows. Below is a visual representation of the graph that powers Automatic Import.Additional benefits of using LangChainElastic Security users have the freedom to integrate the solution‚Äôs generative AI features with their LLM of choice. With Elastic‚Äôs open inference API and"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 6,
    "content": "Security users have the freedom to integrate the solution‚Äôs generative AI features with their LLM of choice. With Elastic‚Äôs open inference API and LangChain‚Äôs extensive chat model ecosystem, the Elastic Security team is quickly expanding customer LLM options.Elastic Security is at the forefront of propelling security operations workflows with generative AI, using the Search AI Platform and our partnership with LangChain. In parallel, Elastic Observability harnesses the Search AI Platform to deliver comprehensive LangChain tracing, along with end-to-end application tracing, logging, and metrics analysis. Discover how you can leverage Elastic Observability with OpenTelemetry to trace your LangChain applications effectively.Interested in the impact? Read EMA‚Äôs views on AI-driven security"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 7,
    "content": "Observability with OpenTelemetry to trace your LangChain applications effectively.Interested in the impact? Read EMA‚Äôs views on AI-driven security analytics from Elastic Security.The release and timing of any features or functionality described in this post remain at Elastic's sole discretion. Any features or functionality not currently available may not be delivered on time or at all.In this blog post, we may have used or referred to third party generative AI tools, which are owned and operated by their respective owners. Elastic does not have any control over the third party tools and we have no responsibility or liability for their content, operation or use, nor for any loss or damage that may arise from your use of such tools. Please exercise caution when using AI tools with personal,"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 8,
    "content": "operation or use, nor for any loss or damage that may arise from your use of such tools. Please exercise caution when using AI tools with personal, sensitive or confidential information. Any data you submit may be used for AI training or other purposes. There is no guarantee that information you provide will be kept secure or confidential. You should familiarize yourself with the privacy practices and terms of use of any generative AI tools prior to use.¬†Elastic, Elasticsearch, and associated marks are trademarks, logos or registered trademarks of Elasticsearch N.V. in the United States and other countries. All other company and product names are trademarks, logos or registered trademarks of their respective owners.ShareShare on TwitterShare on TwitterShare on LinkedInShare on"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 9,
    "content": "names are trademarks, logos or registered trademarks of their respective owners.ShareShare on TwitterShare on TwitterShare on LinkedInShare on LinkedInShare on FacebookShare on FacebookShare by EmailShare by EmailPrint this pagePrintSign up for Elastic Cloud free trialSpin up a fully loaded deployment on the cloud provider you choose. As the company behind Elasticsearch, we bring our features and support to your Elastic clusters in the cloud.Start free trialFollow usAbout usAbout ElasticLeadershipBlogNewsroomJoin usCareersCareer portalHow we hirePartnersFind a partnerPartner loginRequest accessBecome a partnerTrust & SecurityLegalTrust centerPrivacyTrade ComplianceEthics & ComplianceInvestor relationsInvestor resourcesGovernanceFinancialsStockExcellence AwardsPrevious winnersElastic{ON}"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 10,
    "content": "ComplianceEthics & ComplianceInvestor relationsInvestor resourcesGovernanceFinancialsStockExcellence AwardsPrevious winnersElastic{ON} TourBecome a sponsorAll eventsAbout usAbout ElasticLeadershipBlogNewsroomJoin usCareersCareer portalHow we hirePartnersFind a partnerPartner loginRequest accessBecome a partnerTrust & SecurityLegalTrust centerPrivacyTrade ComplianceEthics & ComplianceInvestor relationsInvestor resourcesGovernanceFinancialsStockExcellence AwardsPrevious winnersElastic{ON} TourBecome a sponsorAll eventsTrademarksTerms of UsePrivacySitemap¬© . Elasticsearch B.V. All Rights ReservedThis website and all associated content, software, discussion forums, products, and services are intended for professional use only. No consumer use of this website or its content is intended or"
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 11,
    "content": "discussion forums, products, and services are intended for professional use only. No consumer use of this website or its content is intended or directed."
  },
  {
    "url": "https://www.elastic.co/blog/building-automatic-import-attack-discovery-langchain",
    "chunk_id": 12,
    "content": "Elastic, Elasticsearch, and other related marks are trademarks, logos, or registered trademarks of Elasticsearch B.V. in the United States and other countries.\nApache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the Apache Software Foundation in the United States and/or other countries. All other brand names, product names, or trademarks belong to their respective owners."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEngineer reliable agentsShip agents to production with LangChain's comprehensive platform for agent engineering.Request a demoSign Up"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 1,
    "content": "LangChain products power top engineering teams, from AI startups to global enterprisesVisibility &¬†controlSee exactly what's happening at every step of your agent. Steer your agent to accomplish critical tasks the way you intended.Fast iterationRapidly move through build, test, deploy, learn, repeat with workflows across the entire agent engineering lifecycle.Durable performanceShip at scale with agent infrastructure designed for long-running workloads and human oversight.Model neutralSwap models, tools, and databases without rewriting your app. Future-proof your stack as AI advances with no vendor lock-in.Your agent engineering stackOpen Source FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 2,
    "content": "FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph puts you in control with low-level primitives to build custom agent workflows.Agent Engineering PlatformLangSmithObservabilityEvaluationDeploymentObservabilityEvaluationDeploymentSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 3,
    "content": "Improve agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 4,
    "content": "Build agents your way, with templates or custom controlBring your own frameworkLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK¬†to gain visibility into your agent interactions, whether you use LangChain's frameworks or not.Open Source Frameworks¬†Bring your ownAgent Engineering PlatformLangSmithObservabilityEvaluationDeploymentOpen Source FrameworksBuild agents your way, with templates or custom controlLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraph puts you in control with low-level primitives to build custom agent workflows.LangSmith Agent Engineering PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 5,
    "content": "PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 6,
    "content": "LangSmith Agent Engineering PlatformImprove agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nLangSmith Agent Engineering PlatformDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 7,
    "content": "CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access to information and tools in a compliant manner so they can perform their best.Customer SupportImprove the speed and efficiency of support teams that handle customer requests.ResearchSynthesize data, summarize sources, and uncover insights faster for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 8,
    "content": "Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\n\n\nFinancial ServicesKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\n\n\nB2B SaaSElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\n\n\nAI/MLReplit's AI Agent serves 30+ million developers. Their AI engineers rely on LangSmith to debug complex traces."
  },
  {
    "url": "https://www.langchain.com/#w-tabs-0-data-w-pane-2",
    "chunk_id": 9,
    "content": "Learn alongside the 1 million+ practitioners who are pushing the industry forward90MMonthly downloads100k+GitHub stars#1Downloaded agent framework1000IntegrationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 0,
    "content": "Evaluation Concepts - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation ConceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 1,
    "content": "performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageDatasetsExamplesDataset curationManually curated examplesHistorical tracesSynthetic dataSplitsVersionsEvaluatorsEvaluator inputsEvaluator outputsDefining evaluatorsEvaluation techniquesHumanHeuristicLLM-as-judgePairwiseExperimentExperiment configurationRepetitionsConcurrencyCachingAnnotation queuesOffline evaluationBenchmarkingUnit testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 2,
    "content": "testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy pageCopy pageLangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 3,
    "content": "Datasets: Collections of test inputs and reference outputs.\nEvaluators: Functions for scoring outputs. These can be online evaluators that run on traces in real time or offline evaluators that run on a dataset.\n\n‚ÄãDatasets\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.\n\n‚ÄãExamples\nEach example consists of:\n\nInputs: a dictionary of input variables to pass to your application.\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 4,
    "content": "‚ÄãDataset curation\nThere are various ways to build datasets for evaluation, including:\n‚ÄãManually curated examples\nThis is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what ‚Äúgood‚Äù responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.\n‚ÄãHistorical traces\nOnce you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they‚Äôre, well, the most realistic!"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 5,
    "content": "If you‚Äôre getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 6,
    "content": "User feedback: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.\nHeuristics: You can also use other heuristics to identify ‚Äúinteresting‚Äù datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.\nLLM feedback: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 7,
    "content": "‚ÄãSynthetic data\nOnce you have a few examples, you can try to artificially generate some more. It‚Äôs generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.\n‚ÄãSplits"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 8,
    "content": "‚ÄãSplits\nWhen setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.\nLearn how to create and manage dataset splits.\n‚ÄãVersions"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 9,
    "content": "Learn how to create and manage dataset splits.\n‚ÄãVersions\nDatasets are versioned such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset‚Äôs history.\nYou can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn‚Äôt accidentally break your CI pipelines.\n‚ÄãEvaluators\nEvaluators are functions that score how well your application performs on a particular example.\n‚ÄãEvaluator inputs\nEvaluators receive these inputs:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 10,
    "content": "Example: The example(s) from your Dataset. Contains inputs, (reference) outputs, and metadata.\nRun: The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.\n\n‚ÄãEvaluator outputs\nAn evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:\n\nkey: The name of the metric.\nscore | value: The value of the metric. Use score if it‚Äôs a numerical metric and value if it‚Äôs categorical.\ncomment (optional): The reasoning or additional string information justifying the score.\n\n‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 11,
    "content": "‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:\n\nCustom code: Define custom evaluators as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.\nBuilt-in evaluators: LangSmith has a number of built-in evaluators that you can configure and run via the UI."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 12,
    "content": "You can run evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring Rules to automatically run them on particular tracing projects or datasets.\n‚ÄãEvaluation techniques\nThere are a few high-level approaches to LLM evaluation:\n‚ÄãHuman\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\nLangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 13,
    "content": "LangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic\nHeuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot‚Äôs response isn‚Äôt empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.\n‚ÄãLLM-as-judge\nLLM-as-judge evaluators use LLMs to score the application‚Äôs output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference)."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 14,
    "content": "With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.\nLearn about how to define an LLM-as-a-judge evaluator.\n‚ÄãPairwise\nPairwise evaluators allow you to compare the outputs of two versions of an application. Think LMSYS Chatbot Arena - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic (‚Äúwhich response is longer‚Äù), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\nWhen should you use pairwise evaluation?"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 15,
    "content": "When should you use pairwise evaluation?\nPairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.\nLearn how run pairwise evaluations.\n‚ÄãExperiment\nEach time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 16,
    "content": "Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple experiments in a comparison view."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 17,
    "content": "‚ÄãExperiment configuration\nLangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.\n‚ÄãRepetitions\nRunning an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.\nRepetitions can be configured by passing the num_repetitions argument to evaluate / aevaluate (Python, TypeScript). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.\nTo learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 18,
    "content": "To learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency\nBy passing the max_concurrency argument to evaluate / aevaluate, you can specify the concurrency of your experiment. The max_concurrency argument has slightly different semantics depending on whether you are using evaluate or aevaluate.\nevaluate\nThe max_concurrency argument to evaluate specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.\naevaluate"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 19,
    "content": "aevaluate\nThe max_concurrency argument to aevaluate is fairly similar to evaluate, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. aevaluate works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The max_concurrency argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.\n‚ÄãCaching\nLastly, you can also cache the API calls made in your experiment by setting the LANGSMITH_TEST_CACHE to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 20,
    "content": "‚ÄãAnnotation queues\nHuman feedback is often the most valuable feedback you can gather on your application. With annotation queues you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a dataset for future evaluations. While you can always annotate runs inline, annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.\nLearn more about annotation queues and human feedback.\n‚ÄãOffline evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 21,
    "content": "Learn more about annotation queues and human feedback.\n‚ÄãOffline evaluation\nEvaluating an application on a dataset is what we call ‚Äúoffline‚Äù evaluation. It is offline because we‚Äôre evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application‚Äôs outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.\nYou can run offline evaluations client-side using the LangSmith SDK (Python and TypeScript). You can run them server-side via the Prompt Playground or by configuring automations to run certain evaluators on every new experiment against a specific dataset."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 22,
    "content": "‚ÄãBenchmarking"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 23,
    "content": "Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 24,
    "content": "like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 25,
    "content": "‚ÄãUnit tests\nUnit tests are used in software development to verify the correctness of individual system components. Unit tests in the context of LLMs are often rule-based assertions on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.\nUnit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).\n‚ÄãRegression tests"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 26,
    "content": "‚ÄãRegression tests\nRegression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.\nLangSmith‚Äôs comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 27,
    "content": "‚ÄãBacktesting\nBacktesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\nThis is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.\n‚ÄãPairwise evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 28,
    "content": "‚ÄãPairwise evaluation\nFor some tasks it is easier for a human or LLM grader to determine if ‚Äúversion A is better than B‚Äù than to assign an absolute score to either A or B. Pairwise evaluations are just this ‚Äî¬†a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine ‚ÄúWhich of these two summaries is more clear and concise?‚Äù than to give an absolute score like ‚ÄúGive this summary a score of 1-10 in terms of clarity and concision.‚Äù\nLearn how run pairwise evaluations.\n‚ÄãOnline evaluation"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 29,
    "content": "Learn how run pairwise evaluations.\n‚ÄãOnline evaluation\nEvaluating a deployed application‚Äôs outputs in (roughly) realtime is what we call ‚Äúonline‚Äù evaluation. In this case there is no dataset involved and no possibility of reference outputs ‚Äî we‚Äôre running evaluators on real inputs and real outputs as they‚Äôre produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 30,
    "content": "Online evaluators are generally intended to be run server-side. LangSmith has built-in LLM-as-judge evaluators that you can configure, or you can define custom code evaluators that are also run within LangSmith."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 31,
    "content": "‚ÄãTesting\n‚ÄãEvaluations vs testing\nTesting and evaluation are very similar and overlapping concepts that often get confused.\nAn evaluation measures performance according to a metric(s). Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they‚Äôre often used to compare two systems against each other rather than to assert something about an individual system.\nTesting asserts correctness. A system can only be deployed if it passes all tests.\nEvaluation metrics can be turned into tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 32,
    "content": "It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.\nYou can also choose to write evaluations using standard software testing tools like pytest or vitest/jest out of convenience.\n‚ÄãUsing pytest and Vitest/Jest\nThe LangSmith SDKs come with integrations for pytest and Vitest/Jest. These make it easy to:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 33,
    "content": "Track test results in LangSmith\nWrite evaluations as tests"
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 34,
    "content": "Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.\nWriting evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 35,
    "content": "Using testing tools is also helpful when you want to both evaluate your system‚Äôs outputs and assert some basic things about them."
  },
  {
    "url": "https://docs.langchain.com/langsmith/evaluation-concepts",
    "chunk_id": 36,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoEvaluation quickstartPreviousApplication-specific evaluation approachesNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/resources-items/the-definitive-guide-to-testing-llm-applications",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 0,
    "content": "How Lovable uses LangSmith to debug & monitor agents in production\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Lovable uses LangSmith to debug & monitor agents in production\nDiscover how Lovable leveraged LangSmith to gain visibility into its agent‚Äôs interactions, rapidly scaling its AI software engineer agent to $25M ARR in just 4 months.¬†\n\nCase Studies\n2 min read\nMar 25, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 1,
    "content": "Lovable.dev is an innovative AI-powered platform that lets users build and ship a high-quality v1 of their software without writing code. It offers seamless integration with tools like GitHub and Supabase, making it easy to create and deploy applications. Users can simply chat to rapidly build websites and web apps; for instance, they can build and deploy applications with features like authentication and data storage, achieving results 20x faster than conventional coding practices.¬†Using LangSmith for agent observability¬†As Lovable experienced rapid growth and user adoption, the team needed to gain visibility into their agentic interactions. With an influx of users, understanding the intricacies of how various components of their agent interacted became crucial for the Lovable team to"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 2,
    "content": "With an influx of users, understanding the intricacies of how various components of their agent interacted became crucial for the Lovable team to maintain efficiency and deliver a seamless user experience.¬†In order to solve their bottleneck of diagnosing agent issues and iterating on features quickly, Lovable turned to LangSmith to gain comprehensive insights into its agentic chain, which was essential. One of the key enhancements to their workflow was the addition of an admin-only button to \"open prompt in LangSmith,\" which enabled team members to access detailed agent traces. This feature empowered developers to quickly identify bottlenecks and optimize workflows, significantly enhancing operational efficiency.By combining multiple requests in LangSmith with its low-level API, Lovable"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 3,
    "content": "and optimize workflows, significantly enhancing operational efficiency.By combining multiple requests in LangSmith with its low-level API, Lovable could pinpoint any session in production and instantly review the sequence of actions taken during the application's development. Monitoring charts allowed Lovable to quickly see any spikes in metrics, and then double-click into any problematic traces to find the culprit. This not only improved debugging, but facilitated a deeper understanding of how each component interacted within the overall system, allowing the Lovable team to iterate and make continuous improvements to their agent.¬†Impact & what‚Äôs nextThe integration of LangSmith has led to remarkable outcomes for Lovable.Enhanced debugging: LangSmith enabled Lovable to introspect anything"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 4,
    "content": "nextThe integration of LangSmith has led to remarkable outcomes for Lovable.Enhanced debugging: LangSmith enabled Lovable to introspect anything in the agentic chain, reducing spent diagnosing issues and speeding up resolution time.Improved collaboration: With code stored in GitHub, team members can collaborate seamlessly, fostering a culture of teamwork and shared ownership.Looking ahead, Lovable aims to further refine its agent development process and will explore additional LangSmith features that will enhance user experience and operational efficiency.ConclusionLovable's strategic use of LangSmith has been instrumental in its rapid growth, enabling the company to achieve the milestone of $25M ARR in just four months. The integration of LangSmith has not only streamlined workflows but"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 5,
    "content": "enabling the company to achieve the milestone of $25M ARR in just four months. The integration of LangSmith has not only streamlined workflows but also set the stage for future advancements, showcasing the transformative potential of AI in the software development landscape."
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 6,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-lovable/",
    "chunk_id": 7,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 0,
    "content": "How C.H. Robinson is transforming the logistics industry with LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow C.H. Robinson is transforming the logistics industry with LangChain\nGlobal logistics provider saves 600+ hours a day with tech they built using LangGraph, LangGraph Studio, and LangSmith developer tools.\n\n\n\nCase Studies\n2 min read\nMar 10, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 1,
    "content": "C.H. Robinson is one of the world‚Äôs largest global logistics providers, managing 37 million shipments a year by ocean, air, rail and truck. It‚Äôs known for solving logistics challenges from the simple to the most complex. With the advent of GenAI, the company has created proprietary tech that represents an efficiency breakthrough for its industry and for supply chains around the world.¬†Problem they‚Äôre solvingCustomers using C.H. Robinson‚Äôs digital tools have been able to get instant service for years. But thousands of its 83,000 customers still prefer to conduct many routine transactions by email, requiring people to read the emails and do time-consuming manual data entry.To address these challenges, C.H. Robinson set out to automate email transactions across the lifecycle of a shipment:"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 2,
    "content": "manual data entry.To address these challenges, C.H. Robinson set out to automate email transactions across the lifecycle of a shipment: from giving a price quote, creating an order and setting appointments for pickup and delivery to checking on the load while it‚Äôs in transit. They aimed to cut costs,¬† increase speed-to-market, and free up time for employees to focus on higher-value, strategic¬† work for their customers.¬†For example, C.H. Robinson gets 15,000 emails a day containing requests for shipping. These often contain inconsistent formatting, including handwritten notes on PDFs, and may be missing essential data. As a result, it would take as much as four hours for a person to get to a shipping request in an email queue, and employees spent around seven minutes¬† processing each email"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 3,
    "content": "as much as four hours for a person to get to a shipping request in an email queue, and employees spent around seven minutes¬† processing each email into an order. C.H. Robinson‚Äôs new proprietary AI tech reads the email, connects information in different parts of the email, detects and fetches any missing information, and creates an order.¬†LangChain for interoperability, LangGraph for debugging AI agentsAs part of the development process, C.H. Robinson‚Äôs GenAI engineering team started by building their AI agents using langchain (the open-source framework) for maximum interoperability. langchain allowed them to easily switch between models and combine user instructions (often hard to parse) with the actual order context.¬†When their team began to delve into more complex classification for"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 4,
    "content": "combine user instructions (often hard to parse) with the actual order context.¬†When their team began to delve into more complex classification for less-than-truckload vs. full truckload shipments, they turned to LangGraph. LangGraph provided the most flexibility for the C.H. Robinson team to understand state to track and update information for their orders as needed. The visual LangGraph Studio also helped their engineers prototype and debug complex agent interactions, saving them development time.With approximately 5,500 orders a day now automated, C.H. Robinson is saving over 600 hours per day on this task alone.¬†Real-time¬†observability with LangSmithWith lean development teams, it was important for C.H. Robinson to catch any errors in their application before deploying and to"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 5,
    "content": "with LangSmithWith lean development teams, it was important for C.H. Robinson to catch any errors in their application before deploying and to understand when their system went wrong. LangSmith was their first line of defense in the testing process, as SMEs (subject matter experts) could catch issues and send them to developers, keeping their project focused and high-quality.¬†With LangSmith, the team was able to stitch together traces through the order entry process to quantify errors and gain a real-time view of their application running. They also have been rapidly experimenting to bridge the gap between the input and the eventual state via prompt management. Meta-prompting in particular allows the user to learn how to input better instructions to generate more relevant answers.What‚Äôs"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 6,
    "content": "prompt management. Meta-prompting in particular allows the user to learn how to input better instructions to generate more relevant answers.What‚Äôs next for C.H. Robinson?C.H. Robinson‚Äôs generative AI efforts are redefining logistics, setting new benchmarks for efficiency, scalability, and customer satisfaction. By integrating LangGraph and LangSmith into their¬†AI development, C.H. Robinson has empowered their workforce to further cut down inefficiencies. Looking ahead, the company is expanding its agentic AI capabilities to offer enhanced personalization and deeper automation."
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 7,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-chrobinson/",
    "chunk_id": 8,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#ux",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 0,
    "content": "Vodafone transforms data operations with AI using LangChain and LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVodafone transforms data operations with AI using LangChain and LangGraph\nSee how Vodafone, a leading telecom company serving 340M+ customers, used LangChain and LangGraph for its performance metrics monitoring and information retrieval chatbots.\n\nCase Studies\n5 min read\nMar 23, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 1,
    "content": "Vodafone is a leading European and African telecommunications company, serving over 340 million customers. Its services span mobile and fixed networks and services, IoT, and enterprise solutions, with a strong emphasis on innovation. In the AI and data space, Vodafone is solving complex challenges related to real-time performance analysis, infrastructure management, and operational efficiency for its network of data centers in Europe.To streamline data operations and to empower its engineering teams, Vodafone has built several AI assistants using LangChain and LangGraph that facilitate intelligent data access, natural language-driven insights, and complex problem-solving.Vodafone‚Äôs GenAI ApplicationsCurrently, Vodafone has developed two AI-powered internal chatbots, which are deployed on"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 2,
    "content": "and complex problem-solving.Vodafone‚Äôs GenAI ApplicationsCurrently, Vodafone has developed two AI-powered internal chatbots, which are deployed on Google Cloud to support engineers working in multiple operations across its data centers. These AI assistants help Vodafone improve the customer experience:Performance metrics monitoring (Insight Engine): This assistant analyzes performance metrics by converting natural language queries into SQL to retrieve key data from data centers monitoring systems. This supports engineers and operations staff with dynamic, data-driven insights that were previously accessible only through custom dashboards.Information retrieval from MS-Sharepoint (Enigma): This assistant enables efficient access to thousands of technical documents and resources. Engineers"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 3,
    "content": "retrieval from MS-Sharepoint (Enigma): This assistant enables efficient access to thousands of technical documents and resources. Engineers can ask questions to verify specific designs, retrieve inventory details, or identify contacts within the organization‚Äî reducing time spent sifting through documentation.With these agents, Vodafone can diagnose and respond to incidents faster by dynamically creating views of data to support quick and more accurate decision-making. As a result, this has reduced engineers‚Äô reliance on custom dashboards or queries to see performance metrics, providing them with deeper insights into internal resources.Flexibly building RAG pipelines with LangChain¬†Vodafone adopted LangChain for these GenAI initiatives due to its composable and comprehensive framework."
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 4,
    "content": "building RAG pipelines with LangChain¬†Vodafone adopted LangChain for these GenAI initiatives due to its composable and comprehensive framework. LangChain‚Äôs integration of essential components like document loaders, models, and vector database allowed Vodafone to rapidly prototype and deploy AI applications tailored to their use case. Vodafone leveraged LangChain to experiment with multiple LLMs, including OpenAI‚Äôs models, LLaMA 3, and Google‚Äôs Gemini, optimizing performance for each specific use case.Specifically, the document loader helped Vodafone‚Äôs engineers process a variety of documents ‚Äî from HLD, Blueprints and Request for Proposals (RFPs) ‚Äî that would be onboarded into the multi-vector DB. Through RAG pipelines, these are then converted into actionable insights by their"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 5,
    "content": "(RFPs) ‚Äî that would be onboarded into the multi-vector DB. Through RAG pipelines, these are then converted into actionable insights by their information retrieval assistant, producing images, tables, and other information needed by the end user.LangChain reduced development time by providing ready-made tools for document processing and pipeline testing. It also enabled Vodafone to benchmark LLM performance effectively across various pipelines.‚ÄúWe‚Äôve been using LangChain‚Äôs components for over a year now,‚Äù said Antonino Artale, senior¬†manager of Cloud Solutions, Orchestration and Intelligence. ‚ÄúIt‚Äôs been a critical enabler for our transition from open-source experimentation to production-grade AI systems.‚ÄùScaling multi-agent workflows with LangGraph¬†As Vodafone scaled its GenAI"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 6,
    "content": "transition from open-source experimentation to production-grade AI systems.‚ÄùScaling multi-agent workflows with LangGraph¬†As Vodafone scaled its GenAI capabilities, the company‚Äôs teams turned to LangGraph to implement a multi-agent architecture and build more sophisticated AI systems. LangGraph‚Äôs flexibility and focus on controllable agent frameworks allowed the team to go beyond simple agents, developing complex workflows with inter-agent coordination.The Vodafone team leveraged LangGraph for its:Modular agent design: Vodafone used LangGraph to construct modular agents as subgraphs, each has certain tools responsible for a specific task. This architecture made it easy to add new capabilities, such as Data collection modules, processing modules, report generators and advanced reasoning"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 7,
    "content": "This architecture made it easy to add new capabilities, such as Data collection modules, processing modules, report generators and advanced reasoning using RAG pipelines, without redesigning the entire system.API integration: The ability to quickly deploy APIs with LangGraph ensured seamless integration into Vodafone‚Äôs broader ecosystem, allowing the AI agents to dynamically orchestrate their own tools based on event-driven architectural patterns.Reliable agent performance: LangGraph‚Äôs validation tools helped Vodafone test multi-agent workflows and ensure consistent performance through validating the different workflow states, verification of node connections, and measuring node latency.An example of LangGraph‚Äôs value is its role in configuring multi-agent workflows for multi-agent"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 8,
    "content": "of node connections, and measuring node latency.An example of LangGraph‚Äôs value is its role in configuring multi-agent workflows for multi-agent Insight Engine and Enigma. LangGraph is placed after the user prompt where it will understand the intent of the user query and orchestrate the request to the appropriate chain.In the case of Enigma, if the query is related to document summarization, the agent will direct the request to the appropriate chain. In turn, this will fetch the relevant context from the multi-vector DB and present the grounded summary response to the user.In the case of Insight Engine,¬†if the query is related to inventory data, then the agent will direct the request to a NL2SQL chain that will convert the NL query to a SQL query and send the response back to the agent."
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 9,
    "content": "data, then the agent will direct the request to a NL2SQL chain that will convert the NL query to a SQL query and send the response back to the agent. The agent will then forward the request to another query processing chain that will query the inventory DB, receive the result, and then pass the information to a LLM to create graphs and charts based on the query response.In summary, LangGraph powered both multi-agent workflows for the Vodafone team:¬†Enigma | This agent seamlessly integrates with Knowledge Hub. It employs a vector database that enables faster and accurate context retrieval across documents, providing more context to the LLM and allowing it to deliver more informed and accurate answers.Insight Engine | This agent performs seamless query transformation, effortlessly"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 10,
    "content": "LLM and allowing it to deliver more informed and accurate answers.Insight Engine | This agent performs seamless query transformation, effortlessly converting natural language queries into structured formats such as SQL, NoSQL, and other services. It shall also ensure efficient data retrieval by quickly accessing data center performance metrics, inventory, and detected anomalies using advanced machine learning techniques. Moreover, passing the results to another agent responsible for generating custom visualizations.Future plan to power LLM apps with LangSmithLangSmith offers an all-in-one solution for the entire application lifecycle, including debugging, evaluation, and performance tracking. This makes it particularly useful for large-scale, production-ready applications, allowing for"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 11,
    "content": "debugging, evaluation, and performance tracking. This makes it particularly useful for large-scale, production-ready applications, allowing for better insights into the inner workings of LLMs.¬†Additionally, LangSmith supports collaboration between developers and subject matter experts, ensuring that applications are not only functional but also aligned with user needs. By integrating seamlessly with existing workflows, LangSmith empowers teams to iterate quickly and effectively, ultimately leading to more reliable and robust AI solutions.Conclusion¬†With LangChain, LangGraph and LangSmith, Vodafone has successfully delivered advanced AI-driven solutions to its engineering and operations teams. These tools have enabled:Reduction in time-to-insight for critical infrastructure issues.Enhanced"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 12,
    "content": "solutions to its engineering and operations teams. These tools have enabled:Reduction in time-to-insight for critical infrastructure issues.Enhanced scalability with modular, API-integrated agent designs.Future-proofed systems that can easily accommodate new domains and data sources.Looking ahead, Vodafone plans to extend its GenAI pipeline to additional data lakes, build even more sophisticated multi-agent systems, and refine its benchmarking processes for a wider range of AI applications."
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 13,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-vodafone/",
    "chunk_id": 14,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 0,
    "content": "LangChain partners with Elastic to launch the Elastic AI Assistant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain partners with Elastic to launch the Elastic AI Assistant\n\nCase Studies\n3 min read\nJan 30, 2024"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 1,
    "content": "Elastic, a leading search analytics company, serving over 20k customers worldwide, enables organizations to securely harness search-powered AI so anyone can find the answers they need in real-time using all their data, at scale. By integrating AI with search technology, the company¬† facilitates the discovery of actionable insights from large volumes of both structured and unstructured data, addressing the need for real-time, scalable data processing. They have cloud-based solutions for search, security, and observability, which help businesses deliver on the promise of AI, and recently, with the help of LangChain and LangSmith, they added an AI Assistant to their security suite.The Elastic AI Assistant for security is designed to be a premium product that supports the security analyst"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 2,
    "content": "an AI Assistant to their security suite.The Elastic AI Assistant for security is designed to be a premium product that supports the security analyst workflow. Specifically the product helps security teams with tasks such as:Alert summarization to explain why an alert was triggered along with a recommended playbook to remediate the attack. This feature generates a dynamic runbook for the organization to provide orderliness during an event.Workflow suggestions to guide users on how to complete tasks such as adding an alert exception or creating a custom dashboard.¬†Query generation and conversion to support users in migrating from other SIEMs to Elastic more easily. Now, a user can paste a query from another product, or in natural language, and Elastic AI Assistant will convert it into an"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 3,
    "content": "to Elastic more easily. Now, a user can paste a query from another product, or in natural language, and Elastic AI Assistant will convert it into an Elastic query using proper syntax.Agent integration advice to guide on the best way to collect data in Elastic.And much more.This is an enterprise-only feature, and since its initial launch in June 2023, Elastic has seen significant adoption of the Assistant.The AI Assistant has proven to significantly reduce a customer‚Äôs MTTR (Mean time to respond) to alerts generated by Elastic Security, as well as reduce the time it takes to write queries and detection rules, thanks to its ability to craft queries based off of natural language use cases.How LangChain and LangSmith supported the product developmentElastic designed their application to be"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 4,
    "content": "queries based off of natural language use cases.How LangChain and LangSmith supported the product developmentElastic designed their application to be agnostic to the LLM from the start, as they wanted their end users to be able to bring their own model and would need to support OpenAI, Azure OpenAI, and Bedrock (amongst others). Giving users this level of control and flexibility from the beginning was a requirement.Fortunately, much of the tooling to create a RAG application came natively with LangChain, and since LangChain also abstracts the application logic from each of the underlying components, the Elastic team was able to create swappable models and prompts, depending on the user‚Äôs preference of vendor, without much engineering overhead. As a bonus, LangChain already had a great"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 5,
    "content": "models and prompts, depending on the user‚Äôs preference of vendor, without much engineering overhead. As a bonus, LangChain already had a great integration with Elastic‚Äôs vector database, so it was a perfect fit for the job.As the team started to add additional functionality into the AI Assistant, such as the ability to generate queries in Elastic‚Äôs new query language - ES|QL, LangSmith was critical in helping the Elastic team understand what exactly got sent to the model, how long did the full trace take, and how many tokens were consumed in the process. LangSmith helped the team better understand how different models could be good at different tasks and at different price points. This visibility allowed the development team to think through tradeoffs and create as consistent of an"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 6,
    "content": "different tasks and at different price points. This visibility allowed the development team to think through tradeoffs and create as consistent of an experience as possible across all three models supported. As the team iterated on their application, LangSmith helped highlight variances and prevented regressions from making it to production.‚ÄúWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn‚Äôt have achieved¬† the product experience delivered to our customers without LangChain, and we couldn‚Äôt have done it at the same pace without LangSmith.‚Äù says James Spiteri, Director of Security Product Management, at Elastic.LangChain and LangSmith also supported"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 7,
    "content": "at the same pace without LangSmith.‚Äù says James Spiteri, Director of Security Product Management, at Elastic.LangChain and LangSmith also supported Elastic‚Äôs workflow to deliver a secure application to the enterprise. Mindful that their users were security experts and naturally skeptical, the Elastic team built out data masking in the app to obfuscate any sensitive data before it was sent to the LLM, exposed the token tracking directly in the end product, so that users have full visibility on usage, and integrated their role based access control with the experience so that admins could limit usage as they wanted.What‚Äôs next with Elastic AI Assistant?The goal of the AI Assistant is to alleviate as much work as possible for the security analyst and give them more time back in their day."
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 8,
    "content": "Assistant?The goal of the AI Assistant is to alleviate as much work as possible for the security analyst and give them more time back in their day. While the product supports three model providers today, the team wants to expand to more models to service an even wider audience.¬†The next big step in the AI Assistant is to leverage LangChain‚Äôs agent framework so that more¬† work can be achieved in the background and have users approve actions. Moving beyond knowledge assistance will take the application to the next level, and the Elastic team feels confident they can deliver with the help of LangChain and LangSmith.Giving back to the communityIn the spirit of open source, the Elastic team has made available a lot of their code that powers the Elastic AI Assistant. You can see exactly how the"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 9,
    "content": "the spirit of open source, the Elastic team has made available a lot of their code that powers the Elastic AI Assistant. You can see exactly how the team implemented their solution by checking out the repository here. For other exciting educational content on ML development with Elastic, check out Elastic Search Labs. Enjoy!"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 10,
    "content": "Tags\nCase StudiesBy LangChain\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/langchain-partners-with-elastic-to-launch-the-elastic-ai-assistant/",
    "chunk_id": 11,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#prompt-engineering",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 0,
    "content": "Event Terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEvent Terms"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 1,
    "content": "LangChain Event Terms and ConditionsBy RSVP‚Äôing or attending Interrupt, including any of the events or activities organized by LangChain in association with Interrupt (collectively the ‚ÄúEvent‚Äù), you signal that you have read, understand, and agree to the following Event Terms and Conditions.‚Äç1. HARASSMENT IS NOT TOLERATEDYou will abide by the Community Code of Conduct which seeks to provide a harassment-free event experience for everyone.‚ÄçHarassment includes offensive verbal comments related to gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 2,
    "content": "deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention.‚ÄçParticipants asked to stop any harassing behavior are expected to comply immediately.‚ÄçBe careful in the words that you choose. Remember that sexist, racist, and other exclusionary jokes can be offensive to those around you. Offensive jokes are not appropriate and will not be tolerated under any circumstance at this event.If a participant engages in behavior that violates the Community Code of Conduct, the event organizers may take any action they deem appropriate, including warning the offender or expulsion from the event or conference with no refund.‚Äç2. BASICS IN ORDER TO ATTENDIn order to"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 3,
    "content": "deem appropriate, including warning the offender or expulsion from the event or conference with no refund.‚Äç2. BASICS IN ORDER TO ATTENDIn order to participate in the Event, you represent that (a) you have the requisite power and authority to enter into these Event Terms, and (b) you have read and understood these Event Terms as well as the LangChain Terms of Service.‚Äç3. PICTURESLangChain or its partners sometimes take photos and videos at LangChain events. By participating or attending the Event, you agree that you may appear in some of these photos and videos, and you authorize LangChain‚Äôs use of them. On the flip side, if you take any photos or videos of attendees at the Event and provide them to us, you authorize us to use them in the same fashion, and represent that the attendees that"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 4,
    "content": "or videos of attendees at the Event and provide them to us, you authorize us to use them in the same fashion, and represent that the attendees that appear in your photos and videos have also consented to such use.‚Äç4. ASSUMPTION OF RISK / RELEASE OF LIABILITY / INDEMNITYa. Assumption of Risk. Although we try to create a safe environment at the Event, accidents can happen. We ask that you do your part to help limit the possibility that you might get injured at the Event.‚ÄçYou agree to carefully consider the risks inherent in any activities that you choose to take part in and to take reasonable precautions before you choose to attend or participate at a LangChain event. For example, you should ensure that you are in good physical health before engaging in any physical activity at a LangChain"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 5,
    "content": "at a LangChain event. For example, you should ensure that you are in good physical health before engaging in any physical activity at a LangChain event, and you should always drink responsibly given the risks associated with drunk driving and participating in activities while your judgment is impaired. Remember that other event guests may be less responsible than you, and may themselves create additional risk to you despite their best intentions.‚ÄçIn short, you understand that your attendance and participation at any LangChain event is voluntary, and you agree to assume responsibility for any resulting injuries to the fullest extent permitted under applicable law.By the same token, you agree that LangChain is not responsible for any injuries or accidents that you might sustain at any"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 6,
    "content": "under applicable law.By the same token, you agree that LangChain is not responsible for any injuries or accidents that you might sustain at any events that LangChain does not organize or control.‚Äçb. Release of Liability. You (for yourself, your heirs, personal representatives, or assigns, and anyone else who might make a claim on your behalf) hereby release, waive, discharge and covenant not to sue LangChain and its respective parent companies, subsidiaries, affiliates, officers, directors, partners, shareholders, members, agents, employees, vendors, sponsors, and volunteers from any and all claims, demands, causes of action, damages, losses or expenses (including court costs and reasonable attorneys fees) which may arise out of, result from, or relate in any way to your attendance at the"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 7,
    "content": "or expenses (including court costs and reasonable attorneys fees) which may arise out of, result from, or relate in any way to your attendance at the Event or any related event, except, of course, for any gross negligence or willful misconduct on our part.‚Äçc. Indemnity. By the same token, you agree to indemnify and hold LangChain, its parents, subsidiaries, affiliates, officers, directors, employees, agents and representatives harmless, including costs, liabilities and legal fees, from any claim or demand made by any third party due to, related to, or connected with your attendance or conduct at the Event or any related event.‚Äç5. TERMINATIONWhile we hope not to, LangChain may prohibit your attendance at any LangChain event at any time if you fail to abide by these Event Terms and the"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 8,
    "content": "we hope not to, LangChain may prohibit your attendance at any LangChain event at any time if you fail to abide by these Event Terms and the Community Code of Conduct, the LangChain Terms of Service, or for any or no reason, without notice or liability of any kind. Section 4 will continue to apply following any termination.‚Äç6. CHOICE OF LAW AND VENUECalifornia law will govern these Event Terms, as well as any claim, cause of action or dispute that might arise between you and LangChain (a ‚ÄúClaim‚Äù), without regard to conflict of law provisions. FOR ANY CLAIM BROUGHT BY EITHER PARTY, YOU AGREE TO SUBMIT AND CONSENT TO THE PERSONAL AND EXCLUSIVE JURISDICTION IN, AND THE EXCLUSIVE VENUE OF, THE STATE AND FEDERAL COURTS LOCATED WITHIN SAN FRANCISCO COUNTY, CALIFORNIA.‚Äç7. MISCELLANEOUS TERMSYou"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 9,
    "content": "JURISDICTION IN, AND THE EXCLUSIVE VENUE OF, THE STATE AND FEDERAL COURTS LOCATED WITHIN SAN FRANCISCO COUNTY, CALIFORNIA.‚Äç7. MISCELLANEOUS TERMSYou agree that you will not represent yourself as an employee, representative, or agent of LangChain (unless, of course, you are actually employed by LangChain). We reserve the right to cancel the Event or any related event at any time, for any reason, and without liability or prejudice. Any failure on LangChain‚Äôs part to exercise or enforce any right or provision of these terms does not constitute a waiver of such right or provision. If any provision of these terms is found to be unenforceable or invalid, that provision shall be limited or eliminated to the minimum extent necessary so that the Event Terms shall otherwise remain in full force and"
  },
  {
    "url": "https://www.langchain.com/event-terms",
    "chunk_id": 10,
    "content": "that provision shall be limited or eliminated to the minimum extent necessary so that the Event Terms shall otherwise remain in full force and effect and enforceable.ProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 0,
    "content": "LangSmith Studio - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationStudioLangSmith StudioGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 1,
    "content": "assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesOn this pageFeaturesGraph modeChat modeLearn moreVideo guideStudioLangSmith StudioCopy pageCopy pagePrerequisites"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 2,
    "content": "LangSmith\nLangGraph Server\nLangGraph CLI"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 3,
    "content": "Studio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with tracing, evaluation, and prompt engineering.\n‚ÄãFeatures\nKey features of Studio:\n\nVisualize your graph architecture\nRun and interact with your agent\nManage assistants\nManage threads\nIterate on prompts\nRun experiments over a dataset\nManage long term memory\nDebug agent state via time travel"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 4,
    "content": "Studio works for graphs that are deployed on LangSmith or for graphs that are running locally via the LangGraph Server.\nStudio supports two modes:\n‚ÄãGraph mode\nGraph mode exposes the full feature-set and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).\n‚ÄãChat mode\nChat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph‚Äôs whose state includes or extends MessagesState.\n‚ÄãLearn more\n\nSee this guide on how to get started with Studio.\n\n‚ÄãVideo guide"
  },
  {
    "url": "https://docs.langchain.com/langsmith/studio",
    "chunk_id": 5,
    "content": "See this guide on how to get started with Studio.\n\n‚ÄãVideo guide\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoHow to implement generative user interfaces with LangGraphPreviousGet started with StudioNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 0,
    "content": "Deconstructing RAG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeconstructing RAG\n\n7 min read\nNov 30, 2023"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 1,
    "content": "ContextIn a recent overview on the state of large language models (LLMs), Karpathy described LLMs as the kernel process of a new kind of operating system. Just as modern computers have RAM and access to files, LLMs have a context window that can be loaded with information retrieved from numerous data sources.Retrieval is a core component of the new LLM operating systemThis retrieved information is loaded into the context window and used in LLM output generation, a process typically called retrieval augmented generation (RAG). RAG is one of the most important concepts in LLM app development because it is an easy way to pass external information to an LLM with advantages over more complex / complex fine-tuning on problems that require factual recall.Typically, RAG systems involve: a"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 2,
    "content": "to an LLM with advantages over more complex / complex fine-tuning on problems that require factual recall.Typically, RAG systems involve: a question (often from a user) that determines what information to retrieve, a process of retrieving that information from a data source (or sources), and a process of passing the retrieved information directly to the LLM as part of the prompt (see an example prompt in LangChain hub here).ChallengeThe landscape of RAG methods has expanded greatly in recent months, resulting in some degree of overload or confusion among users about where to start and how to think about the various approaches. Over the past few months, we have worked to group RAG concepts into a few categories and have released guides for each. Below we'll provide a round-up of these"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 3,
    "content": "few months, we have worked to group RAG concepts into a few categories and have released guides for each. Below we'll provide a round-up of these concepts and present some future work. Major RAG themesQuery TransformationsA first question to ask when thinking about RAG: how can we make retrieval robust to variability in user input? For example, user questions may be poorly worded for the challenging task of retrieval. Query transformations are a set of approaches focused on modifying the user input in order to improve retrieval. Query expansionConsider the question \"Who won a championship more recently, the Red Sox or the Patriots?\" Answering this can benefit from asking two specific sub-questions: \"When was the last time the Red Sox won a championship?\" \"When was the last time the"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 4,
    "content": "this can benefit from asking two specific sub-questions: \"When was the last time the Red Sox won a championship?\" \"When was the last time the Patriots won a championship?\"Query expansion decomposes the input into sub-questions, each of which is a more narrow retrieval challenge.¬†The multi-query retriever¬†performs sub-question generation, retrieval, and returns the unique union of the retrieved docs. RAG fusion builds on by ranking of the returned docs from each of the sub-questions. Step-back prompting offers a third approach in this vein, generating a step-back question to ground an answer synthesis in higher-level concepts or principles (see paper). For example, a question about physics can be stepped-back into a question (and LLM-generated answer) about the physical principles behind"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 5,
    "content": "paper). For example, a question about physics can be stepped-back into a question (and LLM-generated answer) about the physical principles behind the user query.¬†Query re-writingTo address poorly framed or worded user inputs, Rewrite-Retrieve-Read (see¬†paper) is an approach¬†re-writes user questions in order to improve retrieval. Query compressionIn some RAG applications, such as WebLang (our open source research assistant), a user question follows a broader chat conversation. In order to properly answer the question, the full conversational context may be required. To address this, we use this prompt to compress chat history into a final question for retrieval.Further readingSee our blog post on query transformationsSee our blog post on OpenAI's RAG strategiesRoutingA second question to"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 6,
    "content": "for retrieval.Further readingSee our blog post on query transformationsSee our blog post on OpenAI's RAG strategiesRoutingA second question to ask when thinking about RAG: where does the data live? In many RAG demos, data lives in a single vectorstore but this is often not the case in production settings. When operating across a set of various datastores, incoming queries need to be routed. LLMs can be used to support dynamic query routing effectively (see here), as discussed in our recent review of OpenAI's RAG strategies. Query ConstructionA third question to ask when thinking about RAG: what syntax is needed to query the data? While routed questions are in natural language, data is stored in sources such as relational or graph databases that require specific syntax to retrieve. And"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 7,
    "content": "questions are in natural language, data is stored in sources such as relational or graph databases that require specific syntax to retrieve. And even vectorstores utilize structured metadata for filtering. In all cases, natural language from the query needs to be converted into a query syntax for retrieval.  Text-to-SQLConsiderable effort has focused on translating natural language into SQL requests. Text-to-SQL can be done easily (here) by providing an LLM the natural language question along with relevant table information; open source LLMs have proven effective at this task, enabling data privacy (see our templates here and here). Mixed type (structured and unstructured) data storage in relational databases is increasingly common (see here); an embedded document column can be included"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 8,
    "content": "(structured and unstructured) data storage in relational databases is increasingly common (see here); an embedded document column can be included using the¬†open-source¬†pgvector extension for PostgreSQL. It's also possible to interact with this semi-structured data using natural language, marrying the expressiveness of SQL with semantic search (see our cookbook and template).Text-to-CypherWhile vector stores readily handle unstructured data, they don't understand the relationships between vectors. While SQL databases can model relationships, schema changes can be disruptive and costly.¬†Knowledge graphs can address these challenges by modeling the relationships between data and extending the types of relationships without a major overhaul. They are desirable for data that has many-to-many"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 9,
    "content": "the relationships between data and extending the types of relationships without a major overhaul. They are desirable for data that has many-to-many relationships or hierarchies that are difficult to represent in tabular form. Like relational databases, graph databases benefit from a natural language interface using text-to-Cypher, a structured query language designed to provide a visual way of matching patterns and relationships (see templates here and here).Text-to-metadata filtersVectorstores equipped with¬†metadata filtering¬†enable structured queries to filter embedded unstructured documents.¬†The¬†self-query retriever¬†can translate natural language into these structured queries with metadata filters using a specification for the metadata fields present in the vectorstore (see our"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 10,
    "content": "language into these structured queries with metadata filters using a specification for the metadata fields present in the vectorstore (see our self-query template).Further readingSee our blog post on query constructionIndexingA fourth question to ask when thinking about RAG: how to design my index? For vectorstores, there is considerable opportunity to tune parameters like the chunk size and / or the document embedding strategy to support variable data types.Chunk sizeIn our review of OpenAI's RAG strategies, we highlight the notable boost in performance that they saw simply from experimenting with the chunk size during document embedding. This makes sense, because chunk size controls how much information we load into the context window (or \"RAM\" in our LLM OS analogy). Since this is a"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 11,
    "content": "This makes sense, because chunk size controls how much information we load into the context window (or \"RAM\" in our LLM OS analogy). Since this is a central step in index building, we have an¬†open source¬†Streamlit app¬†where you can test various chunk sizes to gain some intuition; in particular, it's worth examining where the document is split using various split sizes or strategies and whether semantically related content is unnaturally split.Document embedding strategyOne of the simplest and most useful ideas in index design is to decouple what you embed (for retrieval) from what you pass to the LLM (for answer synthesis). For example, consider a large passage of text with lots of redundant detail. We can embed a few different representations of this to improve retrieval, such as a"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 12,
    "content": "consider a large passage of text with lots of redundant detail. We can embed a few different representations of this to improve retrieval, such as a summary or small chunks to narrow the scope of information that is embedded. In either case, we can then retrieve the full text to pass to the LLM. These can be implemented using multi-vector and parent-document retriever, respectively. The multi-vector retriever also works well for semi-structured documents that contain a mix of text and tables (see our cookbook and template). In these cases, it's possible to extract each table, produce a summary of the table that is well suited for retrieval, but return the raw table to the LLM for answer synthesis.We can take this one step further: with the advent of multi-modal LLMs, it's possible to use"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 13,
    "content": "but return the raw table to the LLM for answer synthesis.We can take this one step further: with the advent of multi-modal LLMs, it's possible to use generate and embed image summaries as one means of image retrieval for documents that contain text and images (see diagram below). This may be appropriate for cases where multi-modal embeddings are not expected to reliably retrieve the images, as may be the case with complex figures or table. As an example, in our cookbook we use this approach with figures from a financial analysis blog (@jaminball's Clouded Judgement). However, we also have another cookbook using open source (OpenCLIP) multi-modal embeddings for retrieval of images based on more straightforward visual concepts.Further readingSee our blog post on multi-vector"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 14,
    "content": "multi-modal embeddings for retrieval of images based on more straightforward visual concepts.Further readingSee our blog post on multi-vector retrieverPost-ProcessingA final question to ask when thinking about RAG: how to combine the documents that I have retrieved? This is important, because the context window has limited size and redundant documents (e.g., from different sources) will utilize tokens without providing unique information to the LLM. A number of approaches for document post-processing (e.g., to improve diversity or filter for recency) have emerged, some of which we discuss in our blog post on OpenAI's RAG strategies.Re-rankingThe¬†Cohere ReRank¬†endpoint can be used for document compression (reduce redundancy) in cases where we are retrieving a large number of documents."
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 15,
    "content": "ReRank¬†endpoint can be used for document compression (reduce redundancy) in cases where we are retrieving a large number of documents. Relatedly, RAG-fusion uses reciprocal rank fusion (see¬†blog¬†and¬†implementation) to ReRank documents returned from a retriever (similar to¬†multi-query).ClassificationOpenAI classified each retrieved document based upon its content and then chose a different prompt depending on that classification. This marries tagging¬†of¬†text¬†for classification with¬†logical routing (in this case, for the prompt) based on a tag.Future PlansGoing forward, we will focus on at least two areas that extend these themes.Open sourceMany of these tasks to improve RAG are narrow and well-defined. For example, query expansion (sub-question generation) or structured query construction"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 16,
    "content": "of these tasks to improve RAG are narrow and well-defined. For example, query expansion (sub-question generation) or structured query construction for metadata filtering are narrow, well-defined tasks that also may be done repeatedly. In turn, they may not require large (and most costly) generalist models to achieve acceptable performance. Instead, smaller open source models (potentially with fine-tuning) may be sufficient. We will be releasing a series of templates that showcases how to use open source models into the RAG stack where appropriate. BenchmarksHand-in-hand with our effort to test open source LLMs, we recently launched public datasets that can serve ground truth for evaluation. We will be expanding these to include some more specific RAG challenges and using them to assess"
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 17,
    "content": "datasets that can serve ground truth for evaluation. We will be expanding these to include some more specific RAG challenges and using them to assess the merits of the above approaches as well as the incorporation of open source LLMs."
  },
  {
    "url": "https://blog.langchain.dev/deconstructing-rag/",
    "chunk_id": 18,
    "content": "Join our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 0,
    "content": "LangChain Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring your agents with authentication and authorization\nAgents can take action which makes proper authentication and authorization critical. Read on for how to implement and evolve agent auth.\n\n\n6 min read\n\n\n\n\nFeatured\n\n\n\n\n\n\n\n\n\n\nLangGraph Platform is now Generally Available: Deploy & manage long-running, stateful Agents\n\n\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Klarna's AI assistant redefined customer support at scale for 85 million active users\n\n\nCase Studies\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIs LangGraph Used In Production?\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 1,
    "content": "Case Studies\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIs LangGraph Used In Production?\n\n\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\n\nHarrison Chase\n2 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot Another Workflow Builder\nBy Harrison Chase\n\nOne of the most common requests we‚Äôve gotten from day zero of LangChain has been a visual workflow builder. We never\n\n\nIn the Loop\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow to turn Claude Code into a domain specific coding agent\nAuthored by: Aliyan Ishfaq\n\nCoding agents are great at writing code that uses popular libraries on which LLMs have been heavily trained on. But point\n\n\n10 min read"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 2,
    "content": "Coding agents are great at writing code that uses popular libraries on which LLMs have been heavily trained on. But point\n\n\n10 min read\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\nSee how Monte Carlo built its AI Troubleshooting Agent on LangGraph and debugged with LangSmith to help data teams resolve issues faster\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nAgent Middleware\nLangChain has had agent abstractions for nearly three years. There are now probably 100s of agent frameworks with the same core abstraction. They all suffer\n\n\n5 min read"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 3,
    "content": "5 min read\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding LangGraph: Designing an Agent Runtime from first principles\nIn this blog piece, you‚Äôll learn why and how we built LangGraph for production agents‚Äîfocusing on control, durability, and the core features needed to scale.\n\n\n15 min read\n\n\n\n\n\n\n\n\n\n\n\n\nStandard message content\nTLDR: We‚Äôve introduced a new view of message content that standardizes reasoning, citations, server-side tool calls, and other modern LLM features across providers. This\n\n\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain & LangGraph 1.0 alpha releases\nToday we are announcing alpha releases of v1.0 for langgraph and langchain, in both Python and JS. LangGraph is a low-level agent orchestration framework,\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 4,
    "content": "3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Open SWE: An Open-Source Asynchronous Coding Agent\nThe use of AI in software engineering has evolved over the past two years. It started as autocomplete, then went to a copilot in an\n\n\n7 min read\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Agents\nUsing an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are ‚Äúshallow‚Äù\n\n\nIn the Loop\n3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Align Evals: Streamlining LLM Application Evaluation\nAlign Evals is a new feature in LangSmith that helps you calibrate your evaluators to better match human preferences.\n\n\n3 min read"
  },
  {
    "url": "https://blog.langchain.com/",
    "chunk_id": 5,
    "content": "3 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\nSee how one of the world‚Äôs biggest media companies leveraged LangGraph from its earliest days to build and deploy a multi-agent system to production that empowers creativity.\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nWhy agent infrastructure matters\nLearn why agent infrastructure is essential to handling stateful, long-running tasks ‚Äî and how LangGraph Platform provides the runtime support needed to build and scale reliable agents.\n\n\n4 min read\n\n\n\n\n\n\n\n            Page\n            1\n            of\n            27\n            \n\n\n\nLoad More\nSomething went wrong with loading more posts\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/contact-sales",
    "chunk_id": 0,
    "content": "Talk to our team\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/contact-sales",
    "chunk_id": 1,
    "content": "AboutCareersPricingGet a demoSign upTrusted by the best teams building with LLMs:Talk to our teamLangSmith is the agent engineering platform, built for developers and teams who need to ship reliable agents fast. LangSmith works with any agent setup, whether or not you‚Äôre using LangChain‚Äôs open source frameworks.Get in touch with our team to see how LangSmith can accelerate your agent development lifecycle. We‚Äôll answer your questions and walk you through a tailored demoLooking for support? Email support here.Trusted by the best teams building with LLMs:First NameLast NameCompany NameWork Email* ¬†personal emails will not be accepted.Job TitleWhich LangSmith product(s) are you interested in?Observability &¬†EvaluationDeploymentCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global"
  },
  {
    "url": "https://www.langchain.com/contact-sales",
    "chunk_id": 2,
    "content": "LangSmith product(s) are you interested in?Observability &¬†EvaluationDeploymentCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global HeadquartersNA - West CoastNA - CentralNA - East CoastAPACEMEALATAMThank you for requesting a demo! You will receive an email from our team soon.Oops! Something went wrong while submitting the form."
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/",
    "chunk_id": 0,
    "content": "LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            \n            \nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025. Visit the v1.0 alpha docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            LangGraph\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Initializing search\n          \n\n\n\n\n\n\n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n\n\n\n\n          \n  \n  \n    \n  \n  Get started\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Guides\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Reference\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Examples\n\n        \n\n\n\n          \n  \n  \n    \n  \n  Additional resources\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    LangGraph\n  \n\n\n\n\n\n\n    GitHub\n  \n\n\n\n\n\n\n    Get started\n    \n  \n\n\n\n\n\n\n    Guides\n    \n  \n\n\n\n\n\n\n    Reference\n    \n  \n\n\n\n\n\n\n    Examples"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/",
    "chunk_id": 1,
    "content": "GitHub\n  \n\n\n\n\n\n\n    Get started\n    \n  \n\n\n\n\n\n\n    Guides\n    \n  \n\n\n\n\n\n\n    Reference\n    \n  \n\n\n\n\n\n\n    Examples\n    \n  \n\n\n\n\n\n\n    Additional resources\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n404 - Not found\n\n\n\n\n\n\n\n  Back to top\n\n\n\n\n\n\n\n      Copyright ¬© 2025 LangChain, Inc | Consent Preferences\n\n  \n  \n    Made with\n    \n      Material for MkDocs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 0,
    "content": "Ramp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nIntroduction\n\nProblem\n\nUX\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 1,
    "content": "IntroductionTour de RampThe best tour guides do more than just point you in the right direction ‚Äì they anticipate your needs, explain complex landmarks, and make each step of the journey easy to follow. Ramp‚Äôs AI-powered assistant ‚Äì aptly dubbed as ‚ÄúTour Guide‚Äù ‚Äì is a seasoned sherpa that helps users navigate Ramp‚Äôs platform for financial operations.¬†This agent-based solution guides users through tasks ranging from expense approval to dynamically adjusting credit limits within the Ramp web application. Armed with knowledge about Ramp‚Äôs platform, Tour Guide increases user productivity by showing users how they should accomplish the most important tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 2,
    "content": "tasks.ProblemImproving delight and platform accessibilityRamp‚Äôs product automates a lot for users, from bill payments, to credit card access, to expense management, and more. Like any software with layers of functionality, users need to become experts on how to use and administer the tool. There‚Äôs an onboarding curve, and Ramp wanted to reduce the time it took for someone to self-serve their needs.Ramp wanted to provide faster, more immediate assistance in the Ramp product that didn‚Äôt involve calling customer support for help, while also maximizing user delight. Instead of aiming for full automation, which could be higher risk and uncomfortable for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 3,
    "content": "for their users, Ramp designed an agent that allowed users to see and pause actions as the agent walked through a task.UXNavigating and educating users with human-agent collaborationRamp‚Äôs Tour Guide UX educates users about the platform functionality while also building user trust as they see the AI agent taking actions step-by-step. Tour Guide takes control of the user‚Äôs cursor to perform actions a human would do in Ramp (e.g. clicking a button, navigating a dropdown, or filling out a form).As the AI navigates through the interface, it provides step-by-step explanations of its actions. A small banner pops up next to each relevant element, offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 4,
    "content": "offering context and rationale for each click or input.What‚Äôs unique about the Tour Guide agent is its strong emphasis on human-agent collaboration. Users can see all the agent actions and interrupt or take control of the agent at any point, rather than just running it in the background. Ramp designers also implemented a springing cursor that keeps users engaged and feeling like active participants as the Tour Guide agent performs actions on their behalf.When designing the user experience for Tour Guide, the Ramp team was careful to meet user needs without overstepping. ‚ÄúWe avoid putting users in flows where they don‚Äôt actually need the Tour Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 5,
    "content": "Guide.‚Äù - Rahul Sengottuvelu, Head of Applied AI at RampIn this vein, users don't need to manually activate the Tour Guide‚Äôs capabilities - instead, the Ramp team developed a classifier that intelligently identifies relevant queries and automatically routes them to the Tour Guide feature when appropriate.Cognitive architectureIterative action-takingOne of the Ramp engineering team‚Äôs unique insights was that every user interaction with the Ramp web app could be categorized into a scrolling-, clicking-button-, or text-fill step. So, to automate a task for the user, the Tour Guide agent would need to generate these interaction steps in the right sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 6,
    "content": "sequence.The Ramp team designed the agent to take as input the current state of the web app session and suggest the next best action. Each action the Tour Guide took updated the state of the app, so the agent generates exactly one action ‚Äì scrolling, clicking, or text fill ‚Äì at a time. The resulting altered session would then be fed to generate the next action on the tour. This iterative action-taking approach was more effective than designing the entire tour from start to finish, which typically required many scrolls, clicks, and text fills to fulfill the user‚Äôs request.To generate the next best action, the team initially built a multi-step agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 7,
    "content": "agent that made two separate LLM calls. The first step was for planning ‚Äì i.e., given an array of options that the agent could interact with, make a plan to interact with these objects. The second step was a grounding step that executed the object interactionHowever, using two discrete LLM calls, while great for accuracy, resulted in too slow of a user experience. Ramp switched instead to using a consolidated, one call prompt that combined planning and action generation in one step.Prompt engineeringOptimizing model inputs for high-accuracy outputsWhen designing model inputs, the Ramp team worked with their own component library and had a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 8,
    "content": "a combination of image and text data. They developed an annotation script that would tag interactive HTML elements with visible labels, similar to functionality provided by the Vimium browser extension. They also incorporated accessibility tags from the DOM, which provided clear, language-based descriptions of interface components to pass into the model.To make sure the model could generate actionable steps instead of just descriptions of the UI, the team focused on refining inputs through data pre-processing. They simplified the DOM to prune out irrelevant objects, which created cleaner, more efficient inputs that could better guide the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 9,
    "content": "the model‚Äôs actions.According to Alex Shevchenko, the engineer behind Ramp Tour Guide: ‚ÄúThe most effective way to improve an agent‚Äôs accuracy is by constraining the decision space. LLMs still struggle to pick the best option among many similar ones.‚ÄùIn addition to streamlining their inputs, the Ramp team also experimented with prompt optimization to improve output accuracy. Instead of letting the model pick from a lengthy list of interactable elements, they found that labeling a fixed set in the prompt with letters (A to Z) made it clear to the model what options were available to process. This led to a significant improvement in output accuracy.In this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 10,
    "content": "this process, Ramp‚Äôs biggest hurdle was keeping the prompt as concise as possible, since longer prompts resulted in increased latency. While they tried context stuffing to piece together extra context with the user screenshot, they found it was more effective to focus on well-enriched interactions without overloading the prompt.EvaluationGuardrails to keep the agent rolling smoothlyRamp relied heavily on manual testing to get a sense of which actions performed well and which didn't. Once they identified the agent‚Äôs patterns of failure or success, they added guardrails. The team hardcoded restrictions to prevent the agent from interacting with tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 11,
    "content": "tricky pages ‚Äì including those containing complex workflows like large canvas interfaces or tables with numerous elements.This approach allowed Ramp to boost reliability by limiting risk in high-failure areas and focusing the agent on tasks it could handle smoothly.ConclusionAdding rigor paid offWhat truly sets Ramp apart is its exceptional user experience design. With seamless integration, a visually engaging interface, and step-by-step guidance, Ramp doesn‚Äôt just solve problems ‚Äì but also empowers users to master the platform over time.Looking ahead, Ramp plans to expand this into a broader \"Ramp Copilot\" - a single entry point for all user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 12,
    "content": "user queries and actions within the platform. This underscores their commitment to simplifying complex financial workflows with AI, while keeping the user at the forefront of their journey.¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/ramp#problem",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storySuperhuman\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents",
    "chunk_id": 0,
    "content": "Breakout Agentic Apps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/breakoutagents",
    "chunk_id": 1,
    "content": "LangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:‚Äç‚Ä¢¬†UX:¬†How users interact with their agent‚Ä¢ Cognitive architecture: How their agent thinks‚Ä¢¬†Prompt engineering: Best practices for prompting‚Ä¢ Evaluations: How to gain confidence in agent performance\n\n\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro"
  },
  {
    "url": "https://www.langchain.com/breakoutagents",
    "chunk_id": 2,
    "content": "An AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nTransforming how users build software from scratch with Replit Agent ¬†\n\n\nAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 0,
    "content": "LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upEngineer reliable agentsShip agents to production with LangChain's comprehensive platform for agent engineering.Request a demoSign Up"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 1,
    "content": "LangChain products power top engineering teams, from AI startups to global enterprisesVisibility &¬†controlSee exactly what's happening at every step of your agent. Steer your agent to accomplish critical tasks the way you intended.Fast iterationRapidly move through build, test, deploy, learn, repeat with workflows across the entire agent engineering lifecycle.Durable performanceShip at scale with agent infrastructure designed for long-running workloads and human oversight.Model neutralSwap models, tools, and databases without rewriting your app. Future-proof your stack as AI advances with no vendor lock-in.Your agent engineering stackOpen Source FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 2,
    "content": "FrameworksLangChainLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraphLangGraph puts you in control with low-level primitives to build custom agent workflows.Agent Engineering PlatformLangSmithObservabilityEvaluationDeploymentObservabilityEvaluationDeploymentSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 3,
    "content": "Improve agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 4,
    "content": "Build agents your way, with templates or custom controlBring your own frameworkLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK¬†to gain visibility into your agent interactions, whether you use LangChain's frameworks or not.Open Source Frameworks¬†Bring your ownAgent Engineering PlatformLangSmithObservabilityEvaluationDeploymentOpen Source FrameworksBuild agents your way, with templates or custom controlLangChain helps you ship quickly with less code using a pre-built agent architecture and model integrations.LangGraph puts you in control with low-level primitives to build custom agent workflows.LangSmith Agent Engineering PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 5,
    "content": "PlatformSee exactly what your agent is doingAgents create dense outputs that make debugging hard. Tracing gives you clear visibility into each step, so you can quickly identify issues and confidently explain what your agent is actually doing.LangSmith Observability"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 6,
    "content": "LangSmith Agent Engineering PlatformImprove agent quality with evalsLLMs are non-deterministic and output natural language, making responses hard to evaluate for accuracy and quality. Build realistic test sets from production data, score performance with evaluators and expert feedback, and iterate to get your agent from 'okay' to 'great'.LangSmith Evaluation\n\n\nLangSmith Agent Engineering PlatformDeploy with infrastructure built for agentsStandard infrastructure can‚Äôt handle long-running agent workloads that need human collaboration. Deploy in one click with APIs that handle memory, auto-scaling, and enterprise-grade security out of¬†the box ‚Äî built for agent workflows that run for hours or days.LangSmith Deployment"
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 7,
    "content": "CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access to information and tools in a compliant manner so they can perform their best.Customer SupportImprove the speed and efficiency of support teams that handle customer requests.ResearchSynthesize data, summarize sources, and uncover insights faster for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way."
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 8,
    "content": "Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\n\n\nFinancial ServicesKlarna's AI assistant reduced customer query resolution time by 80%, powered by LangSmith and LangGraph.\n\n\nB2B SaaSElastic‚Äôs AI security assistant, built with LangSmith and LangGraph, cut alert response times for 20,000+ customers.\n\n\nAI/MLReplit's AI Agent serves 30+ million developers. Their AI engineers rely on LangSmith to debug complex traces."
  },
  {
    "url": "https://www.langchain.com/",
    "chunk_id": 9,
    "content": "Learn alongside the 1 million+ practitioners who are pushing the industry forward90MMonthly downloads100k+GitHub stars#1Downloaded agent framework1000IntegrationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/resources-items/evaluations-method",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 0,
    "content": "Architectural overview - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationArchitectural overviewGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APILangGraph Server APIControl Plane API for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQOn this pageStorage"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 1,
    "content": "for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementAuthentication methodsFAQsRegions FAQPricing FAQOn this pageStorage ServicesClickHousePostgreSQLRedisBlob storageServicesLangSmith frontendLangSmith backendLangSmith queueLangSmith platform backendLangSmith playgroundLangSmith ACE (Arbitrary Code Execution) backendArchitectural overviewCopy pageCopy pageSelf-hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment."
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 2,
    "content": "You can run LangSmith in Kubernetes (recommended) or Docker in a cloud environment that you control. The LangSmith application consists of several components including LangSmith servers and stateful services:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 3,
    "content": "LangSmith frontend\nLangSmith backend\nLangSmith platform backend\nLangSmith Playground\nLangSmith queue\nLangSmith ACE (Arbitrary Code Execution) backend\nClickHouse\nPostgreSQL\nRedis\nBlob storage (Optional, but recommended)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 4,
    "content": "To access the LangSmith UI and send API requests, you will need to expose the LangSmith frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.\n‚ÄãStorage Services\nLangSmith Self-Hosted will bundle all storage services by default. You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services.\n‚ÄãClickHouse\nClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).\nLangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).\n‚ÄãPostgreSQL"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 5,
    "content": "LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).\n‚ÄãPostgreSQL\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads\nLangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).\n‚ÄãRedis\nRedis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.\nLangSmith uses Redis to back queuing and caching operations.\n‚ÄãBlob storage"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 6,
    "content": "LangSmith uses Redis to back queuing and caching operations.\n‚ÄãBlob storage\nLangSmith supports several blob storage providers, including AWS S3, Azure Blob Storage, and Google Cloud Storage.\nLangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments.\n‚ÄãServices\n‚ÄãLangSmith frontend\nThe frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.\n‚ÄãLangSmith backend"
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 7,
    "content": "‚ÄãLangSmith backend\nThe backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.\n‚ÄãLangSmith queue\nThe queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database.\n‚ÄãLangSmith platform backend\nThe platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks."
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 8,
    "content": "The platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.\n‚ÄãLangSmith playground\nThe playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.\n‚ÄãLangSmith ACE (Arbitrary Code Execution) backend\nThe ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith."
  },
  {
    "url": "https://docs.langchain.com/langsmith/architectural-overview",
    "chunk_id": 9,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNo‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#cognitive-architecture",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://status.smith.langchain.com/",
    "chunk_id": 0,
    "content": "LangSmith US StatusReport a problemSubscribe to updatesReport a problemSubscribe to updatesLangSmith USWe‚Äôre fully operationalWe‚Äôre not aware of any issues affecting our systems.System statusJul 2025-Oct 2025LangSmith Application100% uptimeLangSmith ApplicationLangSmith API99.38% uptimeLangSmith APILangSmith Run Ingestion99.99% uptimeLangSmith Run IngestionLangSmith Deployments Control Plane99.87% uptimeLangSmith Deployments Control PlaneLangSmith Deployments Data Plane100% uptimeLangSmith Deployments Data PlaneCalendarOct 2025Loading...MTWTFSS12345678910111213141516171819202122232425262728293031Powered byLooking for the EU status page? Find it here: https://eu.status.smith.langchain.comPrivacy policy¬∑Terms of service"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 0,
    "content": "LangChain Overview - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain OverviewLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingAdvanced usageMiddlewareGuardrailsStructured outputRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 1,
    "content": "memoryUse in productionStudioTestDeployAgent Chat UIObservabilityOn this page Install Create an agent Core benefitsLangChain OverviewCopy pageCopy pageLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 2,
    "content": "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency."
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 3,
    "content": "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n‚Äã Install\npipuvCopypip install -U langchain"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 4,
    "content": "‚Äã Create an agent\nCopy# pip install -qU \"langchain[anthropic]\" to call the model\n\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 5,
    "content": "‚Äã Core benefits"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 6,
    "content": "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 7,
    "content": "LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more"
  },
  {
    "url": "https://python.langchain.com/v0.2/docs/introduction/",
    "chunk_id": 8,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoWhat's new in v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever",
    "chunk_id": 0,
    "content": "Page Not Found | ü¶úÔ∏èüîó LangChain\n\n\n\n\n\n\n\n\nSkip to main contentThese docs will be deprecated and no longer maintained with the release of LangChain v1.0 in October 2025. Visit the v1.0 alpha docsIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchPage Not FoundWe could not find what you were looking for.Please contact the owner of the site that linked you to the original URL and let them know their link is broken.CommunityLangChain ForumTwitterSlackGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc."
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 0,
    "content": "LangGraph Platform is now Generally Available: Deploy & manage long-running, stateful Agents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph Platform is now Generally Available: Deploy & manage long-running, stateful Agents\nLangGraph Platform, our infrastructure for deploying and managing agents at scale, is now generally available. Learn how to deploy\n\n4 min read\nMay 14, 2025"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 1,
    "content": "Note: As of October 2025, LangGraph Platform has been re-named to \"LangSmith Deployment\".Today we‚Äôre excited to announce the general availability of LangGraph Platform ‚Äî our purpose-built infrastructure and management layer for deploying and scaling long-running, stateful agents. Since our beta last June, nearly 400 companies have used LangGraph Platform to deploy their agents into production.¬†Agent deployment is the next hard hurdle for shipping reliable agents, and LangGraph Platform dramatically lowers this barrier with:1-click deployment to go live in minutes,¬†30 API endpoints for designing custom user experiences that fit any interaction patternHorizontal scaling to handle bursty, long-running trafficA persistence layer to support memory, conversational history, and async"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 2,
    "content": "interaction patternHorizontal scaling to handle bursty, long-running trafficA persistence layer to support memory, conversational history, and async collaboration with human-in-the-loop or multi-agent workflowsNative LangGraph Studio, the agent IDE, for easy debugging, visibility, and iterationRead on to learn more about what LangGraph Platform offers and which deployment option is right for you. If you're more of a visual learner, you can check out our video walkthrough here.The challenges of agent infrastructure ‚Äì and how LangGraph Platform can helpOur team has the privilege of working with many of the most exciting companies building agents ‚Äì¬† such as Klarna, Lovable, Replit, Clay, LinkedIn. Through close collaboration, we‚Äôve come to believe that the challenges of running agents, at"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 3,
    "content": "‚Äì¬† such as Klarna, Lovable, Replit, Clay, LinkedIn. Through close collaboration, we‚Äôve come to believe that the challenges of running agents, at scale, in production are often unique relative to traditional apps:Many agents are long running. Like we‚Äôve seen with deep research agents, agents that run in the background on a schedule or in response to environment triggers can take a long time to return a final output. These workflows can be prone to failures mid-task, so they need durable infrastructure to ensure task completion.Many agents rely on async collaboration. Agents need to act on inputs from unpredictable events, whether collaborating with a human to steer / approve an action or waiting on another agent. For example ‚Äì will the human reply immediately, tomorrow, or not at all? Good"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 4,
    "content": "with a human to steer / approve an action or waiting on another agent. For example ‚Äì will the human reply immediately, tomorrow, or not at all? Good infrastructure accounts for this chaos and preserves state throughout.Bursty. While not totally unique to agents, horizontally scaling infra to handle traffic spikes is challenging ‚Äì¬† especially for tasks that run daily or on schedules.¬†We want engineers to obsess over building the best agent architecture ‚Äì not worry about infra. LangGraph Platform‚Äôs server suits these kinds of workloads at scale. Developers can just 1-click deploy their apps directly in the management console to get started.1-click deploy with our native GitHub integration ‚Äî¬†just select a repo, and ship! Accelerate agent development with visual workflowsBuilding great agents"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 5,
    "content": "deploy with our native GitHub integration ‚Äî¬†just select a repo, and ship! Accelerate agent development with visual workflowsBuilding great agents requires fast feedback loops. LangGraph Studio (included as part of LangGraph Platform) helps developers visualize and debug agent workflows in real time, with detailed visibility into agent trajectories and support for branching logic and retries.¬†You can also test edge cases, inspect memory/state at each step, and quickly pinpoint where things go wrong. Instead of retrying things from scratch, built-in checkpointing and memory modules in LangGraph Platform make it easy to rewind, edit, and rerun failure points without frustration.Whether you‚Äôre using our pre-built agent templates for common agent workflows, or building from scratch, LangGraph"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 6,
    "content": "failure points without frustration.Whether you‚Äôre using our pre-built agent templates for common agent workflows, or building from scratch, LangGraph Platform lets you scaffold your agentic apps quickly ‚Äì going from an idea to production in hours.¬†Centralize agent management across your orgAs agents get adopted across teams, managing them becomes a team sport. LangGraph Platform gives organizations a unified view of every agent in development or production ‚Äî helping fellow team members iterate and scale across use cases. The enterprise tier also supports RBAC and workspaces, so that you can control access and sharing.The LangGraph Platform management console makes it easier to enforce consistency, monitor behavior, and ship updates safely ‚Äî all without needing to re-deploy or touch code"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 7,
    "content": "management console makes it easier to enforce consistency, monitor behavior, and ship updates safely ‚Äî all without needing to re-deploy or touch code every time. You can:Discover available agents in the agent registryCreate different versions of your agent (‚Äúassistants‚Äù) in LangGraph platform, allowing you to reuse common agent architectures¬†Leverage other agents as ‚ÄúRemote Graphs‚Äù, allowing you to create multi agent architectures that run in a distributed mannerFor companies like Qualtrics, centralizing agent management with LangGraph Platform has been critical to driving efficiency:¬†\"The future of agentic AI is multi-vendor and AI agents must be built in an ecosystem. By using LangGraph Platform to build and manage our AI agents - Experience Agents - Qualtrics is able to design, deploy,"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 8,
    "content": "be built in an ecosystem. By using LangGraph Platform to build and manage our AI agents - Experience Agents - Qualtrics is able to design, deploy, and manage complex generative AI agent workflows with efficiency, speed, and scale.\" ‚Äì Phil McKennan, VP of Strategy and Partnerships at QualtricsTry LangGraph Platform todayLangGraph Platform reduces time to production and enables better application experiences with a runtime that suits the workload and improves DevEx with well crafted APIs, built-in memory, and a Studio development environment. Today, LangGraph Platform is generally available.¬†To get started, choose the deployment option that fits your team‚Äôs needs:¬†Cloud (SaaS): Fastest way to get started, fully managed and easy deployment from within LangSmith. Available on our Plus plan"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 9,
    "content": "fits your team‚Äôs needs:¬†Cloud (SaaS): Fastest way to get started, fully managed and easy deployment from within LangSmith. Available on our Plus plan and Enterprise plan.¬†To get started, read the docs (Plus plan) or¬†contact sales (Enterprise).Hybrid: SaaS control plane with self-hosted data plane ‚Äî ideal for teams with sensitive data but still want more of a managed service. Available only on the Enterprise plan.¬†To get started, contact salesFully Self-Hosted: Run the entire platform within your own infrastructure. No data leaves your VPC. Available on the Enterprise plan. To get started, contact sales.If you want to try out a basic version of our LangGraph server in your environment, you can also self-host on our Developer plan and get up to 100k nodes executed per month for free ‚Äì great"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 10,
    "content": "our LangGraph server in your environment, you can also self-host on our Developer plan and get up to 100k nodes executed per month for free ‚Äì great to give you exposure to LangGraph Platform and run hobbyist projects on your infra. To get started on the Developer tier, read the docs.Learn more about deployment options here and view pricing details.¬†LangGraph Platform is the easiest way to develop, deploy, and manage long-running, stateful agents. It can be used independently from LangChain‚Äôs other products ‚Äì LangChain (integrations), LangGraph (agent orchestration), and LangSmith (Evals and Observability), or stack together to provide an easy transition from the build phase to production.To learn more, visit the LangGraph Platform webpage. We can‚Äôt wait to see how far you can run with"
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 11,
    "content": "easy transition from the build phase to production.To learn more, visit the LangGraph Platform webpage. We can‚Äôt wait to see how far you can run with your agents."
  },
  {
    "url": "https://blog.langchain.com/langgraph-platform-ga/",
    "chunk_id": 12,
    "content": "Join our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 0,
    "content": "How Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Podium optimized agent behavior and reduced engineering intervention by 90% with LangSmith\nSee how Podium tests across the lifecycle development of their AI employee agent, using LangSmith for dataset curation and finetuning. They improved agent F1 response quality to 98% and reduced the need for engineering intervention by 90%.\n\nCase Studies\n5 min read\nAug 15, 2024"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 1,
    "content": "About PodiumPodium is a communication platform that helps small businesses connect quickly with customers via phone, text, email, and social media. Small businesses often have high-touch interactions with customers ‚Äî think automotive dealers, jewelers, bike shops ‚Äî yet are understaffed. Podium's mission is to help these businesses respond to customer inquiries promptly so that they can convert leads into sales.¬†Podium data shows that responding to customer inquiries within 5 minutes results in a 46% higher lead conversion rate than responding in an hour. To improve lead capture, Podium launched AI Employee, their agentic application (and flagship product) to engage local business customers, schedule appointments, and close sales.¬†Initially, Podium used the LangChain framework for"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 2,
    "content": "(and flagship product) to engage local business customers, schedule appointments, and close sales.¬†Initially, Podium used the LangChain framework for single-turn interactions. As their agentic use cases grew more complex for a wide-ranging set of customers and domains, Podium needed better visibility into their LLM calls and interactions ‚Äî and turned to LangSmith for LLM testing and observability.Testing across the agentic development lifecycleEstablishing feedback loops was especially important to the agentic development lifecycle for Podium. LangSmith allowed the Podium engineers to test and continuously monitor their AI employee‚Äôs performance, adding new edge cases to their dataset to refine and test the model over time.Podium‚Äôs testing approach looks like the following:¬†¬†Baseline"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 3,
    "content": "adding new edge cases to their dataset to refine and test the model over time.Podium‚Äôs testing approach looks like the following:¬†¬†Baseline Dataset Curation: Create an initial dataset to represent basic use cases and requirements for the agent. This serves as a foundation for testing and development.Baseline Offline Evaluation: Conduct initial tests using the curated dataset to assess the agent's performance against the basic requirements before shipping to production.Collecting Feedback:¬†User-Provided Feedback: Collect direct input from users interacting with the agent.¬†Online Evaluation: Use LLMs to self-evaluate and monitor the quality of responses using in real-time, flagging potential issues for further investigation.Optimization:¬†Prompt Tuning: Refine the prompts used to guide the"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 4,
    "content": "responses using in real-time, flagging potential issues for further investigation.Optimization:¬†Prompt Tuning: Refine the prompts used to guide the agent's responses.Retrieval Tuning: Adjust the retrieval mechanisms used to generate responses.Model Fine-Tuning: Use traced data to further train and specialize the model for specific tasks.Ongoing Evaluation:¬†Offline Evaluation: Evaluate the agent's performance and identify opportunities for optimization using backtesting, pairwise comparisons, and other testing methods.Dataset Curation: Continuously update and expand the test dataset with new scenarios and edge cases for regression testing, ensuring new changes don't negatively impact existing capabilities.How Podium creates testing loops for their agent Dataset curation and fine-tuning"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 5,
    "content": "ensuring new changes don't negatively impact existing capabilities.How Podium creates testing loops for their agent Dataset curation and fine-tuning agents with LangSmithPrior to LangSmith, understanding a customer inquiry and what steps employees should take to resolve the inquiry was difficult, since the Podium engineers made 20-30 LLM calls per interaction. With LangSmith, they quickly got set up and logged and viewed traces to aggregate insights.One specific challenge Podium ran into with their AI Employee was that the agent struggled to recognize when a conversation had naturally ended, resulting in awkward repeated goodbyes. To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 6,
    "content": "To address this, Podium began by creating a dataset in LangSmith with various conversation scenarios, including ways different conversations might conclude.¬†Their engineering team then found it helpful to upgrade to a larger model, curating the outputs into a smaller model (using a technique called model distillation). Upgrading their model went smoothly since model inputs and outputs were automatically captured in LangSmith‚Äôs traces, allowing the team to easily curate datasets.Podium engineers also enriched LangSmith traces with metadata on customer profiles, business types, and other parameters important to their business. They grouped traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 7,
    "content": "traces using specific identifiers in LangSmith, making it easy to aggregate related traces during data curation. This enriched data enabled Podium to create a higher-quality and balanced dataset, which improved model fine-tuning and helped them avoid overfitting).¬†With this balanced dataset, the Podium team then compared the results from their fine-tuned model against results from their original, larger model using pairwise evaluations. This comparison allowed them to assess how well the upgraded model could improve the agent‚Äôs ability to know when to conclude a conversation.After fine-tuning, Podium‚Äôs new model showed significant improvement in detecting where natural conversation should end for its agent. Podium‚Äôs F1 scores with the fine-tune model experienced a 7.5% improvement, going"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 8,
    "content": "in detecting where natural conversation should end for its agent. Podium‚Äôs F1 scores with the fine-tune model experienced a 7.5% improvement, going from 91.7% to 98.6% to exceed their quality threshold of 98%.High-quality customer support for AI platform without engineering interventionAt Podium, engineers must understand when communications with customers go awry, so that they can keep shipping reliable and high-quality products.Since publicly launching their AI Employee in January, it became critical for the Technical Product Specialists (TPS) at Podium to troubleshoot issues users were encountering in real-time. At Podium, the TPS team typically provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 9,
    "content": "provides customer support for their small business customers. However, pinpointing the source of issues (and how to take action on them) was challenging.¬†Giving the TPS team access to LangSmith provided clarity, allowing the team to quickly identify customer-reported issues and determine: ‚ÄúIs this issue caused by a bug in the application, incomplete context, misaligned instructions, or an issue with the LLM?‚Äù¬†For Podium, identifying the type of customer issue guided them to the appropriate interventions:For bugs in the application: These are orchestration failures, such as an integration failing to return data. These require engineering intervention.For incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 10,
    "content": "incomplete context: LLM is missing information needed to answer a question. These can be remediated by the TPS team by adding additional content.For misaligned instructions: Instructions are based on business requirements; any issues in the requirements can affect agent behavior. These can be remediated by the TPS team making changes in the content authoring system to better suit business requirements.For an LLM issue: Even with necessary context, an LLM may produce unexpected or incorrect information. These require engineering intervention.For example, many car dealerships use Podium‚Äôs AI Employee to respond to customer inquiries. If the AI Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith‚Äôs playground feature to edit the system"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 11,
    "content": "Employee mistakenly responds that a car dealership does not offer oil changes, the TPS team can use LangSmith‚Äôs playground feature to edit the system output and determine if a simple setting change in the Admin interface can resolve the issue.LangSmith Playground enables Podium‚Äôs support team to troubleshoot agent behavior without engineering interventionBefore LangSmith, troubleshooting agent behavior often required engineering intervention. This was a time-consuming process that involved calling in engineers to first review model inputs and outputs, and then rewrite and refactor the code.By giving their TPS team access to LangSmith traces, Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.In"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 12,
    "content": "Podium has reduced the need for engineering intervention by 90%, allowing their engineers to focus more on development instead of support tasks.In summary, using LangSmith led to:¬†Increased efficiency of Podium‚Äôs support team by enabling them to resolve issues more quickly and independently.Improved customer satisfaction (CSAT) scores for both support interactions and Podium‚Äôs AI-powered services.What‚Äôs Next for PodiumBy integrating LangSmith and LangChain, Podium has gained a competitive edge in the space of customer experience tools. LangSmith has enhanced observability and simplified the management of large datasets and optimizing model performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 13,
    "content": "performance. The Podium team has also been integrating LangGraph into its workflow, reducing complexity in their agent orchestration while serving different target customers, while increasing controllability over their agent conversations.¬†Together, these suite of products have allowed Podium to focus on their core value proposition ‚Äî help small businesses capture leads more effectively ‚Äî¬†and efficiently design, test, and monitor their LLM applications.Podium is hiring across roles to help local businesses win. Inspired by Podium‚Äôs story? You can also try out LangSmith for free or talk to a LangSmith expert to learn more.¬†And for a more comprehensive best practices for testing and evaluating your LLM application, check out this guidebook."
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 14,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-podium/",
    "chunk_id": 15,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 0,
    "content": "Use annotation queues - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationAnnotation & human feedbackUse annotation queuesGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 1,
    "content": "experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageCreate an annotation queueBasic DetailsAnnotation RubricCollaborator SettingsAssign runs to an annotation queueReview runs in an annotation queueVideo guideAnnotation & human feedbackUse annotation queuesCopy pageCopy pageAnnotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs. While you can always annotate traces inline, annotation queues provide another option to"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 2,
    "content": "for human annotators to attach feedback to specific runs. While you can always annotate traces inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 3,
    "content": "‚ÄãCreate an annotation queue"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 4,
    "content": "To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar. Then click + New annotation queue in the top right corner."
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 5,
    "content": "‚ÄãBasic Details\nFill in the form with the name and description of the queue. You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace.\n‚ÄãAnnotation Rubric\nBegin by drafting some high-level instructions for your annotators, which will be shown in the sidebar on every run.\nNext, click ‚Äù+ Desired Feedback‚Äù to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run. Add a description for each, as well as a short description of each category if the feedback is categorical.\n\nReviewers will see this:\n\n‚ÄãCollaborator Settings\nThere are a few settings related to multiple annotators:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 6,
    "content": "Reviewers will see this:\n\n‚ÄãCollaborator Settings\nThere are a few settings related to multiple annotators:\n\n\nNumber of reviewers per run: This determines the number of reviewers that must mark a run as ‚ÄúDone‚Äù for it to be removed from the queue. If you check ‚ÄúAll workspace members review each run,‚Äù then a run will remain in the queue until all workspace members have marked it ‚ÄúDone‚Äù.\n\nReviewers cannot view the feedback left by other reviewers.\nComments on runs are visible to all reviewers.\n\n\n\nEnable reservations on runs: We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.\n\n\n\nHow do reservations work?"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 7,
    "content": "How do reservations work?\n\nWhen a reviewer views a run, the run is reserved for that reviewer for the specified ‚Äúreservation length‚Äù. If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.\n\nWhat happens if time runs out?"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 8,
    "content": "If a reviewer has viewed a run and then leaves the run without marking it ‚ÄúDone‚Äù, the reservation will expire after the specified ‚Äúreservation length‚Äù. The run is then released back into the queue and can be reserved by another reviewer.\nClicking ‚ÄúRequeue at end‚Äù will only move the current run to the end of the current user‚Äôs queue; it won‚Äôt affect the queue order of any other user. It will also release the reservation that the current user has on that run.\nBecause of these settings, it‚Äôs possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else‚Äôs queue size.\nYou can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 9,
    "content": "‚ÄãAssign runs to an annotation queue\nTo assign runs to an annotation queue, either:\n\n\nClick on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span. \n\n\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page. \n\n\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue.\n\n\nSelect one or multiple experiments from the dataset page and click Annotate. From the resulting popup, you may either create a new queue or add the runs to an existing one:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 10,
    "content": "It is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow this guide.\n‚ÄãReview runs in an annotation queue\nTo review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar. Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review."
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 11,
    "content": "You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to ‚ÄúView run‚Äù.\nThe keyboard shortcuts shown can help streamline the review process."
  },
  {
    "url": "https://docs.langchain.com/langsmith/annotation-queues",
    "chunk_id": 12,
    "content": "‚ÄãVideo guide\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoHow to upload experiments run outside of LangSmith with the REST APIPreviousSet up feedback criteriaNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 0,
    "content": "Integration packages - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationIntegration packagesLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewAll providersPopular ProvidersOpenAIAnthropicGoogleAWS (Amazon)Hugging FaceMicrosoftOllamaIntegrations by componentChat modelsTools and toolkitsRetrieversText splittersEmbedding modelsVector storesDocument loadersKey-value storesIntegration packagesCopy pageCopy pageLangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more."
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 1,
    "content": "To see a full list of integrations by component type, refer to the categories in the sidebar.\n‚ÄãPopular providers"
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 2,
    "content": "ProviderPackage API referenceDownloadsLatest versionJS/TS supportOpenAIlangchain-openai‚úÖGoogle VertexAIlangchain-google-vertexai‚úÖAWSlangchain-aws‚úÖGoogle Generative AIlangchain-google-genai‚úÖAnthropiclangchain-anthropic‚úÖGroqlangchain-groq‚úÖOllamalangchain-ollama‚úÖChromalangchain-chroma‚úÖHuggingfacelangchain-huggingface‚úÖCoherelangchain-cohere‚úÖPostgreslangchain-postgres‚úÖMistralAIlangchain-mistralai‚úÖPineconelangchain-pinecone‚úÖDatabricksdatabricks-langchain‚úÖPerplexitylangchain-perplexity‚úÖIBMlangchain-ibm‚úÖMongoDBlangchain-mongodb‚úÖDeepseeklangchain-deepseek‚úÖNvidia AI Endpointslangchain-nvidia-ai-endpoints‚ùåMilvuslangchain-milvus‚úÖTavilylangchain-tavily‚úÖFireworkslangchain-fireworks‚úÖQdrantlangchain-qdrant‚úÖElasticsearchlangchain-elasticsearch‚úÖLiteLLMlangchain-litellm‚ùåDataStax Astra"
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 3,
    "content": "Astra DBlangchain-astradb‚úÖTogetherlangchain-together‚úÖXAIlangchain-xai‚úÖRedislangchain-redis‚úÖAzure AIlangchain-azure-ai‚úÖMCP Toolboxtoolbox-langchain‚ùåGoogle Communitylangchain-google-community‚ùåUnstructuredlangchain-unstructured‚úÖGraph RAGlangchain-graph-retriever‚ùåNeo4Jlangchain-neo4j‚úÖ"
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 4,
    "content": "‚ÄãAll providers\nSee all providers or search for a provider using the search field.\nIf you‚Äôd like to contribute an integration, see our contributing guide."
  },
  {
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "chunk_id": 5,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoAll integration providersNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 0,
    "content": "Evaluation Concepts - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation ConceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 1,
    "content": "performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageDatasetsExamplesDataset curationManually curated examplesHistorical tracesSynthetic dataSplitsVersionsEvaluatorsEvaluator inputsEvaluator outputsDefining evaluatorsEvaluation techniquesHumanHeuristicLLM-as-judgePairwiseExperimentExperiment configurationRepetitionsConcurrencyCachingAnnotation queuesOffline evaluationBenchmarkingUnit testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 2,
    "content": "testsRegression testsBacktestingPairwise evaluationOnline evaluationTestingEvaluations vs testingUsing pytest and Vitest/JestEvaluation ConceptsCopy pageCopy pageLangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 3,
    "content": "Datasets: Collections of test inputs and reference outputs.\nEvaluators: Functions for scoring outputs. These can be online evaluators that run on traces in real time or offline evaluators that run on a dataset.\n\n‚ÄãDatasets\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.\n\n‚ÄãExamples\nEach example consists of:\n\nInputs: a dictionary of input variables to pass to your application.\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 4,
    "content": "‚ÄãDataset curation\nThere are various ways to build datasets for evaluation, including:\n‚ÄãManually curated examples\nThis is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what ‚Äúgood‚Äù responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.\n‚ÄãHistorical traces\nOnce you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they‚Äôre, well, the most realistic!"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 5,
    "content": "If you‚Äôre getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 6,
    "content": "User feedback: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.\nHeuristics: You can also use other heuristics to identify ‚Äúinteresting‚Äù datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.\nLLM feedback: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 7,
    "content": "‚ÄãSynthetic data\nOnce you have a few examples, you can try to artificially generate some more. It‚Äôs generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.\n‚ÄãSplits"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 8,
    "content": "‚ÄãSplits\nWhen setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.\nLearn how to create and manage dataset splits.\n‚ÄãVersions"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 9,
    "content": "Learn how to create and manage dataset splits.\n‚ÄãVersions\nDatasets are versioned such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset‚Äôs history.\nYou can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn‚Äôt accidentally break your CI pipelines.\n‚ÄãEvaluators\nEvaluators are functions that score how well your application performs on a particular example.\n‚ÄãEvaluator inputs\nEvaluators receive these inputs:"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 10,
    "content": "Example: The example(s) from your Dataset. Contains inputs, (reference) outputs, and metadata.\nRun: The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.\n\n‚ÄãEvaluator outputs\nAn evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:\n\nkey: The name of the metric.\nscore | value: The value of the metric. Use score if it‚Äôs a numerical metric and value if it‚Äôs categorical.\ncomment (optional): The reasoning or additional string information justifying the score.\n\n‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 11,
    "content": "‚ÄãDefining evaluators\nThere are a number of ways to define and run evaluators:\n\nCustom code: Define custom evaluators as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.\nBuilt-in evaluators: LangSmith has a number of built-in evaluators that you can configure and run via the UI."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 12,
    "content": "You can run evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring Rules to automatically run them on particular tracing projects or datasets.\n‚ÄãEvaluation techniques\nThere are a few high-level approaches to LLM evaluation:\n‚ÄãHuman\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\nLangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 13,
    "content": "LangSmith‚Äôs annotation queues make it easy to get human feedback on your application‚Äôs outputs.\n‚ÄãHeuristic\nHeuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot‚Äôs response isn‚Äôt empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.\n‚ÄãLLM-as-judge\nLLM-as-judge evaluators use LLMs to score the application‚Äôs output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference)."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 14,
    "content": "With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.\nLearn about how to define an LLM-as-a-judge evaluator.\n‚ÄãPairwise\nPairwise evaluators allow you to compare the outputs of two versions of an application. Think LMSYS Chatbot Arena - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic (‚Äúwhich response is longer‚Äù), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\nWhen should you use pairwise evaluation?"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 15,
    "content": "When should you use pairwise evaluation?\nPairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.\nLearn how run pairwise evaluations.\n‚ÄãExperiment\nEach time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 16,
    "content": "Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple experiments in a comparison view."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 17,
    "content": "‚ÄãExperiment configuration\nLangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.\n‚ÄãRepetitions\nRunning an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.\nRepetitions can be configured by passing the num_repetitions argument to evaluate / aevaluate (Python, TypeScript). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.\nTo learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 18,
    "content": "To learn more about running repetitions on experiments, read the how-to-guide.\n‚ÄãConcurrency\nBy passing the max_concurrency argument to evaluate / aevaluate, you can specify the concurrency of your experiment. The max_concurrency argument has slightly different semantics depending on whether you are using evaluate or aevaluate.\nevaluate\nThe max_concurrency argument to evaluate specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.\naevaluate"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 19,
    "content": "aevaluate\nThe max_concurrency argument to aevaluate is fairly similar to evaluate, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. aevaluate works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The max_concurrency argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.\n‚ÄãCaching\nLastly, you can also cache the API calls made in your experiment by setting the LANGSMITH_TEST_CACHE to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 20,
    "content": "‚ÄãAnnotation queues\nHuman feedback is often the most valuable feedback you can gather on your application. With annotation queues you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a dataset for future evaluations. While you can always annotate runs inline, annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.\nLearn more about annotation queues and human feedback.\n‚ÄãOffline evaluation"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 21,
    "content": "Learn more about annotation queues and human feedback.\n‚ÄãOffline evaluation\nEvaluating an application on a dataset is what we call ‚Äúoffline‚Äù evaluation. It is offline because we‚Äôre evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application‚Äôs outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.\nYou can run offline evaluations client-side using the LangSmith SDK (Python and TypeScript). You can run them server-side via the Prompt Playground or by configuring automations to run certain evaluators on every new experiment against a specific dataset."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 22,
    "content": "‚ÄãBenchmarking"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 23,
    "content": "Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 24,
    "content": "like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 25,
    "content": "‚ÄãUnit tests\nUnit tests are used in software development to verify the correctness of individual system components. Unit tests in the context of LLMs are often rule-based assertions on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.\nUnit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).\n‚ÄãRegression tests"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 26,
    "content": "‚ÄãRegression tests\nRegression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.\nLangSmith‚Äôs comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 27,
    "content": "‚ÄãBacktesting\nBacktesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\nThis is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.\n‚ÄãPairwise evaluation"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 28,
    "content": "‚ÄãPairwise evaluation\nFor some tasks it is easier for a human or LLM grader to determine if ‚Äúversion A is better than B‚Äù than to assign an absolute score to either A or B. Pairwise evaluations are just this ‚Äî¬†a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine ‚ÄúWhich of these two summaries is more clear and concise?‚Äù than to give an absolute score like ‚ÄúGive this summary a score of 1-10 in terms of clarity and concision.‚Äù\nLearn how run pairwise evaluations.\n‚ÄãOnline evaluation"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 29,
    "content": "Learn how run pairwise evaluations.\n‚ÄãOnline evaluation\nEvaluating a deployed application‚Äôs outputs in (roughly) realtime is what we call ‚Äúonline‚Äù evaluation. In this case there is no dataset involved and no possibility of reference outputs ‚Äî we‚Äôre running evaluators on real inputs and real outputs as they‚Äôre produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 30,
    "content": "Online evaluators are generally intended to be run server-side. LangSmith has built-in LLM-as-judge evaluators that you can configure, or you can define custom code evaluators that are also run within LangSmith."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 31,
    "content": "‚ÄãTesting\n‚ÄãEvaluations vs testing\nTesting and evaluation are very similar and overlapping concepts that often get confused.\nAn evaluation measures performance according to a metric(s). Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they‚Äôre often used to compare two systems against each other rather than to assert something about an individual system.\nTesting asserts correctness. A system can only be deployed if it passes all tests.\nEvaluation metrics can be turned into tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 32,
    "content": "It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.\nYou can also choose to write evaluations using standard software testing tools like pytest or vitest/jest out of convenience.\n‚ÄãUsing pytest and Vitest/Jest\nThe LangSmith SDKs come with integrations for pytest and Vitest/Jest. These make it easy to:"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 33,
    "content": "Track test results in LangSmith\nWrite evaluations as tests"
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 34,
    "content": "Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.\nWriting evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 35,
    "content": "Using testing tools is also helpful when you want to both evaluate your system‚Äôs outputs and assert some basic things about them."
  },
  {
    "url": "https://docs.smith.langchain.com/concepts/evaluation",
    "chunk_id": 36,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoEvaluation quickstartPreviousApplication-specific evaluation approachesNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/",
    "chunk_id": 0,
    "content": "Redirecting...\n\n\n\n\n\n\nRedirecting..."
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 0,
    "content": "Assistants - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationAssistantsAssistantsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartApp developmentConfigure for deploymentApplication structureSetupDeployment componentsRebuild graph at runtimeInteract with a deployment using RemoteGraphAdd semantic search to your agent deploymentAdd TTLs to your applicationApp developmentData modelsAssistantsOverviewManage assistantsUse threadsRunsCore capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd"
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 1,
    "content": "capabilitiesTutorialsStudioOverviewQuickstartRuns, assistants, threadsTraces, datasets, promptsTroubleshootingAuth & access controlOverviewAdd custom authenticationSet up custom authenticationMake conversations privateConnect an authentication providerDocument API authentication in OpenAPISet up Agent Auth (Beta)Server customizationAdd custom lifespan eventsAdd custom middlewareAdd custom routesReferenceRemoteGraphLangGraph CLILangGraph Server environment variablesOn this pageConfigurationVersioningExecutionVideo guideApp developmentData modelsAssistantsAssistantsCopy pageCopy pageAssistants allow you to manage configurations (like prompts, LLM selection, tools) separately from your graph‚Äôs core logic, enabling rapid changes that don‚Äôt alter the graph architecture. It is a way to create"
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 2,
    "content": "LLM selection, tools) separately from your graph‚Äôs core logic, enabling rapid changes that don‚Äôt alter the graph architecture. It is a way to create multiple specialized versions of the same graph architecture, each optimized for different use cases through configuration variations rather than structural changes."
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 3,
    "content": "For example, imagine a general-purpose writing agent built on a common graph architecture. While the structure remains the same, different writing styles‚Äîsuch as blog posts and tweets‚Äîrequire tailored configurations to optimize performance. To support these variations, you can create multiple assistants (e.g., one for blogs and another for tweets) that share the underlying graph but differ in model selection and system prompt."
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 4,
    "content": "The LangGraph API provides several endpoints for creating and managing assistants and their versions. See the API reference for more details.\nAssistants are a LangSmith concept. They are not available in the open source LangGraph library.\n‚ÄãConfiguration\nAssistants build on the LangGraph open source concept of configuration.\nWhile configuration is available in the open source LangGraph library, assistants are only present in LangSmith. This is due to the fact that assistants are tightly coupled to your deployed graph. Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph‚Äôs default configuration settings."
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 5,
    "content": "In practice, an assistant is just an instance of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangGraph Server API provides several endpoints for creating and managing assistants. See the API reference and this how-to for more details on how to create assistants.\n‚ÄãVersioning\nAssistants support versioning to track changes over time.\nOnce you‚Äôve created an assistant, subsequent edits to that assistant will create new versions. See this how-to for more details on how to manage assistant versions.\n‚ÄãExecution"
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 6,
    "content": "‚ÄãExecution\nA run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread.\nLangSmith API provides several endpoints for creating and managing runs. See the API reference for more details.\n‚ÄãVideo guide"
  },
  {
    "url": "https://docs.langchain.com/langsmith/assistants",
    "chunk_id": 7,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoHow to add TTLs to your applicationPreviousManage assistantsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#ux",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/langgraph-cloud-beta",
    "chunk_id": 0,
    "content": "LangGraph Cloud Beta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upLangGraph Cloud WaitlistThank you for your interest in LangGraph Cloud. The waitlist form has closed. LangGraph Cloud is currently available in open beta to all LangSmith Plus and Enterprise plan users.\n\n\nGo to LangGraph webpage"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 0,
    "content": "Perplexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nAn AI¬†answer engine that lets you handle complex query searches like a (Perplexity) Pro\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 1,
    "content": "IntroductionSearch like a pro‚ÄúWhere knowledge begins.‚Äù Perplexity‚Äôs pithy motto reflects its mission to save users time by providing precise knowledge as an AI ‚Äúanswer engine.‚ÄùRecently, the Perplexity team launched Pro Search, a feature that can answer complex, nuanced questions using multi-step reasoning. Unlike Perplexity‚Äôs quick search, which is designed for off-the-cuff questions, this advanced modality helps students, researchers, and enterprises gain precise, relevant responses to even the most complex and detailed questions.¬†¬†Thanks to the Perplexity team‚Äôs thoughtful approach to crafting user experience and agent architecture, they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 2,
    "content": "they didn‚Äôt have to compromise on speed and accuracy, even when increasing the complexity of its systems.ProblemWhen traditional search falls shortTraditional search engines may struggle to answer complex queries that require connecting the dots across multiple ideas or extracting detailed information. For instance, searching \"What‚Äôs the educational background of the founders of LangChain?\" involves not only identifying the founders but also researching into each individual founder‚Äôs background.This is where Perplexity Pro Search shines. Their AI agent breaks down multi-step questions to deliver well-organized, factual answers. Instead of sifting through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 3,
    "content": "through countless pages of search results, users get direct responses from Perplexity Pro Search that summarize the most relevant information. In fact, query search volume of Perplexity Pro Search has increased by over 50% in the past few months, as more users discover its ability to answer tricky questions quickly and efficiently.Cognitive architectureStep-by-step planning and executionPerplexity Pro‚Äôs AI agent separates planning from execution, which yields better results for multi-step search.¬†When a user submits a query, the AI creates a plan‚Äî a step-by-step guide to answering it. For each step in the plan, a list of search queries are generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 4,
    "content": "generated and executed. These steps are executed sequentially, and results from previous steps are passed when executing steps after. These search queries return a list of documents, which are grouped and then filtered down to the most relevant ones. The highly-ranked documents are then passed to an LLM to generate a final answer.Perplexity Pro Search also supports specialized tools such as code interpreters, which allow users to run calculations or analyze files on the fly, as well as mathematics evaluations tools like Wolfram Alpha.Prompt engineeringBalancing prompt length to yield fast, accurate responsesPerplexity uses a variety of language models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 5,
    "content": "models to break down web search tasks, giving users the flexibility to choose the model that best fits the problem they‚Äôre trying to solve. Since each language model processes and interprets prompts differently, Perplexity customizes prompts on the backend that are tailored to each individual model.¬†In order to guide the model‚Äôs behavior, Perplexity leverages techniques like few-shot prompt examples and chain-of-thought prompting. Few-shot examples allow engineers to steer the search agent‚Äôs behavior. When constructing few-shot examples, maintaining the right balance in prompt length was crucial. Crafting the rules that the language model should follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 6,
    "content": "follow also involved several rounds of iteration.¬†William Zhang, the engineer who led this effort at Perplexity, shared: \"It‚Äôs harder for models to follow the instructions of really complex prompts. Much of the iteration involves asking queries after each prompt change and checking that not only the output made sense, but that the intermediate steps were sensible as well.\" By keeping the rules in the system prompt simple and precise, Perplexity reduced the cognitive load for models to understand the task and generate relevant responses.EvaluationHow much smarter is this product?Perplexity relied on both answer quality metrics and internal dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 7,
    "content": "dogfooding before shipping the upgrade of Pro Search. The team conducted manual evaluations by testing Pro Search on a wide range of queries and comparing its answers side-by-side with other AI products. The ability to inspect intermediate steps was also critical in helping identify common errors before shipping to users.¬†To scale up their evaluations, Perplexity gathered a large batch of questions and used an LLM-as-a-Judge to rank the answers. Additionally, A/B tests were run on users to gauge their reactions to different possible configurations of the product, such as tradeoffs between latency and costs across different models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 8,
    "content": "models. The product was ready to be shipped after the Perplexity team was satisfied with the product experience from both an answer quality and UX perspective.UXDesigning a better waiting game for usersOne of the biggest challenges for the team was designing the Perplexity Pro Search user interface. Perplexity found that users were more willing to wait for results if the product would display the intermediate progress.This led to the development of an interactive UI that shows the plan being executed step-by-step. The team iterated on expandable sections that allow the user to click on individual steps to see more details on a search. They also introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 9,
    "content": "introduced the ability to hover over citations to see snippets from sources that the user could click on to open in a new window.¬†Zhang highlights their guiding philosophy behind the design:‚ÄúYou don‚Äôt want to overload the user with too much information until they are actually curious. Then, you feed their curiosity.‚Äù¬†The team wanted to make sure that the user interface found the best balance of simplicity and utility, requiring several iteration cycles.¬†ConclusionSearch at the speed of curiosityPerplexity‚Äôs Pro Search represents a significant advancement in AI-powered search and question-answering. By breaking down complex queries into manageable steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 10,
    "content": "steps and providing a transparent, interactive interface, Perplexity has created a powerful tool that works at the speed of curiosity.¬†As Zhang emphasizes: ‚ÄúIt is important that we design our product with the user in mind, since our users span a wide range of familiarity with AI systems. Some are experts while others are new to AI search interfaces ‚Äì so we have to make sure we‚Äôre creating a positive experience for everyone, regardless of their expertise level.‚Äù¬†Their development process offers valuable lessons for others building AI agents:1. Have the LLM do an explicit planning step when doing more complicated research2. Speed alongside answer quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 11,
    "content": "quality is important for creating a good user experience. Keep users engaged with dynamic UI feedback instead of leaving them waiting.But that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/perplexity#introduction",
    "chunk_id": 12,
    "content": "Go back to main pageRead next storyReplit\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy",
    "chunk_id": 0,
    "content": "Talk to our team\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy",
    "chunk_id": 1,
    "content": "AboutCareersPricingGet a demoSign upTrusted by the best teams building with LLMs:Talk to our teamLangChain provides a suite of products to help teams build AI applications. This includes LangSmith ‚Äî our platform for prompt engineering, evaluation, and observability of LLM apps ‚Äî and LangGraph Platform for building and deploying AI agents at scale.Get in touch. Our team will answer your questions and follow up with a product demo to help you drive results.Looking for support? Email support here.Trusted by the best teams building with LLMs:First NameLast NameCompany NameWork Email* ¬†personal emails will not be accepted.Job TitleWhich of our products are you interested in?LangSmithLangGraph PlatformCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global HeadquartersNA - West CoastNA -"
  },
  {
    "url": "https://www.langchain.com/contact-sales-copy",
    "chunk_id": 2,
    "content": "are you interested in?LangSmithLangGraph PlatformCompany Size1 - 20 21 - 100101 - 500501 - 2k2k+Company Global HeadquartersNA - West CoastNA - CentralNA - East CoastAPACEMEALATAMMessage (optional)Thank you for requesting a demo. There‚Äôs an email waiting in your inbox. Talk to you soon!Oops! Something went wrong while submitting the form."
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 0,
    "content": "Morningstar Intelligence Engine puts personalized investment insights at analysts' fingertips\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorningstar Intelligence Engine puts personalized investment insights at analysts' fingertips\n\nCase Studies\n2 min read\nNov 14, 2023"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 1,
    "content": "ChallengeFinancial services is one of the most data-driven industries and financial professionals are always hungry for more data and better tools to drive value for their clients. Morningstar‚Äìa publicly-traded investment research firm‚Äìhas been compiling and analyzing fund, stock, and general market data for finance professionals since its founding in 1984.¬†Getting the most out of Morningstar and its products has historically required a deep familiarity with the financial landscape, an ability to parse meticulous research reports, and a mastery of their proprietary‚Äìand powerful‚Äìtoolkit. With the rise of Generative AI, the Morningstar team saw an opportunity to make their data more accessible and more immediately useful to a wider range of users.SolutionMorningstar started by building a"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 2,
    "content": "saw an opportunity to make their data more accessible and more immediately useful to a wider range of users.SolutionMorningstar started by building a chatbot, Mo, that allows Morningstar customers to query their extensive research database using natural language in a conversational format to generate concise yet nuanced insights in seconds.¬†The team also decided to extend the benefits of this innovation to their customers‚Äìoften asset managers and wealth advisors‚Äìand enable them to build white-labeled chatbots and other AI tools of their own. Using their new platform, Morningstar Intelligence Engine, Morningstar customers can now securely upload their own research information - to provide more context and personalization to Morningstar data - and then ‚Äúwhitelist‚Äù the chatbot for their end"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 3,
    "content": "their own research information - to provide more context and personalization to Morningstar data - and then ‚Äúwhitelist‚Äù the chatbot for their end customers to use.¬†Several Morningstar team members were already familiar with LangChain through their contributions to the open source repository. The collaborative, community-driven spirit first drew the team to LangChain. When it came time to build a production-ready application, LangChain was the obvious choice because:LangChain offered a robust framework for initiating LLM-powered application development. The concepts and ideas presented in LangChain were easily comprehensible and implementable from prototype to production, which expedited the team‚Äôs development process.LangChain had a blueprint for RAG applications, making discovery and"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 4,
    "content": "from prototype to production, which expedited the team‚Äôs development process.LangChain had a blueprint for RAG applications, making discovery and summarization easy. Morningstar integrated various essential concepts from LangChain, including prompt templates, RAG-based approach, the ReAct framework, creating vector embeddings, LangChain evaluators, function calling, and output parsing. The choice of retrieval techniques improved performance out-of-the-box.LangChain helped with edge cases like retries that helped harden Morningstar‚Äôs applicationLangChain integrations provided wide-ranging support across various vector databases, enhancing its versatility and applicability.‚ÄúLangChain introduced critical cognitive architectures that facilitate a better grasp of generative AI, enriching our"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 5,
    "content": "versatility and applicability.‚ÄúLangChain introduced critical cognitive architectures that facilitate a better grasp of generative AI, enriching our team‚Äôs understanding of this evolving technology,‚Äù said Jinyoung Kim, Head of Development. ‚ÄúThe LangChain team keeps up the rapidly-evolving corpus of research, and then implements that into LangChain so we can benefit from the latest advances.‚Äù¬†Morningstar Intelligence Engine went from idea to production in under 60 days with a lean team of 5 developers.Results‚ÄúCustomers have been coming to us for almost 40 years to deliver cutting-edge insights and tools that help them make better investment decisions,\" said Adam Wheat, Chief Technology Officer.¬† ‚ÄúBy launching and evolving the Intelligence Engine, Morningstar continues its tradition of"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 6,
    "content": "decisions,\" said Adam Wheat, Chief Technology Officer.¬† ‚ÄúBy launching and evolving the Intelligence Engine, Morningstar continues its tradition of creating innovative products that help drive more value for their clients.‚Äù"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 7,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/morningstar-intelligence-engine-puts-personalized-investment-insights-at-analysts-fingertips/",
    "chunk_id": 8,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r?_gl=1*h5vp1f*_gcl_au*MTU4Njc4OTgwMy4xNzQ3NzYyMjgx*_ga*MTA0MzgxMzQ2OS4xNzM3MDUyNDkw*_ga_47WX3HKKY2*czE3NDkxNTQ1NDEkbzEzNyRnMSR0MTc0OTE1Njk5MSRqMjQkbDAkaDA.",
    "chunk_id": 0,
    "content": "LangSmith"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 0,
    "content": "Superhuman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nEvaluation\n\nUX\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 1,
    "content": "IntroductionAsk away with Ask AI Superhuman‚Äôs Ask AI product has users declaring: ‚ÄúI can‚Äôt live without it!‚ÄùSearching through the 45,873 emails in your inbox and finding yourself unable to recall the right keyword or fumbling with Gmail tags is an all-too-common frustration for busy people who spend their days in email and calendars. Superhuman set out to solve this challenge with Ask AI, its AI-powered search assistant. Designed to transform how users navigate their inboxes and calendars, Ask AI delivers instant, context-aware answers to even the most complex queries ‚Äì such as ‚ÄúWhen did I meet the founder of that series A startup for lunch?‚ÄùProblemWho, what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 2,
    "content": "what, when, where, why use email keyword search?Superhuman noticed there was one area that users were spending a significant amount of time ‚Äì email and calendar search. For up to 35 minutes per week, users tried to recall exact phrases and sender names using the traditional keyword search in their email clients.The team realized that a semantic search experience could improve productivity and help users spend less time searching. In the past few months since the release of Ask AI, Superhuman has already seen users cut search time by 5 minutes every week, for a 14% time savings. Cognitive architectureTransforming queries into insightful responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 3,
    "content": "responsesWhen initially designing the Ask AI architecture, the Superhuman team used a single prompt LLM that performed retrieval augmented generation (RAG). The goal was to empower users to query their inboxes and calendars and retrieve relevant tasks, events, or messages.¬†The diagram below above shows their first version, which generated retrieval parameters using JSON mode that were passed through hybrid search and heuristic reranking before the LLM produced an answer.However, the single-prompt design had a few shortcomings. First, the LLM did not always follow task-specific instructions reliably. They also found that the LLM struggled to reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 4,
    "content": "reason about dates accurately (e.g. identify upcoming deadlines). Their system also only handled certain search types well ‚Äì such as finding flights or summarizing company updates ‚Äì but not others such as calendar availability or complex multi-step searchesThese limitations pushed the Superhuman team to transition to a more complex cognitive architecture. Their new agent architecture (as shown in the diagram below) could understand user intent and provide more accurate responses. It worked as follows:1. Query classification and parameter generationWhen a user submits a query, two parallel processes occur for the Ask AI agent:¬†Tool classification: The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 5,
    "content": "The system classifies the query based on user intent to determine which tools or data sources to activate. The classifier identifies whether the query requires:some text1) Email search only¬†2) Email + calendar event search¬†3) Checking availability¬†4) Scheduling an event¬†5) Direct LLM response without tools.Metadata extraction: Simultaneously, the system extracts relevant tool parameters such as time filters, sender names, or relevant attachments. These will be used in retrieval to narrow the scope of search to improve accuracy.¬†This tool classification ensures that only relevant tools are invoked, which improves response quality. It will also be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 6,
    "content": "be used in the response generation step (to specify which prompts to use).‚Äç2. Task-specific tool useOnce the query is classified, the appropriate tools would be called. If the task required search, it would be passed into the search tool (with a hybrid semantic + keyword search) with reranking algorithms to prioritize the most relevant information.‚Äç‚Äç3. Response generation:‚ÄçBased on the classification in step 1, the system would select different prompts and preferences. Prompts would contain context-specific instructions with query-specific examples, and also encoded user preferences. The LLM, guided by a system prompt with clear instructions and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 7,
    "content": "and encoded user preferences, would then synthesize information to generate a tailored response.¬†Instead of relying on one large, all-encompassing prompt, the Ask AI agent used task-specific guidelines during post-processing. This allowed the agent to maintain consistent quality across diverse tasks. ‚ÄçBy transitioning to this parallel, multi-process architecture, Superhuman created a more reliable agent and also hit these RAG expectations:Sub-2-second responses to maintain a smooth user experienceReduced hallucinations through post-processing layers and brief follow-upPrompt engineeringDouble dippingTo ensure consistent quality across responses, Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 8,
    "content": "Superhuman implemented a few different prompt engineering strategies. First, they structured their prompts by adding in chatbot rules to define system behavior, task-specific guidelines, and semantic few-shot examples to guide the LLM. This nesting of rules helped the LLM reliably follow instructions.¬†The most interesting technique the Superhuman team adopted was \"double dipping\" instructions. By repeating key instructions in both the initial system prompt and final user message, they ensured that essential guidelines were rigorously followed. This dual reinforcement of instructions helped maintain clarity and consistency, leading to more reliable outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 9,
    "content": "outputs.EvaluationValidating results with feedbackWhen starting to test Ask AI‚Äôs performance, Superhuman first tested against a static dataset of questions and answers. They looked at retrieval accuracy based on this test set and would compare how changes to their prompt impacted accuracy.¬†The team also adopted a \"launch and learn\" approach, systematically rolling out Ask AI to more users. First, they collected thumbs up / thumbs down feedback from internal pod stakeholders. Then, they launched the feature to the whole company with the same method.Once they received enough positive feedback, Ask AI was launched to a dedicated AI beta group, then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 10,
    "content": "then to their community champions, and eventually a beta waitlist. This strategy allowed the Superhuman team to identify the most pressing user needs and prioritize improvements accordingly ‚Äì leading to a four-month testing process that culminated in a GA launch.UXDual power: Integrating Ask AI for email search flexibilityAsk AI integrates into Superhuman's email app interface in two key ways:1. Within the search bar, where users can toggle between traditional search and Ask AI.2. As a chat-like interface, where users can ask follow-up questions and see the conversation history.The team deliberated a lot on whether to integrate Ask AI solely in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 11,
    "content": "in search, solely as an agent, or both. Ultimately, through user feedback and testing, they found that there was value to users in both options ‚Äì so they kept both interfaces available.¬†With Ask AI, users also have the flexibility to choose between semantic or regular search, offering greater control over their search experience. To avoid incorrect answers, Ask AI would also validate uncertain results with the user before providing a final answer. As such, the Superhuman team paid careful attention to response speed, aiming to provide answers as quickly as possible while maintaining accuracyConclusionSmarter searches, happier usersSuperhuman's Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 12,
    "content": "Ask AI represents a thoughtful approach to transforming email search through AI. By honing in on user needs, iterating quickly, and employing clever prompting techniques like double dipping instructions, they've created a tool that slashes search time and improves the overall email experience.As AI continues to advance, tools like Ask AI pave the way for more capable assistants that seamlessly blend into our everyday workflows.And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/superhuman#introduction",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyPerplexity\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/built-with-langgraph",
    "chunk_id": 0,
    "content": "Built with LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upBuilt with LangGraphHear how industry leaders use LangGraph to ship powerful, production-ready AI applications.Start buildingGet a demo"
  },
  {
    "url": "https://www.langchain.com/built-with-langgraph",
    "chunk_id": 1,
    "content": "With LangGraph, build expressive AI agent workflows that won‚Äôt break under pressure.AI that hires top talentLinkedIn's AI recruiter streamlines hiring with conversational search, candidate matching, and a hierarchal agent system powered by LangGraph. \n\n\nRead moreAI that tests and (and fixes) codeTo tackle large-scale code migrations, Uber's Developer Platform team used LangGraph to build a network of agents and automate unit test generation.\n\n\nRead moreAI that scales customer supportKlarna's AI Assistant, powered by LangGraph and LangSmith, handles customer support tasks for 85 million active users ‚Äî reducing customer resolution time by 80%."
  },
  {
    "url": "https://www.langchain.com/built-with-langgraph",
    "chunk_id": 2,
    "content": "Read moreAI that identifies security attacksElastic orchestrates their AI¬†agents for threat detection scenarios using LangGraph.¬†Their GenAI¬†features have reduced ¬†labor-intensive SecOps tasks.\n\n\nRead moreAI that runs property managementAppFolio's copilot, Realm-X, helps property managers make decisions faster. After switching to LangGraph, response accuracy increased 2x ‚Äî¬†and they've saved 10+ hours a week."
  },
  {
    "url": "https://www.langchain.com/built-with-langgraph",
    "chunk_id": 3,
    "content": "Read moreResources for building agents in productionLangChain Academy CourseIntroduction to LangGraphVIDEOBuilding Effective Agents with LangGraphBLOGIn The Loop Blog SeriesReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#agent-adoption",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 0,
    "content": "Trace with OpenTelemetry - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationIntegrationsTrace with OpenTelemetryGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude Agent SDKClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 1,
    "content": "& troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pageTrace a LangChain applicationTrace a non-LangChain applicationSend traces to an alternate providerUse environment variables for global configurationConfigure alternate OTLP"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 2,
    "content": "a non-LangChain applicationSend traces to an alternate providerUse environment variables for global configurationConfigure alternate OTLP endpointsSupported OpenTelemetry attribute and event mappingCore LangSmith attributesGenAI standard attributesGenAI request parametersGenAI usage metricsTraceLoop attributesOpenInference attributesLLM attributesPrompt template attributesRetriever attributesTool attributesLogfire attributesOpenTelemetry event mappingEvent attribute extractionImplementation examplesTrace using the LangSmith SDKAdvanced configurationUse OpenTelemetry Collector for fan-outDistributed tracing with LangChain and OpenTelemetryContext propagation in distributed tracingSet up distributed tracing with LangChainTracing setupIntegrationsTrace with OpenTelemetryCopy pageCopy"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 3,
    "content": "propagation in distributed tracingSet up distributed tracing with LangChainTracing setupIntegrationsTrace with OpenTelemetryCopy pageCopy pageLangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks."
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 4,
    "content": "Learn how to trace your LLM applications using OpenTelemetry with LangSmith.\nUpdate the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com.\n‚ÄãTrace a LangChain application\nIf you‚Äôre using LangChain or LangGraph, use the built-in integration to trace your application:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 5,
    "content": "Install the LangSmith package with OpenTelemetry support:\npipCopypip install \"langsmith[otel]\"\npip install langchain\n\nRequires Python SDK version langsmith>=0.3.18. We recommend langsmith>=0.4.25 to benefit from important OpenTelemetry fixes.\n\n\nIn your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the LANGSMITH_OTEL_ENABLED environment variable:\nShellCopyLANGSMITH_OTEL_ENABLED=true\nLANGSMITH_TRACING=true\nLANGSMITH_ENDPOINT=https://api.smith.langchain.com\nLANGSMITH_API_KEY=<your_langsmith_api_key>\n# For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use."
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 6,
    "content": "Create a LangChain application with tracing. For example:\nCopyimport os\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Create a chain\nprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nmodel = ChatOpenAI()\nchain = prompt | model\n\n# Run the chain\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result.content)\n\n\n\nView the traces in your LangSmith dashboard (example) once your application runs.\n\n\n‚ÄãTrace a non-LangChain application\nFor non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend langsmith ‚â• 0.4.25.)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 7,
    "content": "Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:\npipCopypip install openai\npip install opentelemetry-sdk\npip install opentelemetry-exporter-otlp\n\n\n\nSetup environment variables for the endpoint, substitute your specific values:\nShellCopyOTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otel\nOTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=<your langsmith api key>\""
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 8,
    "content": "Depending on how your otel exporter is configured, you may need to append /v1/traces to the endpoint if you are only sending traces.\nIf you‚Äôre self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append /api/v1. For example: OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel\nOptional: Specify a custom project name other than ‚Äúdefault‚Äù:\nShellCopyOTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otel\nOTEL_EXPORTER_OTLP_HEADERS=\"x-api-key=<your langsmith api key>,Langsmith-Project=<project name>\""
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 9,
    "content": "Log a trace.\nThis code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes.\nCopyfrom openai import OpenAI\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n)\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\notlp_exporter = OTLPSpanExporter(\n    timeout=10,\n)\n\ntrace.set_tracer_provider(TracerProvider())\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(otlp_exporter)\n)\n\ntracer = trace.get_tracer(__name__)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 10,
    "content": "tracer = trace.get_tracer(__name__)\n\ndef call_openai():\n    model = \"gpt-4o-mini\"\n    with tracer.start_as_current_span(\"call_open_ai\") as span:\n        span.set_attribute(\"langsmith.span.kind\", \"LLM\")\n        span.set_attribute(\"langsmith.metadata.user_id\", \"user_123\")\n        span.set_attribute(\"gen_ai.system\", \"OpenAI\")\n        span.set_attribute(\"gen_ai.request.model\", model)\n        span.set_attribute(\"llm.request.type\", \"chat\")\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"Write a haiku about recursion in programming.\"\n            }\n        ]"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 11,
    "content": "for i, message in enumerate(messages):\n            span.set_attribute(f\"gen_ai.prompt.{i}.content\", str(message[\"content\"]))\n            span.set_attribute(f\"gen_ai.prompt.{i}.role\", str(message[\"role\"]))\n\n        completion = client.chat.completions.create(\n            model=model,\n            messages=messages\n        )"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 12,
    "content": "completion = client.chat.completions.create(\n            model=model,\n            messages=messages\n        )\n\n        span.set_attribute(\"gen_ai.response.model\", completion.model)\n        span.set_attribute(\"gen_ai.completion.0.content\", str(completion.choices[0].message.content))\n        span.set_attribute(\"gen_ai.completion.0.role\", \"assistant\")\n        span.set_attribute(\"gen_ai.usage.prompt_tokens\", completion.usage.prompt_tokens)\n        span.set_attribute(\"gen_ai.usage.completion_tokens\", completion.usage.completion_tokens)\n        span.set_attribute(\"gen_ai.usage.total_tokens\", completion.usage.total_tokens)\n\n        return completion.choices[0].message\n\nif __name__ == \"__main__\":\n    call_openai()\n\n\n\nView the trace in your LangSmith dashboard (example)."
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 13,
    "content": "‚ÄãSend traces to an alternate provider\nWhile LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.\nAvailable in LangSmith Python SDK ‚â• 0.4.1. We recommend ‚â• 0.4.25 for fixes that improve OTEL export and hybrid fan-out stability.\n‚ÄãUse environment variables for global configuration\nBy default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:\nCopyOTEL_EXPORTER_OTLP_ENDPOINT: Override the endpoint URL\nOTEL_EXPORTER_OTLP_HEADERS: Add custom headers (LangSmith API keys and Project are added automatically)\nOTEL_SERVICE_NAME: Set a custom service name (defaults to \"langsmith\")"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 14,
    "content": "LangSmith uses the HTTP trace exporter by default. If you‚Äôd like to use your own tracing provider, you can either:\n\nSet the OTEL environment variables as shown above, or\nSet a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.\n\n‚ÄãConfigure alternate OTLP endpoints\nTo send traces to a different provider, configure the OTLP exporter with your provider‚Äôs endpoint:\nCopyimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 15,
    "content": "# Set environment variables for LangChain\nos.environ[\"LANGSMITH_OTEL_ENABLED\"] = \"true\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\n# Configure the OTLP exporter for your custom endpoint\nprovider = TracerProvider()\notlp_exporter = OTLPSpanExporter(\n    # Change to your provider's endpoint\n    endpoint=\"https://otel.your-provider.com/v1/traces\",\n    # Add any required headers for authentication\n    headers={\"api-key\": \"your-api-key\"}\n)\nprocessor = BatchSpanProcessor(otlp_exporter)\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n\n# Create and run a LangChain application\nprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nmodel = ChatOpenAI()\nchain = prompt | model\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result.content)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 16,
    "content": "Hybrid tracing is available in version ‚â• 0.4.1. To send traces only to your OTEL endpoint, set:LANGSMITH_OTEL_ONLY=\"true\"\n(Recommendation: use langsmith ‚â• 0.4.25.)\n‚ÄãSupported OpenTelemetry attribute and event mapping\nWhen sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:\n‚ÄãCore LangSmith attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 17,
    "content": "When sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:\n‚ÄãCore LangSmith attributes\nOpenTelemetry attributeLangSmith fieldNoteslangsmith.trace.nameRun nameOverrides the span name for the runlangsmith.span.kindRun typeValues: llm, chain, tool, retriever, embedding, prompt, parserlangsmith.trace.session_idSession IDSession identifier for related traceslangsmith.trace.session_nameSession nameName of the sessionlangsmith.span.tagsTagsCustom tags attached to the span (comma-separated)langsmith.metadata.{key}metadata.{key}Custom metadata with langsmith prefix\n‚ÄãGenAI standard attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 18,
    "content": "OpenTelemetry attributeLangSmith fieldNotesgen_ai.systemmetadata.ls_providerThe GenAI system (e.g., ‚Äúopenai‚Äù, ‚Äúanthropic‚Äù)gen_ai.operation.nameRun typeMaps ‚Äúchat‚Äù/‚Äúcompletion‚Äù to ‚Äúllm‚Äù, ‚Äúembedding‚Äù to ‚Äúembedding‚Äùgen_ai.promptinputsThe input prompt sent to the modelgen_ai.completionoutputsThe output generated by the modelgen_ai.prompt.{n}.roleinputs.messages[n].roleRole for the nth input messagegen_ai.prompt.{n}.contentinputs.messages[n].contentContent for the nth input messagegen_ai.prompt.{n}.message.roleinputs.messages[n].roleAlternative format for rolegen_ai.prompt.{n}.message.contentinputs.messages[n].contentAlternative format for contentgen_ai.completion.{n}.roleoutputs.messages[n].roleRole for the nth output messagegen_ai.completion.{n}.contentoutputs.messages[n].contentContent for"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 19,
    "content": "for the nth output messagegen_ai.completion.{n}.contentoutputs.messages[n].contentContent for the nth output messagegen_ai.completion.{n}.message.roleoutputs.messages[n].roleAlternative format for rolegen_ai.completion.{n}.message.contentoutputs.messages[n].contentAlternative format for contentgen_ai.input.messagesinputs.messagesArray of input messagesgen_ai.output.messagesoutputs.messagesArray of output messagesgen_ai.tool.nameinvocation_params.tool_nameTool name, also sets run type to ‚Äútool‚Äù"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 20,
    "content": "‚ÄãGenAI request parameters"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 21,
    "content": "OpenTelemetry attributeLangSmith fieldNotesgen_ai.request.modelinvocation_params.modelThe model name used for the requestgen_ai.response.modelinvocation_params.modelThe model name returned in the responsegen_ai.request.temperatureinvocation_params.temperatureTemperature settinggen_ai.request.top_pinvocation_params.top_pTop-p sampling settinggen_ai.request.max_tokensinvocation_params.max_tokensMaximum tokens settinggen_ai.request.frequency_penaltyinvocation_params.frequency_penaltyFrequency penalty settinggen_ai.request.presence_penaltyinvocation_params.presence_penaltyPresence penalty settinggen_ai.request.seedinvocation_params.seedRandom seed used for generationgen_ai.request.stop_sequencesinvocation_params.stopSequences that stop"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 22,
    "content": "settinggen_ai.request.seedinvocation_params.seedRandom seed used for generationgen_ai.request.stop_sequencesinvocation_params.stopSequences that stop generationgen_ai.request.top_kinvocation_params.top_kTop-k sampling parametergen_ai.request.encoding_formatsinvocation_params.encoding_formatsOutput encoding formats"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 23,
    "content": "‚ÄãGenAI usage metrics\nOpenTelemetry attributeLangSmith fieldNotesgen_ai.usage.input_tokensusage_metadata.input_tokensNumber of input tokens usedgen_ai.usage.output_tokensusage_metadata.output_tokensNumber of output tokens usedgen_ai.usage.total_tokensusage_metadata.total_tokensTotal number of tokens usedgen_ai.usage.prompt_tokensusage_metadata.input_tokensNumber of input tokens used (deprecated)gen_ai.usage.completion_tokensusage_metadata.output_tokensNumber of output tokens used (deprecated)gen_ai.usage.details.reasoning_tokensusage_metadata.reasoning_tokensNumber of reasoning tokens used\n‚ÄãTraceLoop attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 24,
    "content": "‚ÄãTraceLoop attributes\nOpenTelemetry attributeLangSmith fieldNotestraceloop.entity.inputinputsFull input value from TraceLooptraceloop.entity.outputoutputsFull output value from TraceLooptraceloop.entity.nameRun nameEntity name from TraceLooptraceloop.span.kindRun typeMaps to LangSmith run typestraceloop.llm.request.typeRun type‚Äùembedding‚Äù maps to ‚Äúembedding‚Äù, others to ‚Äúllm‚Äùtraceloop.association.properties.{key}metadata.{key}Custom metadata with traceloop prefix\n‚ÄãOpenInference attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 25,
    "content": "‚ÄãOpenInference attributes\nOpenTelemetry attributeLangSmith fieldNotesinput.valueinputsFull input value, can be string or JSONoutput.valueoutputsFull output value, can be string or JSONopeninference.span.kindRun typeMaps various kinds to LangSmith run typesllm.systemmetadata.ls_providerLLM system providerllm.model_namemetadata.ls_model_nameModel name from OpenInferencetool.nameRun nameTool name when span kind is ‚ÄúTOOL‚Äùmetadatametadata.*JSON string of metadata to be merged\n‚ÄãLLM attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 26,
    "content": "‚ÄãLLM attributes\nOpenTelemetry attributeLangSmith fieldNotesllm.input_messagesinputs.messagesInput messagesllm.output_messagesoutputs.messagesOutput messagesllm.token_count.promptusage_metadata.input_tokensPrompt token countllm.token_count.completionusage_metadata.output_tokensCompletion token countllm.token_count.totalusage_metadata.total_tokensTotal token countllm.usage.total_tokensusage_metadata.total_tokensAlternative total token countllm.invocation_parametersinvocation_params.*JSON string of invocation parametersllm.presence_penaltyinvocation_params.presence_penaltyPresence penaltyllm.frequency_penaltyinvocation_params.frequency_penaltyFrequency penaltyllm.request.functionsinvocation_params.functionsFunction definitions\n‚ÄãPrompt template attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 27,
    "content": "‚ÄãPrompt template attributes\nOpenTelemetry attributeLangSmith fieldNotesllm.prompt_template.variablesRun typeSets run type to ‚Äúprompt‚Äù, used with input.value\n‚ÄãRetriever attributes\nOpenTelemetry attributeLangSmith fieldNotesretrieval.documents.{n}.document.contentoutputs.documents[n].page_contentContent of the nth retrieved documentretrieval.documents.{n}.document.metadataoutputs.documents[n].metadataMetadata of the nth retrieved document (JSON)\n‚ÄãTool attributes\nOpenTelemetry attributeLangSmith fieldNotestoolsinvocation_params.toolsArray of tool definitionstool_argumentsinvocation_params.tool_argumentsTool arguments as JSON or key-value pairs\n‚ÄãLogfire attributes"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 28,
    "content": "‚ÄãLogfire attributes\nOpenTelemetry attributeLangSmith fieldNotespromptinputsLogfire prompt inputall_messages_eventsoutputsLogfire message events outputeventsinputs/outputsLogfire events array, splits input/choice events\n‚ÄãOpenTelemetry event mapping"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 29,
    "content": "‚ÄãOpenTelemetry event mapping\nEvent nameLangSmith fieldNotesgen_ai.content.promptinputsExtracts prompt content from event attributesgen_ai.content.completionoutputsExtracts completion content from event attributesgen_ai.system.messageinputs.messages[]System message in conversationgen_ai.user.messageinputs.messages[]User message in conversationgen_ai.assistant.messageoutputs.messages[]Assistant message in conversationgen_ai.tool.messageoutputs.messages[]Tool response messagegen_ai.choiceoutputsModel choice/response with finish reasonexceptionstatus, errorSets status to ‚Äúerror‚Äù and extracts exception message/stacktrace\n‚ÄãEvent attribute extraction\nFor message events, the following attributes are extracted:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 30,
    "content": "content ‚Üí message content\nrole ‚Üí message role\nid ‚Üí tool_call_id (for tool messages)\ngen_ai.event.content ‚Üí full message JSON\n\nFor choice events:\n\nfinish_reason ‚Üí choice finish reason\nmessage.content ‚Üí choice message content\nmessage.role ‚Üí choice message role\ntool_calls.{n}.id ‚Üí tool call ID\ntool_calls.{n}.function.name ‚Üí tool function name\ntool_calls.{n}.function.arguments ‚Üí tool function arguments\ntool_calls.{n}.type ‚Üí tool call type\n\nFor exception events:\n\nexception.message ‚Üí error message\nexception.stacktrace ‚Üí error stacktrace (appended to message)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 31,
    "content": "For exception events:\n\nexception.message ‚Üí error message\nexception.stacktrace ‚Üí error stacktrace (appended to message)\n\n‚ÄãImplementation examples\n‚ÄãTrace using the LangSmith SDK\nUse the LangSmith SDK‚Äôs OpenTelemetry helper to configure export:\nCopyimport asyncio\nfrom langsmith.integrations.otel import configure\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\n\n# Configure LangSmith OpenTelemetry export (no OTEL env vars or headers needed)\nconfigure(project_name=\"adk-otel-demo\")\n\n\nasync def main():\n    agent = LlmAgent(\n        name=\"travel_assistant\",\n        model=\"gemini-2.5-flash-lite-latest\",\n        instruction=\"You are a helpful travel assistant.\",\n    )"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 32,
    "content": "session_service = InMemorySessionService()\n    runner = Runner(app_name=\"travel_app\", agent=agent, session_service=session_service)\n\n    user_id = \"user_123\"\n    session_id = \"session_abc\"\n    await session_service.create_session(app_name=\"travel_app\", user_id=user_id, session_id=session_id)\n\n    new_message = types.Content(parts=[types.Part(text=\"Hi! Recommend a weekend trip to Paris.\")], role=\"user\")\n\n    for event in runner.run(user_id=user_id, session_id=session_id, new_message=new_message):\n        print(event)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nYou do not need to set OTEL environment variables or exporters. configure() wires them for LangSmith automatically; instrumentors (like GoogleADKInstrumentor) create the spans."
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 33,
    "content": "View the trace in your LangSmith dashboard (example).\n\n‚ÄãAdvanced configuration\n‚ÄãUse OpenTelemetry Collector for fan-out\nFor more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.\n\n\nInstall the OpenTelemetry Collector for your environment.\n\n\nCreate a configuration file (e.g., otel-collector-config.yaml) that exports to multiple destinations:\nCopyreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n\nprocessors:\n  batch:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 34,
    "content": "processors:\n  batch:\n\nexporters:\n  otlphttp/langsmith:\n    endpoint: https://api.smith.langchain.com/otel/v1/traces\n    headers:\n      x-api-key: ${env:LANGSMITH_API_KEY}\n      Langsmith-Project: my_project\n  otlphttp/other_provider:\n    endpoint: https://otel.your-provider.com/v1/traces\n    headers:\n      api-key: ${env:OTHER_PROVIDER_API_KEY}\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp/langsmith, otlphttp/other_provider]"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 35,
    "content": "service:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlphttp/langsmith, otlphttp/other_provider]\n\n\n\nConfigure your application to send to the collector:\nCopyimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 36,
    "content": "# Point to your local OpenTelemetry Collector\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4318/v1/traces\"\n)\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(otlp_exporter)\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n\n# Set environment variables for LangChain\nos.environ[\"LANGSMITH_OTEL_ENABLED\"] = \"true\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\n\n# Create and run a LangChain application\nprompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nmodel = ChatOpenAI()\nchain = prompt | model\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result.content)\n\n\n\nThis approach offers several advantages:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 37,
    "content": "This approach offers several advantages:\n\nCentralized configuration for all your telemetry destinations\nReduced overhead in your application code\nBetter scalability and resilience\nAbility to add or remove destinations without changing application code\n\n‚ÄãDistributed tracing with LangChain and OpenTelemetry\nDistributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry‚Äôs context propagation capabilities ensure that traces remain connected across service boundaries.\n‚ÄãContext propagation in distributed tracing\nIn distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 38,
    "content": "Trace ID: A unique identifier for the entire trace\nSpan ID: A unique identifier for the current span\nSampling Decision: Indicates whether this trace should be sampled\n\n‚ÄãSet up distributed tracing with LangChain\nTo enable distributed tracing across multiple services:\nCopyimport os\nfrom opentelemetry import trace\nfrom opentelemetry.propagate import inject, extract\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nimport requests\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 39,
    "content": "# Set up OpenTelemetry trace provider\nprovider = TracerProvider()\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"https://api.smith.langchain.com/otel/v1/traces\",\n    headers={\"x-api-key\": os.getenv(\"LANGSMITH_API_KEY\"), \"Langsmith-Project\": \"my_project\"}\n)\nprocessor = BatchSpanProcessor(otlp_exporter)\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\ntracer = trace.get_tracer(__name__)\n\n# Service A: Create a span and propagate context to Service B\ndef service_a():\n    with tracer.start_as_current_span(\"service_a_operation\") as span:\n        # Create a chain\n        prompt = ChatPromptTemplate.from_template(\"Summarize: {text}\")\n        model = ChatOpenAI()\n        chain = prompt | model"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 40,
    "content": "# Run the chain\n        result = chain.invoke({\"text\": \"OpenTelemetry is an observability framework\"})\n\n        # Propagate context to Service B\n        headers = {}\n        inject(headers)  # Inject trace context into headers\n\n        # Call Service B with the trace context\n        response = requests.post(\n            \"http://service-b.example.com/process\",\n            headers=headers,\n            json={\"summary\": result.content}\n        )\n        return response.json()\n\n# Service B: Extract the context and continue the trace\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 41,
    "content": "# Service B: Extract the context and continue the trace\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/process\", methods=[\"POST\"])\ndef service_b_endpoint():\n    # Extract the trace context from the request headers\n    context = extract(request.headers)\n    with tracer.start_as_current_span(\"service_b_operation\", context=context) as span:\n        data = request.json\n        summary = data.get(\"summary\", \"\")\n\n        # Process the summary with another LLM chain\n        prompt = ChatPromptTemplate.from_template(\"Analyze the sentiment of: {text}\")\n        model = ChatOpenAI()\n        chain = prompt | model\n        result = chain.invoke({\"text\": summary})\n\n        return jsonify({\"analysis\": result.content})\n\nif __name__ == \"__main__\":\n    app.run(port=5000)"
  },
  {
    "url": "https://docs.langchain.com/langsmith/trace-with-opentelemetry",
    "chunk_id": 42,
    "content": "return jsonify({\"analysis\": result.content})\n\nif __name__ == \"__main__\":\n    app.run(port=5000)\n\n\nEdit the source of this page on GitHubWas this page helpful?YesNoTrace with OpenAI Agents SDKPreviousTrace with Semantic KernelNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#evaluation",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 0,
    "content": "The LangChain Community\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upConnect with theLangChain CommunityMeet new peers, ask for advice, and share your knowledge.The LangChain Community is where you learn to build the agents of tomorrow.\n\n\nMeet members of the communityand share your knowledge"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 1,
    "content": "Meet members of the communityand share your knowledge\n\n\n\nHelp ForumsNeed help with LangChain products or have questions about implementation? Connect with fellow builders for advice, share best practices, and explore answers in our community-run forums.Sign up\n\n\n\n\n\n\nCommunity SlackJump into our Slack and hang out with the LangChain developer community. Stay in the loop on industry events, job opportunities, and projects other developers are building.Sign up\n\n\n\n\n\n\nEventsMeet IRL or virtually with fellow LangChain enthusiasts. Stay in the loop by subscribing to our calendar for all the latest meetups, events, and hands-on workshops.Visit calendar"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 2,
    "content": "LangChain Community ChampionsBuilding LangChain is a team effort ‚Äî over 3,500 amazing contributors have poured their time into making LangChain the vibrant ecosystem it is today.Our Community Champions are our MVPs ‚Äî squashing bugs, tackling issues, adding new features, and crafting great docs. Not all heroes wear capes; some submit incredible PRs! And we‚Äôre here to celebrate them.Community Champions enjoy direct access to the LangChain team, influence the roadmap, get priority reviews, and, of course, score some company swag.cbornetLangChain PythonmarlenezwLangChain PythonsepiatoneLangChain PythonafirstenbergLangChain JSclemenspetersLangChain JSgbaian10LangGraph PythonInterested in becoming a Community Champion? ¬†Start by contributing to our open source packages.Start contributing"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 3,
    "content": "LangChain AmbassadorsThe LangChain Ambassador Program brings together our most passionate community members from across the globe. Ambassadors aren‚Äôt just volunteers ‚Äî they're the heart and soul of spreading the LangChain magic!Whether they‚Äôre hosting meetups, leading vibrant local or digital communities, or creating amazing educational content, they‚Äôre making LangChain accessible for everyone.And the best part? Ambassadors get special perks like sneak peeks at new products, grants to sponsor their events, and free access to LangSmith.ColinAustin, TXEdenTel Aviv, IsraelViratNew York, NYFranciscoBuenos Aires, ArgentinaRickPhoenix, AZDanielBucharest, RomaniaHarryShanghai, ChinaTomOrange County, CAGregSan Francisco, CAAndresMiami, FLYukiTokyo, JPMasahiroTokyo, JPShingoTokyo, JPKim Wee"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 4,
    "content": "RomaniaHarryShanghai, ChinaTomOrange County, CAGregSan Francisco, CAAndresMiami, FLYukiTokyo, JPMasahiroTokyo, JPShingoTokyo, JPKim Wee TehSingapore, SingaporeHaiOttawa, CanadaSriAmsterdam, NetherlandsShivDublin, IrelandMayoLondon, UKTeddySeoul, KoreaRaviHyderabad, IndiaNirUnited StatesLeeTel Aviv, IsraelGalNew York, NYIvanDubai, UAESanjeedBengaluru, IndiaZamalNuremberg, GermanyGustafStockholm, SwedenInterested in becoming a LangChain Ambassador?Applications are now open until¬†October 6th.Apply nowProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your"
  },
  {
    "url": "https://www.langchain.com/community",
    "chunk_id": 5,
    "content": "AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/resources-items/state-of-ai-agents-report",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://forum.langchain.com/",
    "chunk_id": 0,
    "content": "LangChain Forum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain Forum\n\n\n\n\n\n\n\n\nCategory\nTopics\n\n\n\n\n\n\n\n\n\n\nAnnouncements\n\n\nOfficial updates from the LangChain team including product updates.\n\n\n\n3\n\n\n\n\n\n\n\n\n\nOSS Product Help\n\n\nGet help with using our OSS frameworks from experts in the community.\n\n\n\n0\n\n\n\n\n\n\n\n\n\nLangSmith Product Help\n\n\nGet help with using LangSmith from experts in the community.\n\n\n\n45\n\n\n\n\n\n\n\n\n\nLangChain Academy\n\n\nFor questions and discussions about LangChain Academy courses.\n\n\n\n19\n\n\n\n\n\n\n\n\n\nTalking Shop\n\n\nFor discussion about LangChain or anything related to building agentic applications.\n\n\n\n11\n\n\n\n\n\n\n\n\n\nForum Feedback\n\n\nDiscussion about our Help Forums, its organization, how it works, and how we can improve it.\n\n\n\n7\n\n\n\n\n\n\n\n\n\n\n\nHome \n\n\n\n\nCategories \n\n\n\n\nGuidelines"
  },
  {
    "url": "https://forum.langchain.com/",
    "chunk_id": 1,
    "content": "7\n\n\n\n\n\n\n\n\n\n\n\nHome \n\n\n\n\nCategories \n\n\n\n\nGuidelines \n\n\n\n\nTerms of Service \n\n\n\n\nPrivacy Policy \n\n\n\n\nPowered by Discourse, best viewed with JavaScript enabled"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 0,
    "content": "Discover errors and usage patterns with Insights - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangSmithSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationMonitoring & alertingDiscover errors and usage patterns with InsightsGet startedObservabilityEvaluationPrompt engineeringDeploymentHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 1,
    "content": "& managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pagePrerequisitesRun your first Insights jobUnderstand the resultsTop-level categoriesSubcategoriesIndividual tracesConfigure a jobSelect runsCategoriesSummary promptAttributesSave your configMonitoring & alertingDiscover errors and usage patterns with InsightsCopy pageCopy pageLangSmith Insights"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 2,
    "content": "promptAttributesSave your configMonitoring & alertingDiscover errors and usage patterns with InsightsCopy pageCopy pageLangSmith Insights automatically analyzes your traces to detect usage patterns, common agent behaviors and failure modes ‚Äî without requiring you to manually review thousands of traces."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 3,
    "content": "Insights uses hierarchical categorization to make sense of your data and highlight actionable trends.\nInsights is in Beta and under active development. To provide feedback or use this feature, reach out to the LangSmith team.\n‚ÄãPrerequisites\nAn OpenAI API key ‚Äî generate one from the OpenAI dashboard.\n‚ÄãRun your first Insights job\nFrom the LangSmith UI:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 4,
    "content": "Navigate to Tracing Projects in the left-hand menu and select a tracing project.\nClick +New in the top right corner then New Insights Job to kick off a new Insights job.\nEnter a name for your job.\nClick the  icon in the top right of the job creation pane to set your OpenAI API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step.\nClick Create."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 5,
    "content": "This will kick off a background Insights job. Jobs can take up to 20 minutes to complete.\nGenerating insights over 1,000 runs typically costs $0.50-$1.00 in OpenAI API calls. The cost grows linearly in the number of runs and the size of each run.\n‚ÄãUnderstand the results\nOnce your job has completed, you can navigate to the Insights tab where you‚Äôll see a table of Insights jobs. Each job contains insights generated over a specific sample of runs from the tracing project.\nInsights jobs for a single tracing project\nClick into your job to see traces organized into a set of auto-generated categories.\nYou can drill down through categories and subcategories to view the underlying traces, feedback, and run statistics.\nCommon topics of conversations with the https://chat.langchain.com chatbot"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 6,
    "content": "Common topics of conversations with the https://chat.langchain.com chatbot\n‚ÄãTop-level categories\nYour traces are automatically grouped into top-level categories that represent the broadest patterns in your data.\nThe distribution bars show how frequently each pattern occurs, making it easy to spot behaviors that happen more or less than expected.\nEach category has a brief description and displays aggregated metrics over the traces it contains, including:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 7,
    "content": "Typical runs stats (like error rates, latency, cost)\nFeedback scores from your evaluators\nAttributes extracted as part of the job\n\n‚ÄãSubcategories\nClicking on any category shows a breakdown into subcategories, which gives you a more granular understanding of interaction patterns in that category of traces.\nIn the Chat Langchain example pictured above, under ‚ÄúData & Retrieval‚Äù there are subcategories like ‚ÄúVector Stores‚Äù and ‚ÄúData Ingestion‚Äù.\n‚ÄãIndividual traces\nYou can view the traces assigned to each category or subcategory by clicking through to see the runs table. From there, you can click into any trace to see the full conversation details.\n‚ÄãConfigure a job\nWhen kicking off an Insights job, you can configure the following:\n‚ÄãSelect runs"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 8,
    "content": "Sample size: The maximum number of traces to analyze. Currently capped at 1,000\nTime range: Traces are sampled from this time range\nFilters: Additional run filters. As you adjust filters, you‚Äôll see how many traces match your criteria"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 9,
    "content": "‚ÄãCategories\nBy default, top-level categories are automatically generated bottom-up from the underlying traces.\nIn some instances, you know specific categories you‚Äôre interested in upfront and want the job to bucket traces into those predefined categories.\nThe Categories section of the config lets you do this by enumerating the names and descriptions of the top-level categories you want to be used.\nSubcategories are still auto-generated by the algorithm within the predefined top-level categories.\n‚ÄãSummary prompt\nThe first step of the job is to create a brief summary of every trace ‚Äî¬†it is these summaries that are then categorized.\nExtracting the right information in the summary is essential for getting useful categories.\nThe prompt used to generate these summaries can be edited."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 10,
    "content": "The prompt used to generate these summaries can be edited.\nThe two things to think about when editing the prompt are:"
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 11,
    "content": "Summarization instructions: Any information that isn‚Äôt in the trace summary won‚Äôt affect the categories that get generated, so make sure to provide clear instructions on what information is important to extract from each trace.\nTrace content: Use mustache formatting to specify which parts of each trace are passed to the summarizer. Large traces with lots of inputs and outputs can be expensive and noisy. Reducing the prompt to only include the most relevant parts of the trace can improve your results."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 12,
    "content": "For specifying trace content, you can access run inputs via {{run.inputs}} and the outputs via {{run.outputs}}. For example, the prompt \"Summarize this: {{run.inputs}}\" will include (a JSON serialization of) all of the run inputs. The prompt \"Summarize this: {{run.inputs.foo.bar}}\" will include only the ‚Äúbar‚Äù value within the ‚Äúfoo‚Äù value of the run inputs.\n‚ÄãAttributes\nAlong with a summary, you can define additional categorical, numerical, and boolean attributes to be extracted from each trace.\nThese attributes will influence the categorization step ‚Äî¬†traces with similar attribute values will tend to be categorized together.\nYou can also see aggregations of these attributes per category."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 13,
    "content": "You can also see aggregations of these attributes per category.\nAs an example, you might want to extract the attribute user_satisfied: boolean from each trace to steer the algorithm towards categories that split up positive and negative user experiences, and to see the average user satisfaction per category.\n‚ÄãSave your config\nYou can optionally save Insights job configs for future reuse.\nThis is especially useful if you want to periodically run Insights and compare results over time to identify changes in user and agent behavior.\nSelect from previously saved configs in the dropdown in the top-left corner of the pane when creating a new Insights job."
  },
  {
    "url": "https://docs.langchain.com/langsmith/insights",
    "chunk_id": 14,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoConfigure webhook notifications for LangSmith alertsPreviousRun (span) data formatNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://trust.langchain.com/",
    "chunk_id": 0,
    "content": "Vanta"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 0,
    "content": "Add and manage memory - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesAdd and manage memoryLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangGraph v1.0Release notesGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelAdd and manage memorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 1,
    "content": "APIsGraph APIFunctional APIRuntimeOn this pageAdd short-term memoryUse in productionUse in subgraphsAdd long-term memoryUse in productionUse semantic searchManage short-term memoryTrim messagesDelete messagesSummarize messagesManage checkpointsView thread stateView the history of the threadDelete all checkpoints for a threadPrebuilt memory toolsCapabilitiesAdd and manage memoryCopy pageCopy pageLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 2,
    "content": "AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 3,
    "content": "Add short-term memory as a part of your agent‚Äôs state to enable multi-turn conversations.\nAdd long-term memory to store user-specific or application-level data across sessions.\n\n‚ÄãAdd short-term memory\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:\nCopyfrom langgraph.checkpoint.memory import InMemorySaver  \nfrom langgraph.graph import StateGraph\n\ncheckpointer = InMemorySaver()  \n\nbuilder = StateGraph(...)\ngraph = builder.compile(checkpointer=checkpointer)  \n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  \n)\n\n‚ÄãUse in production\nIn production, use a checkpointer backed by a database:\nCopyfrom langgraph.checkpoint.postgres import PostgresSaver"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 4,
    "content": "‚ÄãUse in production\nIn production, use a checkpointer backed by a database:\nCopyfrom langgraph.checkpoint.postgres import PostgresSaver\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n    builder = StateGraph(...)\n    graph = builder.compile(checkpointer=checkpointer)  \n\nExample: using Postgres checkpointerCopypip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\nYou need to call checkpointer.setup() the first time you‚Äôre using Postgres checkpointer Sync AsyncCopyfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 5,
    "content": "model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:  \n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 6,
    "content": "graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 7,
    "content": "Example: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointerCopypip install -U pymongo langgraph langgraph-checkpoint-mongodb\nSetup\nTo use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don‚Äôt already have one. Sync AsyncCopyfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.mongodb import MongoDBSaver  \n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"localhost:27017\"\nwith MongoDBSaver.from_conn_string(DB_URI) as checkpointer:  \n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 8,
    "content": "def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 9,
    "content": "Example: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointerCopypip install -U langgraph langgraph-checkpoint-redis\nYou need to call checkpointer.setup() the first time you‚Äôre using Redis checkpointer Sync AsyncCopyfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver  \n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:  \n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 10,
    "content": "def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(checkpointer=checkpointer)  \n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        config,  \n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 11,
    "content": "‚ÄãUse in subgraphs\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\nCopyfrom langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()  \n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  \nbuilder.add_edge(START, \"node_1\")"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 12,
    "content": "# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)  \nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)  \n\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.\nCopysubgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)  \n\n‚ÄãAdd long-term memory\nUse long-term memory to store user-specific or application-specific data across conversations.\nCopyfrom langgraph.store.memory import InMemoryStore  \nfrom langgraph.graph import StateGraph\n\nstore = InMemoryStore()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 13,
    "content": "store = InMemoryStore()  \n\nbuilder = StateGraph(...)\ngraph = builder.compile(store=store)  \n\n‚ÄãUse in production\nIn production, use a store backed by a database:\nCopyfrom langgraph.store.postgres import PostgresStore\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresStore.from_conn_string(DB_URI) as store:  \n    builder = StateGraph(...)\n    graph = builder.compile(store=store)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 14,
    "content": "Example: using Postgres storeCopypip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\nYou need to call store.setup() the first time you‚Äôre using Postgres store Sync AsyncCopyfrom langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.store.postgres import PostgresStore  \nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 15,
    "content": "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\nwith (\n    PostgresStore.from_conn_string(DB_URI) as store,  \n    PostgresSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # store.setup()\n    # checkpointer.setup()\n\n    def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        store: BaseStore,  \n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  \n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 16,
    "content": "# Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})  \n\n        response = model.invoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        store=store,  \n    )"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 17,
    "content": "graph = builder.compile(\n        checkpointer=checkpointer,\n        store=store,  \n    )\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  \n            \"user_id\": \"1\",  \n        }\n    }\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        config,  \n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"2\",  \n            \"user_id\": \"1\",\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        config,  \n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 18,
    "content": "Example: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) storeCopypip install -U langgraph langgraph-checkpoint-redis\nYou need to call store.setup() the first time you‚Äôre using Redis store Sync AsyncCopyfrom langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver\nfrom langgraph.store.redis import RedisStore  \nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\n\nwith (\n    RedisStore.from_conn_string(DB_URI) as store,  \n    RedisSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    store.setup()\n    checkpointer.setup()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 19,
    "content": "def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        store: BaseStore,  \n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))  \n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 20,
    "content": "response = model.invoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        store=store,  \n    )\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  \n            \"user_id\": \"1\",  \n        }\n    }\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        config,  \n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 21,
    "content": "config = {\n        \"configurable\": {\n            \"thread_id\": \"2\",  \n            \"user_id\": \"1\",\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        config,  \n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n‚ÄãUse semantic search\nEnable semantic search in your graph‚Äôs memory store to let graph agents search for items in the store by semantic similarity.\nCopyfrom langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 22,
    "content": "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n\nLong-term memory with semantic searchCopy\nfrom langchain.embeddings import init_embeddings\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nllm = init_chat_model(\"openai:gpt-4o-mini\")\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 23,
    "content": "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n    # Search based on user's last message\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = llm.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 24,
    "content": "builder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")\n\n‚ÄãManage short-term memory\nWith short-term memory enabled, long conversations can exceed the LLM‚Äôs context window. Common solutions are:\n\nTrim messages: Remove first or last N messages (before calling LLM)\nDelete messages from LangGraph state permanently\nSummarize messages: Summarize earlier messages in the history and replace them with a summary\nManage checkpoints to store and retrieve message history\nCustom strategies (e.g., message filtering, etc.)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 25,
    "content": "This allows the agent to keep track of the conversation without exceeding the LLM‚Äôs context window.\n‚ÄãTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you‚Äôre using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.\nTo trim message history, use the trim_messages function:\nCopyfrom langchain_core.messages.utils import (  \n    trim_messages,  \n    count_tokens_approximately  \n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 26,
    "content": "def call_model(state: MessagesState):\n    messages = trim_messages(  \n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n\nFull example: trim messagesCopyfrom langchain_core.messages.utils import (\n    trim_messages,  \n    count_tokens_approximately  \n)\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START, MessagesState\n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-5\")\nsummarization_model = model.bind(max_tokens=128)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 27,
    "content": "model = init_chat_model(\"anthropic:claude-sonnet-4-5\")\nsummarization_model = model.bind(max_tokens=128)\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  \n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(START, \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 28,
    "content": "config = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\nCopy================================== Ai Message ==================================\n\nYour name is Bob, as you mentioned when you first introduced yourself."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 29,
    "content": "Your name is Bob, as you mentioned when you first introduced yourself.\n\n‚ÄãDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\nTo delete messages from the graph state, you can use the RemoveMessage. For RemoveMessage to work, you need to use a state key with add_messages reducer, like MessagesState.\nTo remove specific messages:\nCopyfrom langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 30,
    "content": "To remove all messages:\nCopyfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you‚Äôre using. For example:\nsome providers expect message history to start with a user message\nmost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\n\nFull example: delete messagesCopyfrom langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 31,
    "content": "def call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_sequence([call_model, delete_messages])\nbuilder.add_edge(START, \"call_model\")\n\ncheckpointer = InMemorySaver()\napp = builder.compile(checkpointer=checkpointer)\n\nfor event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 32,
    "content": "for event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\nCopy[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 33,
    "content": "‚ÄãSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the MessagesState to include a summary key:\nCopyfrom langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 34,
    "content": "Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This summarize_conversation node can be called after some number of messages have accumulated in the messages state key.\nCopydef summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\""
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 35,
    "content": "else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\nFull example: summarize messagesCopyfrom typing import Any, TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 36,
    "content": "Full example: summarize messagesCopyfrom typing import Any, TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import AnyMessage\nfrom langchain_core.messages.utils import count_tokens_approximately\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langmem.short_term import SummarizationNode, RunningSummary  \n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-5\")\nsummarization_model = model.bind(max_tokens=128)\n\nclass State(MessagesState):\n    context: dict[str, RunningSummary]  \n\nclass LLMInputState(TypedDict):  \n    summarized_messages: list[AnyMessage]\n    context: dict[str, RunningSummary]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 37,
    "content": "class LLMInputState(TypedDict):  \n    summarized_messages: list[AnyMessage]\n    context: dict[str, RunningSummary]\n\nsummarization_node = SummarizationNode(  \n    token_counter=count_tokens_approximately,\n    model=summarization_model,\n    max_tokens=256,\n    max_tokens_before_summary=256,\n    max_summary_tokens=128,\n)\n\ndef call_model(state: LLMInputState):  \n    response = model.invoke(state[\"summarized_messages\"])\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(State)\nbuilder.add_node(call_model)\nbuilder.add_node(\"summarize\", summarization_node)  \nbuilder.add_edge(START, \"summarize\")\nbuilder.add_edge(\"summarize\", \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 38,
    "content": "# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\nprint(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 39,
    "content": "final_response[\"messages\"][-1].pretty_print()\nprint(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n\nWe will keep track of our running summary in the context field\n(expected by the SummarizationNode).\nDefine private state that will be used only for filtering\nthe inputs to call_model node.\nWe‚Äôre passing a private input state here to isolate the messages returned by the summarization node\nCopy================================== Ai Message ==================================\n\nFrom our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 40,
    "content": "From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n\n‚ÄãManage checkpoints\nYou can view and delete the information stored by the checkpointer."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 41,
    "content": "‚ÄãManage checkpoints\nYou can view and delete the information stored by the checkpointer.\n\n‚ÄãView thread state\n Graph/Functional API Checkpointer APICopyconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\",  \n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 42,
    "content": "}\n}\ngraph.get_state(config)  \nCopyStateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 43,
    "content": "parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 44,
    "content": "‚ÄãView the history of the thread\n Graph/Functional API Checkpointer APICopyconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\nlist(graph.get_state_history(config))  \nCopy[\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 45,
    "content": "created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n        next=('call_model',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863421+00:00',"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 46,
    "content": "created_at='2025-05-05T16:01:23.863421+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=('__start__',),\n        config={...},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863173+00:00',"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 47,
    "content": "created_at='2025-05-05T16:01:23.863173+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=(),\n        config={...},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 48,
    "content": "created_at='2025-05-05T16:01:23.862295+00:00',\n        parent_config={...}\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n        next=('call_model',),\n        config={...},\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.278960+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n        interrupts=()\n    ),\n    StateSnapshot("
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 49,
    "content": "interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.277497+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n        interrupts=()\n    )\n]"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/add-memory",
    "chunk_id": 50,
    "content": "‚ÄãDelete all checkpoints for a thread\nCopythread_id = \"1\"\ncheckpointer.delete_thread(thread_id)\n\n‚ÄãPrebuilt memory tools\nLangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.\n\nEdit the source of this page on GitHubWas this page helpful?YesNoUse time-travelPreviousSubgraphsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 0,
    "content": "Why agent infrastructure matters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy agent infrastructure matters\nLearn why agent infrastructure is essential to handling stateful, long-running tasks ‚Äî and how LangGraph Platform provides the runtime support needed to build and scale reliable agents.\n\n4 min read\nJul 27, 2025"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 1,
    "content": "Over the past year, we‚Äôve seen the rise of agentic applications, from workflow copilots to codegen assistants to deep researchers. These agentic apps often combine tool use, memory, and reasoning to complete complex, multi-step tasks ‚Äî moving beyond retrieval and chat to action-taking.Agents are altering how we think of applications. Apps are no longer just a series of requests and responses ‚Äî reduced to input forms, buttons, visuals, or chat interfaces. Instead, agents remember, reason, and act. They work in the background off of events and information. They not only act but also ask for help, show their work, reason through problems, and collaborate in groups. And while stateless, serverless compute remains a north star for distributed systems, it falls short for agent-based"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 2,
    "content": "and collaborate in groups. And while stateless, serverless compute remains a north star for distributed systems, it falls short for agent-based applications. This new style of application requires new infrastructure.Agents often are:Long-running ‚Äî reasoning and executing on a task for minutes, not milliseconds.Stateful ‚Äî carrying memory and persisting context across steps or from past interactionsBursty ‚Äî especially in scheduled or user-triggered scenarios.Trying to shoehorn an agent with these characteristics into existing serverless or microservice architectures often leads to brittle, fault-prone systems.Why agent infrastructure is neededAgentic workloads demand new primitives ‚Äî ones that neither web backends nor traditional distributed systems provide. A new layer to the agent stack"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 3,
    "content": "workloads demand new primitives ‚Äî ones that neither web backends nor traditional distributed systems provide. A new layer to the agent stack is emerging: agent infrastructure. It sits between your agent logic and raw compute, providing structure, control, and reliability.Agent infrastructure is infrastructure purpose-built to support durable execution, complex state management, human-in-the-loop coordination, streaming and more ‚Äî without requiring you to stitch everything together yourself. That's why we built LangGraph Platform to be the best place to deploy and run your LangGraph applications with scalable, reliable agent infrastructure.Durable execution: Agent runs take timeMost serverless environments are optimized for short-lived tasks. But agents ‚Äî from research assistants to"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 4,
    "content": "execution: Agent runs take timeMost serverless environments are optimized for short-lived tasks. But agents ‚Äî from research assistants to workflow copilots ‚Äî can run for at least several seconds, and often minutes, or even hours in extremes. They might pause to call tools, wait on APIs, or receive human-in-the-loop feedback.In LangGraph, these agents are suspended and safely resumed later, but many systems can‚Äôt handle these long or unpredictable pauses. Without the right runtime, long-running agents risk timing out, crashing, or losing progress.Agent infrastructure provides:Background execution so agents can run independently of the initial requestHeartbeat signals to prevent timeouts by signaling ongoing progressResumable runs to restore from checkpoints after crashes or pausesThis"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 5,
    "content": "requestHeartbeat signals to prevent timeouts by signaling ongoing progressResumable runs to restore from checkpoints after crashes or pausesThis durability is essential for building reliable, production-grade agents that work through to successful completion of the task no matter how long it takes.State management: Agents need more than message buffersAn agent‚Äôs ‚Äústate‚Äù can include intermediate results, tool outputs, embedded documents, or multi-turn context. Traditional infra doesn't give you a structured way to store, resume, or edit this state mid-run. An agent infrastructure can provide the storage and checkpointing needed OOTB to persist state across steps, failures, or user interventions.Human-in-the-loop: Agents need to wait, pause, and resumeAgents frequently rely on human"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 6,
    "content": "persist state across steps, failures, or user interventions.Human-in-the-loop: Agents need to wait, pause, and resumeAgents frequently rely on human feedback for approval or clarification before continuing. That means you can‚Äôt just fire off a request and forget it. You need resumable state and support for arbitrarily long delays between steps.Agent infrastructure lets you use APIs to pause or resume without wiring together queues, caches, and polling endpoints manually.Bursty concurrency: Agents must absorb spikes in trafficReal-world agentic apps can face unpredictable surges ‚Äî whether it‚Äôs millions of users querying an AI search app at once, or a scheduled deep research assistant running its end-of-the-day job for entire teams. These spikes can lead to request traffic jams, dropped"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 7,
    "content": "once, or a scheduled deep research assistant running its end-of-the-day job for entire teams. These spikes can lead to request traffic jams, dropped requests, or degraded performance.An agent infrastructure like LangGraph Platform is designed to absorb these bursts with:Task queues, which buffer and schedule incoming runs effectively (even under heavy load)Horizontal auto-scaling, where new workers and queue instances can be added dynamically to ensure each run is processed just onceThis thus eliminates the need for custom load balancing logic and helps your agent remain stable even under high-throughput or unpredictable workloads.Streaming: See what your agents are thinkingAgents don‚Äôt just return a final result ‚Äî they think, act, and refine their response over time.Good agent"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 8,
    "content": "See what your agents are thinkingAgents don‚Äôt just return a final result ‚Äî they think, act, and refine their response over time.Good agent infrastructure makes their intermediate output (e.g. thoughts, tool calls, partial completions) visible to enhance user experience and developer visibility. For example, LangGraph Platform enables:Token-level streaming, which lets you stream LLM output token-by-token from any node, subgraph, tool, or task in LangGraph.Custom data streaming, which emits structured, user-defined data at any point in the graph using custom stream mode, enabling flexible communication beyond standard text responsesStreaming generative UI, which renders dynamic components for rich user interfaces.Since all streaming modes contain metadata context, you can trace every output"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 9,
    "content": "UI, which renders dynamic components for rich user interfaces.Since all streaming modes contain metadata context, you can trace every output back to its origin ‚Äî making it easier to debug and monitor your agent behavior too.How LangGraph Platform helpsLangGraph Platform gives you the infrastructure needed to run agentic workloads in production without building it yourself. You can deploy agents directly from GitHub with 1-click, and the runtime handles long-running execution, checkpointing, retries, memory, streaming, and traffic spikes out-of-the-box.If you‚Äôre building agents that are stateful, action taking, or need to pause and resume ‚Äî LangGraph Platform takes care of the hard parts so you can focus on agent logic and behavior, not infrastructure. See the docs to learn more.The future"
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 10,
    "content": "Platform takes care of the hard parts so you can focus on agent logic and behavior, not infrastructure. See the docs to learn more.The future is agentic ‚Äî so build on a solid foundationAgentic apps are here to stay. But like every shift in software ‚Äî whether from monoliths to microservices, or from on-prem to cloud‚Äî the transition requires new tools.Agent infrastructure is the missing layer. And if you're building anything beyond a stateless prompt, it's time to think seriously about what supports your agents behind the scenes.Get started with LangGraph Platform to learn more."
  },
  {
    "url": "https://blog.langchain.com/why-agent-infrastructure/",
    "chunk_id": 11,
    "content": "Join our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 0,
    "content": "LangChain State of AI Agents Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nState of AI AgentsWe surveyed over 1,300 professionals ‚Äî from engineers and product managers to business leaders and executives ‚Äî to uncover the state of AI agents. Dive into the data as we break down how AI agents are being used (or not) today."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 1,
    "content": "IntroductionInsightsAgent adoption trendsLeading agent use casesControls & guardrails for agentsBiggest challenges to deploying agents in productionAgent success storiesEmerging themes in AI agent adoptionConclusionMethodology"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 2,
    "content": "IntroductionIn 2024, AI agents are no longer a niche interest. Companies across industries are getting more serious about incorporating agents into their workflows - from automating mundane tasks, to assisting with data analysis or writing code.But what‚Äôs really happening behind the scenes? Are AI agents living up to their potential, or are they just another buzzword? Who‚Äôs been deploying them, and what‚Äôs preventing others from diving in headfirst?We surveyed over 1,300 professionals to learn about the state of AI agents in 2024. Let's dive into the data below.InsightsFirst, what even is an agent?At LangChain, we define an agent as a system that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 3,
    "content": "that uses an LLM to decide the control flow of an application. Just like the levels of autonomy for autonomous vehicles, there is also a spectrum of agentic capabilities.Agent adoption is a coin toss - but nearly everyone has plans for itThe agent race is heating up. In the past year, numerous agentic frameworks have gained enormous popularity ‚Äî whether it‚Äôs using ReAct to combine LLM reasoning and acting, multi-agent orchestrators, or a more controllable framework like LangGraph.Not all of the chatter on agents is Twitter hype. About 51% of respondents are using agents in production today. When we looked at the data by company size, mid-sized companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 4,
    "content": "companies (100 - 2000 employees) were the most aggressive with putting agents in production (at 63%).‚ÄçEncouragingly, 78% have active plans to implement agents into production soon. While it‚Äôs clear that the appetite for AI agents is strong, the actual production deployment still remains a hurdle to many.Does your company currently have agents in production?Are you currently developing an agent with plans to put it into production?We also continue to see companies moving beyond simple chat-based implementations into more advanced frameworks that emphasize multi-agent collaboration and more autonomous capabilities. (See more in the¬†\"Emerging themes\" section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90%"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 5,
    "content": "section below.)While the tech industry is usually known to be early adopters, interest in agents is gaining traction across all industries. 90% of respondents working in non-tech companies have or are planning to put agents in production (nearly equivalent to tech companies, at 89%).Leading agent use casesWhat are people using agents for? Agents are handling both routine tasks but also opening doors to new possibilities for knowledge work.‚ÄçThe top use cases for agents include performing research and summarization (58%), followed by streamlining tasks for personal productivity or assistance (53.5%).‚ÄçThese speak to the desire of people to have someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 6,
    "content": "someone (or something) else handle time-consuming tasks for them. Instead of sifting through endless data for literature review or research analysis, users can rely on AI agents to distill key insights from volumes of information. Similarly, AI agents are boosting personal productivity by assisting with everyday tasks like scheduling and organization, freeing users to focus on what matters.‚ÄçEfficiency gains aren‚Äôt limited to the individual. Customer service (45.8%) is another prime area for agent use cases, helping companies handle inquiries, troubleshoot, and speed up customer response times across teams.In your opinion, which tasks are agents best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 7,
    "content": "best suited to perform today?Better safe than sorry: Tracing and human oversight are needed to keep agents in checkWith great power comes great responsibility ‚Äî or at least the need for some brakes and controls for your agent. Tracing and observability tools top the list of must-have controls, helping developers get visibility into ¬†agent behaviors and performance. Most companies are also employing guardrails to keep agents from veering off course.What kind of controls do you have in place for agents?When it came to testing LLM applications, offline evaluation (39.8%) was mentioned as a strategy more often than online evaluation (32.5%). This may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 8,
    "content": "may speak to the difficulty of monitoring real-time performance. In the write-in responses, many companies also had human experts manually checking or evaluating responses for an added layer of precaution.‚ÄçDespite the excitement that folks have projected onto agents, most are taking a more conservative approach when it comes to how far we‚Äôll let agents go off the leash. Very few respondents allow their agent to read, write, and delete freely. Instead, most teams allow either read-only tool permissions or require human approval for more significant actions, such as writing or deleting.What kind of tool permissions does your agent have?Companies of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees)"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 9,
    "content": "of different sizes also weight their priorities differently when it comes to agent controls. Unsurprisingly, larger enterprises (2000+ employees) are more cautious, leaning heavily on ‚Äúread-only‚Äù permissions to avoid unnecessary risks. They also tend to pair guardrails with offline evaluations to catch regressions in pre-production, before customers see any responses.Tool permissions by company sizeMeanwhile, small companies and startups (<100 employees) are more focused on tracing to understand what‚Äôs happening in their agentic app (over other controls). From our conversations, smaller companies tend to focus on shipping and understanding the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 10,
    "content": "the results by just looking at the data; whereas enterprises put more controls in place across the board.Agent controls by company sizeWhile rates of agent adoption were similar across non-tech and tech company respondents, among those using agent controls in production, tech companies were more likely to use multiple control methods. 51% of tech respondents are currently using 2 or more control methods, compared to only 39% of respondents in other sectors. This suggests that tech companies may be further along in building reliable agents, as controls are needed for high-quality experiences.Number of methods used for controls or guardrailsBarriers and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 11,
    "content": "and challenges to getting agents into productionKeeping quality of an LLM application‚Äôs performance high ‚Äî from whether a response is accurate or if it adheres to the right style ‚Äî is not easy.Performance quality stands out as the top concern among respondents ‚Äî more than twice as significant as other factors like cost and safety.The inherent unpredictability of agents using LLMs to control workflows introduces more room for error, making it tough for teams to ensure that their agent consistently provides accurate, contextually-appropriate responses.What is your biggest limitation of putting more agents in production?For small companies especially, performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 12,
    "content": "performance quality far outweighs other considerations, with 45.8% citing it as a primary concern, compared to just 22.4% for cost (the next biggest concern). This gap underscores just how critical reliable, high-quality performance is for organizations to move agents from development to production.While quality is still top-of-mind for enterprises, safety concerns are also prevalent for these larger companies that must adhere to regulations and handle client data more sensitively.Barriers to deploying agents by company sizeThe challenge doesn‚Äôt end with quality. From write-in responses, many people feel uncertain about best practices for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 13,
    "content": "for building and testing agents. In particular, two major hurdles stand out: knowledge and time.‚Ä¢ Knowledge: Teams often struggle with the technical know-how required to work with agents, including implementing them for specific use cases. Many employees are still learning the ropes and need to upskill to leverage AI agents effectively. ‚Ä¢ Time: The time investment needed to build and deploy is significant, especially when trying to make sure agents perform reliably - which may require debugging, evaluating, finetuning, etc.Agent success stories: Cursor steals the spotlightThe buzziest AI agent applicationsCursor takes the crown as the most talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 14,
    "content": "talked-about agent application in our survey, followed closely by heavyweights like Perplexity and Replit.‚ÄçCursor is an AI-powered code editor that helps developers write, debug, and parse code with smart autocompletes and contextual assistance. Replit also accelerates the software development lifecycle by setting up environments, configurations, and letting you build and deploy fully functional apps in minutes. Perplexity is an AI-powered answer engine that can answer complex queries with web search and link sources in its responses.‚ÄçThese applications are pushing the boundaries of what agents can do, showing that AI agents are no longer theoretical ‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 15,
    "content": "‚Äî they‚Äôre solving real problems in production environments today.Emerging themes in AI agent adoptionFrom our write-in responses, we see a number of evolving expectations and challenges organizations face as they bring AI agents into their workflows.There is an admiration for these capabilities of AI agents:"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 16,
    "content": "Managing multistep tasksAgents are more capable of deeper reasoning and context management, allowing them to handle more complex tasks.\n\n\n\nAutomating repetitive tasksAI agents continue to be viewed as essential for automating administrative tasks that can free up time for users to engage in more creative problem solving.\n\n\n\nTask routing & collaborationBetter task routing ensures that the right agent handles the right problem at the right time ‚Äî especially in multi-agent systems. Many wonder how to orchestrate tasks and collaborate across agent networks effectively."
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 17,
    "content": "Human-like reasoningUnlike traditional LLMs, AI agents can trace back their decisions, including time-traveling, reviewing, and revising past decisions based on new information.But there are also challenges to consider for teams building agents. This includes:‚Ä¢ Barriers to understanding agent behavior. Several engineers wrote in about their difficulties in explaining the capabilities and behaviors of AI agents to other stakeholders in their companies. Sometimes a little extra visualization of steps can explain what happened with an agent response. Other times, the LLM is still a blackbox. The additional burden of explainability is left with the engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 18,
    "content": "engineering team.In spite of the challenges, there is notable buzz and energy around the following areas:Excitement for open source AI agentsThere is palpable interest in open-source AI agents, with many citing how a collective intelligence can accelerate the innovation of agents.Anticipation for more powerful modelsMany are waiting for the next leap in AI agents to be powered by larger, more capable models ‚Äî so that agents may be able to tackle even more complex tasks with greater efficiency and autonomyConclusionThe race to integrate AI agents is on, as companies are already starting to reshape workflows and design their future with LLMs at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully,"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 19,
    "content": "at the helm of improved decision making and human productivity.But while excitement is high, companies are also aware that they must tread carefully, seeding the right controls to navigate new use cases and applications. Teams are eager but cautious, experimenting with frameworks to try to keep their agent responses high-quality and hallucination-free.As we look ahead, companies who can crack the code on reliable, controllable agents will have a headstart in the next wave of AI innovation - and begin setting the standard for the future of intelligent automation.MethodologyTop 5 industries:Technology (60% of respondents)Financial Services (11% of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 20,
    "content": "of respondents)Healthcare (6% of respondents)Education (5% of respondents)Consumer Goods (4%)Company size:<100 people (51% of respondents)100-2000 people (22% of respondents)2000-10,000 people (11% of respondents)10,000+ people (16% of respondents)Check out relevant resourcesLangGraph: Building Controllable Agents\"In the Loop\": A Blog Series on AI Agents5 Breakout Agent StoriesProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 21,
    "content": "dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/stateofaiagents#leading-agent",
    "chunk_id": 22,
    "content": "Products\n\nLangChainLangSmithLangGraphMethods\n\nRetrievalAgentsEvaluationResources\n\nBlogCase StudiesLangChain AcademyCommunityExpertsChangelogDocs\n\nPythonLangChainLangSmithLangGraphJavaScriptLangChainLangSmithLangGraphCompany\n\nAboutCareersPricing"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 0,
    "content": "Streaming - Docs by LangChainSkip to main contentLangGraph Platform is now part of LangSmith. Check out the Changelog for more information.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphIntegrationsLearnReferenceContributingPythonOverviewLangGraph v1.0Release notesGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelAdd and manage memorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 1,
    "content": "APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 2,
    "content": "LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\nWhat‚Äôs possible with LangGraph streaming:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 3,
    "content": "Stream graph state ‚Äî get state updates / values with updates and values modes.\n Stream subgraph outputs ‚Äî include outputs from both the parent graph and any nested subgraphs.\n Stream LLM tokens ‚Äî capture token streams from anywhere: inside nodes, subgraphs, or tools.\n Stream custom data ‚Äî send custom updates or progress signals directly from tool functions.\n Use multiple streaming modes ‚Äî choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces)."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 4,
    "content": "‚ÄãSupported stream modes\nPass one or more of the following stream modes as a list to the stream() or astream() methods:\nModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph.\n‚ÄãBasic usage example\nLangGraph graphs expose the .stream() (sync) and .astream() (async) methods to yield streamed outputs as iterators."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 5,
    "content": "‚ÄãBasic usage example\nLangGraph graphs expose the .stream() (sync) and .astream() (async) methods to yield streamed outputs as iterators.\nCopyfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\n    print(chunk)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 6,
    "content": "Extended example: streaming updatesCopyfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(refine_topic)\n    .add_node(generate_joke)\n    .add_edge(START, \"refine_topic\")\n    .add_edge(\"refine_topic\", \"generate_joke\")\n    .add_edge(\"generate_joke\", END)\n    .compile()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 7,
    "content": "# The stream() method returns an iterator that yields streamed outputs\nfor chunk in graph.stream(  \n    {\"topic\": \"ice cream\"},\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n    # Other stream modes are also available. See supported stream modes for details\n    stream_mode=\"updates\",  \n):\n    print(chunk)\nCopy{'refineTopic': {'topic': 'ice cream and cats'}}\n{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 8,
    "content": "‚ÄãStream multiple modes\nYou can pass a list as the stream_mode parameter to stream multiple modes at once.\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\nCopyfor mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n\n‚ÄãStream graph state\nUse the stream modes updates and values to stream the state of the graph as it executes.\n\nupdates streams the updates to the state after each step of the graph.\nvalues streams the full value of the state after each step of the graph.\n\nCopyfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 9,
    "content": "Copyfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 10,
    "content": "updates valuesUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.Copyfor chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"updates\",  \n):\n    print(chunk)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 11,
    "content": "‚ÄãStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nThe outputs will be streamed as tuples (namespace, data), where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\").\nCopyfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    # Set subgraphs=True to stream outputs from subgraphs\n    subgraphs=True,  \n    stream_mode=\"updates\",\n):\n    print(chunk)\n\nExtended example: streaming from subgraphsCopyfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 12,
    "content": "Extended example: streaming from subgraphsCopyfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 13,
    "content": "# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 14,
    "content": "for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    # Set subgraphs=True to stream outputs from subgraphs\n    subgraphs=True,  \n):\n    print(chunk)\nCopy((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\nNote that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 15,
    "content": "‚ÄãDebugging\nUse the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\nCopyfor chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"debug\",  \n):\n    print(chunk)\n\n\n‚ÄãLLM tokens\nUse the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from messages mode is a tuple (message_chunk, metadata) where:\n\nmessage_chunk: the token or message segment from the LLM.\nmetadata: a dictionary containing details about the graph node and LLM invocation."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 16,
    "content": "message_chunk: the token or message segment from the LLM.\nmetadata: a dictionary containing details about the graph node and LLM invocation.\n\n\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details.\n\nManual config required for async in Python < 3.11\nWhen using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\nCopyfrom dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 17,
    "content": "@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n    llm_response = llm.invoke(  \n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": llm_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 18,
    "content": "graph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor message_chunk, metadata in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  \n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n\n‚ÄãFilter by LLM invocation\nYou can associate tags with LLM invocations to filter the streamed tokens by LLM invocation.\nCopyfrom langchain.chat_models import init_chat_model"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 19,
    "content": "# llm_1 is tagged with \"joke\"\nllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['joke'])\n# llm_2 is tagged with \"poem\"\nllm_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['poem'])\n\ngraph = ... # define a graph that uses these LLMs\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the tags field in the metadata to only include\n    # the tokens from the LLM invocation with the \"joke\" tag\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\n\nExtended example: filtering by tagsCopyfrom typing import TypedDict"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 20,
    "content": "Extended example: filtering by tagsCopyfrom typing import TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import START, StateGraph\n\n# The joke_model is tagged with \"joke\"\njoke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"])\n# The poem_model is tagged with \"poem\"\npoem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"])\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 21,
    "content": "async def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Writing joke...\")\n      # Note: Passing the config through explicitly is required for python < 3.11\n      # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n      # The config is passed through explicitly to ensure the context vars are propagated correctly\n      # This is required for Python < 3.11 when using async code. Please see the async section for more details\n      joke_response = await joke_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n            config,\n      )\n      print(\"\\n\\nWriting poem...\")\n      poem_response = await poem_model.ainvoke("
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 22,
    "content": "config,\n      )\n      print(\"\\n\\nWriting poem...\")\n      poem_response = await poem_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n            config,\n      )\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 23,
    "content": "graph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n)\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n      {\"topic\": \"cats\"},\n      stream_mode=\"messages\",\n):\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 24,
    "content": "‚ÄãFilter by node\nTo stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:\nCopy# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    inputs,\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the specified node\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\n        ..."
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 25,
    "content": "Extended example: streaming LLM tokens from specific nodesCopyfrom typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str\n\n\ndef write_joke(state: State):\n      topic = state[\"topic\"]\n      joke_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n      )\n      return {\"joke\": joke_response.content}\n\n\ndef write_poem(state: State):\n      topic = state[\"topic\"]\n      poem_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n      )\n      return {\"poem\": poem_response.content}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 26,
    "content": "graph = (\n      StateGraph(State)\n      .add_node(write_joke)\n      .add_node(write_poem)\n      # write both the joke and the poem concurrently\n      .add_edge(START, \"write_joke\")\n      .add_edge(START, \"write_poem\")\n      .compile()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 27,
    "content": "# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  \n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the write_poem node\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n        print(msg.content, end=\"|\", flush=True)\n\n‚ÄãStream custom data\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 28,
    "content": "‚ÄãStream custom data\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:\n\nUse get_stream_writer to access the stream writer and emit custom data.\nSet stream_mode=\"custom\" when calling .stream() or .astream() to get the custom data in the stream. You can combine multiple modes (e.g., [\"updates\", \"custom\"]), but at least one must be \"custom\".\n\nNo get_stream_writer in async for Python < 3.11\nIn async code running on Python < 3.11, get_stream_writer will not work.\nInstead, add a writer parameter to your node or tool and pass it manually.\nSee Async with Python < 3.11 for usage examples.\n node toolCopyfrom typing import TypedDict\nfrom langgraph.config import get_stream_writer\nfrom langgraph.graph import StateGraph, START"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 29,
    "content": "class State(TypedDict):\n    query: str\n    answer: str\n\ndef node(state: State):\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()\n    # Emit a custom key-value pair (e.g., progress update)\n    writer({\"custom_key\": \"Generating custom data inside node\"})\n    return {\"answer\": \"some data\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(node)\n    .add_edge(START, \"node\")\n    .compile()\n)\n\ninputs = {\"query\": \"example\"}\n\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\n    print(chunk)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 30,
    "content": "# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\n    print(chunk)\n\n‚ÄãUse with any LLM\nYou can use stream_mode=\"custom\" to stream data from any LLM API ‚Äî even if that API does not implement the LangChain chat model interface.\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\nCopyfrom langgraph.config import get_stream_writer"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 31,
    "content": "def call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()  \n    # Assume you have a streaming client that yields chunks\n    # Generate LLM tokens using your custom streaming client\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\n        # Use the writer to send custom data to the stream\n        writer({\"custom_llm_chunk\": chunk})  \n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\","
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 32,
    "content": "):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n\nExtended example: streaming arbitrary chat modelCopyimport operator\nimport json\n\nfrom typing import TypedDict\nfrom typing_extensions import Annotated\nfrom langgraph.graph import StateGraph, START\n\nfrom openai import AsyncOpenAI\n\nopenai_client = AsyncOpenAI()\nmodel_name = \"gpt-4o-mini\"\n\n\nasync def stream_tokens(model_name: str, messages: list[dict]):\n    response = await openai_client.chat.completions.create(\n        messages=messages, model=model_name, stream=True\n    )\n    role = None\n    async for chunk in response:\n        delta = chunk.choices[0].delta\n\n        if delta.role is not None:\n            role = delta.role"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 33,
    "content": "if delta.role is not None:\n            role = delta.role\n\n        if delta.content:\n            yield {\"role\": role, \"content\": delta.content}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 34,
    "content": "# this is our tool\nasync def get_items(place: str) -> str:\n    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n    writer = get_stream_writer()\n    response = \"\"\n    async for msg_chunk in stream_tokens(\n        model_name,\n        [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"Can you tell me what kind of items \"\n                    f\"i might find in the following place: '{place}'. \"\n                    \"List at least 3 such items separating them by a comma. \"\n                    \"And include a brief description of each item.\"\n                ),\n            }\n        ],\n    ):\n        response += msg_chunk[\"content\"]\n        writer(msg_chunk)\n\n    return response"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 35,
    "content": "return response\n\n\nclass State(TypedDict):\n    messages: Annotated[list[dict], operator.add]\n\n\n# this is the tool-calling graph node\nasync def call_tool(state: State):\n    ai_message = state[\"messages\"][-1]\n    tool_call = ai_message[\"tool_calls\"][-1]\n\n    function_name = tool_call[\"function\"][\"name\"]\n    if function_name != \"get_items\":\n        raise ValueError(f\"Tool {function_name} not supported\")\n\n    function_arguments = tool_call[\"function\"][\"arguments\"]\n    arguments = json.loads(function_arguments)\n\n    function_response = await get_items(**arguments)\n    tool_message = {\n        \"tool_call_id\": tool_call[\"id\"],\n        \"role\": \"tool\",\n        \"name\": function_name,\n        \"content\": function_response,\n    }\n    return {\"messages\": [tool_message]}"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 36,
    "content": "graph = (\n    StateGraph(State)\n    .add_node(call_tool)\n    .add_edge(START, \"call_tool\")\n    .compile()\n)\nLet‚Äôs invoke the graph with an AIMessage that includes a tool call:Copyinputs = {\n    \"messages\": [\n        {\n            \"content\": None,\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"1\",\n                    \"function\": {\n                        \"arguments\": '{\"place\":\"bedroom\"}',\n                        \"name\": \"get_items\",\n                    },\n                    \"type\": \"function\",\n                }\n            ],\n        }\n    ]\n}\n\nasync for chunk in graph.astream(\n    inputs,\n    stream_mode=\"custom\",\n):\n    print(chunk[\"content\"], end=\"|\", flush=True)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 37,
    "content": "async for chunk in graph.astream(\n    inputs,\n    stream_mode=\"custom\",\n):\n    print(chunk[\"content\"], end=\"|\", flush=True)\n\n‚ÄãDisable streaming for specific chat models\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\nSet disable_streaming=True when initializing the model.\n init_chat_model chat model interfaceCopyfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"anthropic:claude-sonnet-4-5\",\n    # Set disable_streaming=True to disable streaming for the chat model\n    disable_streaming=True\n\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 38,
    "content": ")\n\n\n‚ÄãAsync with Python < 3.11\nIn Python versions < 3.11, asyncio tasks do not support the context parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph‚Äôs streaming mechanisms in two key ways:\n\nYou must explicitly pass RunnableConfig into async LLM calls (e.g., ainvoke()), as callbacks are not automatically propagated.\nYou cannot use get_stream_writer in async nodes or tools ‚Äî you must pass a writer argument directly.\n\nExtended example: async LLM call with manual configCopyfrom typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\nclass State(TypedDict):\n    topic: str\n    joke: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 39,
    "content": "llm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\n# Accept config as an argument in the async node function\nasync def call_model(state, config):\n    topic = state[\"topic\"]\n    print(\"Generating joke...\")\n    # Pass config to llm.ainvoke() to ensure proper context propagation\n    joke_response = await llm.ainvoke(  \n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n        config,\n    )\n    return {\"joke\": joke_response.content}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 40,
    "content": "graph = (\n    StateGraph(State)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# Set stream_mode=\"messages\" to stream LLM tokens\nasync for chunk, metadata in graph.astream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  \n):\n    if chunk.content:\n        print(chunk.content, end=\"|\", flush=True)\n\nExtended example: async custom streaming with stream writerCopyfrom typing import TypedDict\nfrom langgraph.types import StreamWriter\n\nclass State(TypedDict):\n      topic: str\n      joke: str"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 41,
    "content": "class State(TypedDict):\n      topic: str\n      joke: str\n\n# Add writer as an argument in the function signature of the async node or tool\n# LangGraph will automatically pass the stream writer to the function\nasync def generate_joke(state: State, writer: StreamWriter):  \n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n      StateGraph(State)\n      .add_node(generate_joke)\n      .add_edge(START, \"generate_joke\")\n      .compile()\n)\n\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\nasync for chunk in graph.astream(\n      {\"topic\": \"ice cream\"},\n      stream_mode=\"custom\",\n):\n      print(chunk)"
  },
  {
    "url": "https://docs.langchain.com/oss/python/langgraph/streaming",
    "chunk_id": 42,
    "content": "Edit the source of this page on GitHubWas this page helpful?YesNoDurable executionPreviousInterruptsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#conclusion",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://academy.langchain.com/",
    "chunk_id": 0,
    "content": "LangChain Academy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\nPython\n\nLangChain\nLangSmith\nLangGraph\n\n\n\nJavaScript\n\nLangChain\nLangSmith\nLangGraph\n\n\n\n\nCommunity\nLangSmith\nAll Courses\n\n\nSign In\n\n\nRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain Academy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel up with LangChain Academy\nDive into self-paced, comprehensive courses designed to help you build relevant skills and knowledge to succeed with LangChain products.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWatch Intro Video\n\n\n\n\n\n\n\n\n\nOur Courses\n\n\n\n\n\n\n\n\n\n\n\n          Project: Deep Agents with LangGraph\n        \n\n\n          \n          Course\n          \n        \n\n            Learn the fundamental characteristics of Deep Agents and how to implement your own Deep Agent for complex, long-running tasks."
  },
  {
    "url": "https://academy.langchain.com/",
    "chunk_id": 1,
    "content": "Project: Ambient Agents with LangGraph\n        \n\n\n          \n          Course\n          \n        \n\n            Build your own ambient agent to manage your email. You‚Äôll learn the fundamentals of LangGraph as you build an email assistant from scratch, and use LangSmith to evaluate its performance.\n          \n\n\n\n\n\n\n\n\n\n\n\n\n          Foundation: Introduction to Agent Observability & Evaluations\n        \n\n\n          \n          Course\n          \n        \n\n            Learn the essentials of agent observability & evaluations with LangSmith ‚Äî our platform for agent development. Continuously improve your agents with LangSmith's tools for observability, evaluation, and prompt engineering.\n          \n\n\n\n\n\n\n\n\n\n\n\n\n          Project: Deep Research with LangGraph"
  },
  {
    "url": "https://academy.langchain.com/",
    "chunk_id": 2,
    "content": "Project: Deep Research with LangGraph\n        \n\n\n          \n          Course\n          \n        \n\n            Build your own deep research agent to handle research tasks. Learn how to use LangGraph to build a multi-agent system, then use LangSmith to evaluate its performance.\n          \n\n\n\n\n\n\n\n\n\n\n\n\n          Foundation: Introduction to LangGraph\n        \n\n\n          \n          Course\n          \n        \n\n            Learn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows."
  },
  {
    "url": "https://academy.langchain.com/",
    "chunk_id": 3,
    "content": "Ready to start shipping reliable GenAI apps faster?\nGet started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.\n\n\n\n                Contact Sales\n              \n\n\n\n\n\n\n\n\n\n\n\nBring LangChain Academy to Your Company\nNominate your company to receive a LangChain Academy training, available both in-person at your office or remotely, at no cost.\n\n\n\n                Learn More\n              \n\n\n\n\n\n\n\n\n\n\n\n\n\n          \n            ¬© Copyright LangChain Academy 2025"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=2",
    "chunk_id": 0,
    "content": "Resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upGuides\n\n\nFilters\n\nGuides & ReportsUse cases & inspirationMethodsThank you! Your submission has been received!Oops! Something went wrong while submitting the form.MethodsUpcomingDesigning AgentsLearn the method\n\n\nMethodsUpcomingDesigning AgentsLearn the method"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=2",
    "chunk_id": 1,
    "content": "MethodsUpcomingDesigning AgentsLearn the method\n\n\nMethodsUpcomingRetrieval Augmented Generation (RAG)Learn the method\n\n\nMethodsUpcomingRetrieval Augmented Generation (RAG)Learn the method"
  },
  {
    "url": "https://www.langchain.com/resources?b55f76e7_page=2",
    "chunk_id": 2,
    "content": "MethodsUpcomingRetrieval Augmented Generation (RAG)Learn the method\n\n\nMethodsUpcomingRetrieval Augmented Generation (RAG)Learn the method\n\n\nPreviousReady to start shipping ‚Ä®reliable, GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Contact UsSign UpProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 0,
    "content": "Foundation: Introduction to LangGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\nPython\n\nLangChain\nLangSmith\nLangGraph\n\n\n\nJavaScript\n\nLangChain\nLangSmith\nLangGraph\n\n\n\n\nCommunity\nLangSmith\nAll Courses\n\n\nSign In\n\n\nRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangChain Academy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundation: Introduction to LangGraph\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\n\n\n\nEnroll for free\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWatch Intro Video\n\n\n\n\n\n\n\n\n\n\nCourse Curriculum\n\n\n\n\n\n\n\n\n                    Welcome to the course!\n                    \n                      \n\n\n\n\n\n\n\n\nCourse Overview"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 1,
    "content": "Course Curriculum\n\n\n\n\n\n\n\n\n                    Welcome to the course!\n                    \n                      \n\n\n\n\n\n\n\n\nCourse Overview\n\n\n\n\n\n\n\nGetting Set Up\n\n\n\n\n\n\n\nGetting Set Up (Video Guide)\n\n\n\n\n\n\n\nModule 0 Resources\n\n\n\n\n\n\n\n\n\n\n                    Module 1: Introduction\n                    \n                      \n\n\n\n\n\n\n\n\nModule 1 Introduction\n\n\n\n\n\n\n\nModule 1 Resources\n\n\n\n\n\n\n\nLesson 1: Motivation\n\n\n\n\n\n\n\nLesson 2: Simple Graph\n\n\n\n\n\n\n\nLesson 3: LangSmith Studio\n\n\n\n\n\n\n\nLesson 4: Chain\n\n\n\n\n\n\n\nLesson 5: Router\n\n\n\n\n\n\n\nLesson 6: Agent\n\n\n\n\n\n\n\nLesson 7: Agent with Memory\n\n\n\n\n\n\n\n[Optional] Lesson 8: Intro to Deployment\n\n\n\n\n\n\n\nModule 1 Feedback\n\n\n\n\n\n\n\n\n\n\n                    Module 2: State and Memory\n                    \n                      \n\n\n\n\n\n\n\n\nModule 2 Introduction"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 2,
    "content": "Module 2: State and Memory\n                    \n                      \n\n\n\n\n\n\n\n\nModule 2 Introduction\n\n\n\n\n\n\n\nModule 2 Resources\n\n\n\n\n\n\n\nLesson 1: State Schema\n\n\n\n\n\n\n\nLesson 2: State Reducers\n\n\n\n\n\n\n\nLesson 3: Multiple Schemas\n\n\n\n\n\n\n\nLesson 4: Trim and Filter Messages\n\n\n\n\n\n\n\nLesson 5: Chatbot w/ Summarizing Messages and Memory\n\n\n\n\n\n\n\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\n\n\n\n\n\n\n\nModule 2 Feedback\n\n\n\n\n\n\n\n\n\n\n                    Module 3: UX and Human-in-the-Loop\n                    \n                      \n\n\n\n\n\n\n\n\nModule 3 Introduction\n\n\n\n\n\n\n\nModule 3 Resources\n\n\n\n\n\n\n\nLesson 1: Streaming\n\n\n\n\n\n\n\nLesson 2: Breakpoints\n\n\n\n\n\n\n\nLesson 3: Editing State and Human Feedback\n\n\n\n\n\n\n\nLesson 4: Dynamic Breakpoints\n\n\n\n\n\n\n\nLesson 5: Time Travel"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 3,
    "content": "Lesson 2: Breakpoints\n\n\n\n\n\n\n\nLesson 3: Editing State and Human Feedback\n\n\n\n\n\n\n\nLesson 4: Dynamic Breakpoints\n\n\n\n\n\n\n\nLesson 5: Time Travel\n\n\n\n\n\n\n\nModule 3 Feedback\n\n\n\n\n\n\n\n\n\n\n                    Module 4: Building Your Assistant\n                    \n                      \n\n\n\n\n\n\n\n\nModule 4 Introduction\n\n\n\n\n\n\n\nModule 4 Resources\n\n\n\n\n\n\n\nLesson 1: Parallelization\n\n\n\n\n\n\n\nLesson 2: Sub-graphs\n\n\n\n\n\n\n\nLesson 3: Map-reduce\n\n\n\n\n\n\n\nLesson 4: Research Assistant\n\n\n\n\n\n\n\nModule 4 Feedback\n\n\n\n\n\n\n\n\n\n\n                    Module 5: Long-Term Memory\n                    \n                      \n\n\n\n\n\n\n\n\nModule 5 Resources\n\n\n\n\n\n\n\nLesson 1: Short vs. Long-Term Memory\n\n\n\n\n\n\n\nLesson 2: LangGraph Store\n\n\n\n\n\n\n\nLesson 3: Memory Schema + Profile\n\n\n\n\n\n\n\nLesson 4: Memory Schema + Collection"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 4,
    "content": "Lesson 2: LangGraph Store\n\n\n\n\n\n\n\nLesson 3: Memory Schema + Profile\n\n\n\n\n\n\n\nLesson 4: Memory Schema + Collection\n\n\n\n\n\n\n\nLesson 5: Build an Agent with Long-Term Memory\n\n\n\n\n\n\n\nModule 5 Feedback\n\n\n\n\n\n\n\n\n\n\n                    Module 6: Deployment\n                    \n                      \n\n\n\n\n\n\n\n\nModule 6 Resources\n\n\n\n\n\n\n\nLesson 1: Deployment Concepts\n\n\n\n\n\n\n\nLesson 2: Creating a Deployment\n\n\n\n\n\n\n\nLesson 3: Connecting to a Deployment\n\n\n\n\n\n\n\nLesson 4: Double Texting\n\n\n\n\n\n\n\nLesson 5: Assistants\n\n\n\n\n\n\n\nModule 6 Feedback\n\n\n\n\n\n\n\nEnd of Course Feedback\n\n\n\n\n\n\n\n\n\n                Show more\n              \n\n\n\n\n\n\n\n\n\n        About this course\n      \n\n\n\n\nFree\n\n\n\n\n            54 lessons\n          \n\n\n\n\n            \n            6 hours of video content\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph FAQs"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 5,
    "content": "Free\n\n\n\n\n            54 lessons\n          \n\n\n\n\n            \n            6 hours of video content\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\nLangGraph FAQs\n\n\n\n\n\n\n\n\n                      Do I need to use LangChain to use LangGraph? What‚Äôs the difference?\n                    \n\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\n\n\n\n\n\n\n\n                      How is LangGraph different from other agent frameworks?"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 6,
    "content": "How is LangGraph different from other agent frameworks?\n                    \n\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company‚Äôs needs. LangGraph provides a more expressive framework to handle companies‚Äô unique tasks without restricting users to a single black-box cognitive architecture.\n\n\n\n\n\n\n\n                      Does LangGraph impact the performance of my app?\n                    \n\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\n\n\n\n\n\n\n\n                      Is LangGraph open source? Is it free?\n                    \n\nYes. LangGraph is an MIT-licensed open-source library and is free to use."
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 7,
    "content": "Yes. LangGraph is an MIT-licensed open-source library and is free to use.\n\n\n\n\n\n\n\n                      What is LangSmith Deployment?\n                    \n\nLangSmith Deployment helps you ship your agent in one click, using scalable infrastructure built for long-running tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReady to start shipping reliable agents faster?\nOur platform provides tools for every step of the agent development lifecycle ‚Äî built to unlock powerful AI in production.\n\n\n\n                Contact Sales\n              \n\n\n\n\n\n\n\n\n\n\n\nLearn with the community\nLearn alongside other builders at in-person LangChain meetups. Subscribe to our calendar to be the first to know about upcoming events near you.\n\n\n\n                Subscribe"
  },
  {
    "url": "https://academy.langchain.com/courses/intro-to-langgraph",
    "chunk_id": 8,
    "content": "Subscribe\n              \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          \n            ¬© Copyright LangChain Academy 2025"
  },
  {
    "url": "https://www.langchain.com/experts",
    "chunk_id": 0,
    "content": "404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up404Oops! page not found.The page you are looking for might have been removed had¬†its name changed or is temporarily unavailable.Go to Home Page"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 0,
    "content": "How Vizient empowers healthcare providers with reliable GenAI insights using LangGraph and LangSmith\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\nIn the Loop\n\n\n\n\nLangChain\n\n\n\n\nDocs\n\n\n\n\nChangelog\n\n\n\n\n\nSign in\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Vizient empowers healthcare providers with reliable GenAI insights using LangGraph and LangSmith\nVizient's GenAI platform helps users manage data to query for information ranging from patient outcomes to clinical benchmarking. See how they used LangGraph and LangSmith for multi-agent system reliability and prompt management.\n\nCase Studies\n3 min read\nFeb 10, 2025"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 1,
    "content": "Vizient, a leader in healthcare performance improvement, is revolutionizing how healthcare providers access and analyze data. Today, many healthcare providers rely on disparate data sources, needing to mine for data to produce actionable insights on patient care ‚Äî a long, drawn-out process. Vizient's GenAI platform empowers systems of all sizes to query and unify siloed datasets, driving better decisions in supply chain management and clinical outcomes.Vizient's GenAI platform helps answer questions like: \"Are my ambulatory investments effective?\" or \"Are we delivering the most cost-effective care?\" and get immediate, data-backed answers. The goal is to improve operational efficiency and democratize data analysis for resource-limited health facilities ‚Äî all while maintaining strong trust"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 2,
    "content": "goal is to improve operational efficiency and democratize data analysis for resource-limited health facilities ‚Äî all while maintaining strong trust and data privacy among their members.Scorecard performance for an example hospital system in Vizient‚Äôs GenAI platformReliable AI agent workflows with LangGraph¬†Before adopting LangGraph, Vizient's multi-agent system faced several challenges. Each agent had been designed to handle a specific task, such as analyzing historical data or generating visualizations. However, coordinating them was tricky. These agents worked in silos, leading to inconsistent responses and a lack of reliability. Some underlying API workflows also involved managing hundreds of parameters per call, making it difficult to maintain and update application logic.Vizient‚Äôs AI"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 3,
    "content": "API workflows also involved managing hundreds of parameters per call, making it difficult to maintain and update application logic.Vizient‚Äôs AI user interface to chat with data and generate visualizationsTo coordinate their multi-agent system and ensure their platform met high-reliability standards, Vizient chose LangGraph to orchestrate their agentic system. With LangGraph's graph structure and fully descriptive primitives, Vizient's engineering team could control and plan their workflows and represent steps that an agent should perform as tools or nodes programmatically to improve reliability. Today, their hierarchical agent structure (with worker agents reporting to a supervisor agent) has greatly streamlined the process of routing requests to the appropriate APIs.¬†As Vizient continues"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 4,
    "content": "worker agents reporting to a supervisor agent) has greatly streamlined the process of routing requests to the appropriate APIs.¬†As Vizient continues to expand and enhance its GenAI platform, LangGraph remains a cornerstone of its strategy, enabling the team to adapt and scale its system confidently.LLM observability and prompt management with LangSmithTo ensure their GenAI platform runs smoothly, Vizient needed visibility into its performance. That's where LangSmith came in. By leveraging LangSmith's tracing capabilities, Vizient's engineers could quickly pinpoint and resolve issues, even during high-stakes, real-time demos. For example, they easily navigated problems caused by Azure OpenAI's content filters and external rate-limiting errors.LangSmith's Prompt Hub has also proved"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 5,
    "content": "they easily navigated problems caused by Azure OpenAI's content filters and external rate-limiting errors.LangSmith's Prompt Hub has also proved invaluable. By isolating prompt logic, Vizient's teams gained the flexibility to version and iterate on prompts with ease‚Äî a much more flexible approach. As the number of GenAI development teams grows, having this logic separated out will help teams handle and iterate on prompts quickly.¬†Looking ForwardVizient is focused on refining evaluations to ensure output consistency and trust. Key initiatives include:Evaluating consistency across data domains: Aligning generated answers with established tools like Q&A scorecards.Rapid data onboarding: The team aims to quickly onboard product data to fuel its agentic system using various existing product"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 6,
    "content": "like Q&A scorecards.Rapid data onboarding: The team aims to quickly onboard product data to fuel its agentic system using various existing product APIs and other data sources.Vizient is building a transformative GenAI platform that empowers healthcare providers. It enables even non-experts to ask complex questions and get actionable insights while maintaining the highest trust, security, and innovation standards. With LangGraph and LangSmith as foundational technologies, Vizient is poised to continue raising the bar for healthcare performance improvement."
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 7,
    "content": "Tags\nCase Studies\n\n\nJoin our newsletter\nUpdates from the LangChain team and community\n\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\nSuccess! Please check your inbox and click the link to confirm your subscription.\nSorry, something went wrong. Please try again.\n\n\n\n\n\nYou might also like\n\n\n\n\n\n\n\n\n\n\nMonte Carlo: Building Data + AI Observability Agents with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Bertelsmann Built a Multi-Agent System to Empower Creatives\n\n\nCase Studies\n6 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Exa built a Web Research Multi-Agent System with LangGraph and LangSmith\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read"
  },
  {
    "url": "https://blog.langchain.dev/customers-vizient/",
    "chunk_id": 8,
    "content": "How Captide agents running on LangGraph Platform compress investment research from days to seconds\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Webtoon Entertainment built agentic workflows with LangGraph to scale story understanding\n\n\nCase Studies\n4 min read\n\n\n\n\n\n\n\n\n\n\n\n\nHow Outshift by Cisco achieved a 10x productivity boost with their Agentic AI Platform Engineer\n\n\nCase Studies\n5 min read\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSign up\n\n\n\n\n\n            ¬© LangChain Blog 2025"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 0,
    "content": "Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign upHarden your application with¬†LangSmith evaluationDon‚Äôt ship on ‚Äúvibes‚Äù alone.¬†Measure your LLM application'sperformance by testing across its development lifecycle.Get a demoSign up for free"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 1,
    "content": "Continuously improve your LLM¬†system by capturing new metrics for style and accuracy, identifying regressions and errors, and fixing them quickly.Test early, test oftenLangSmith helps test application code pre-release and while it runs in production.\n\n\n\nOffline EvaluationTest your application on reference LangSmith datasets. Use a combination of human review and auto-evals to score your results.\n\n\n\nIntegrate with CIUnderstand how changes to your prompt, model, or retrieval strategy impact your app before they hit prod. Catch regressions in¬†CI and prevent them from impacting users."
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 2,
    "content": "Online evaluationContinuously monitor qualitative characteristics of your live application to spot problems or drift.Lots of options to ensure ‚Ä®full¬†testing coverageEvaluation is a critical, yet difficult, part of shipping quality applications. We make it easy to add automatic and human evaluation on every trace.001AI-Judge evaluation\n\nUse an LLM and prompt to evaluate the response of your application ‚Äì ¬†testing against any custom rubric.002Gold standard evaluation\n\nBuild up a labeled dataset of inputs and gold standard outputs in¬†LangSmith, and then evaluate the similarity of your application‚Äôs response compared to the reference output.003Functional tests"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 3,
    "content": "Write a custom evaluator to test that the application‚Äôs response meets your expectations. For example, if you expect the response to be formatted in JSON, write a test to check for proper deserialization.004Human evaluation\n\nUse the LangSmith Annotation Queue to have human labelers add feedback on each trace. This workflow is great for quality assurance teams and subject matter experts to collaborate in ensuring a high quality bar.001Dataset ConstructionA strong testing framework starts with building a reference dataset, often a tedious task. LangSmith streamlines this by letting you save debugging and production traces to¬†datasets. Datasets are collections of exemplary or¬†problematic inputs and outputs that should be replicated or¬†corrected, respectively.Go to Docs"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 4,
    "content": "002Regression TestingWhen there are so many moving parts to an LLM-app, it can be hard to¬†attribute regressions to a specific model, prompt, or other system change. LangSmith lets you track how different versions of your app stack up based on the evaluation criteria that you‚Äôve defined.Go to Docs"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 5,
    "content": "003Human AnnotationWhile LangSmith has many options for automatic evaluation, sometimes you need a human touch. LangSmith speeds up the human labeler workflow significantly by supporting a feedback config and queue of traces that users can easily work through by annotating application responses with scores.004Online EvaluationTesting needs to happen continuously for any live application. LangSmith helps you monitor not only latency, errors, and cost, but also qualitative measures to make sure your application responds effectively and meets company expectations.Don‚Äôt fly blind. Easily benchmark performance.Evaluation gives developers a framework to make trade-off¬†decisions¬†between cost, latency, and quality.Go to Docs"
  },
  {
    "url": "https://www.langchain.com/evaluation",
    "chunk_id": 6,
    "content": "Resources for LangSmith EvaluationE-BOOKThe Definitive Guide to Testing LLM¬†ApplicationsVIDEO¬†SERIESLangSmith:¬†Why Evaluations MatterdemoLangSmith: Datasets &¬†EvaluationsReady to start shipping ‚Ä®reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangGraphLangSmith ObservabilityLangSmith EvaluationLangSmith DeploymentResourcesGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocsCompanyAboutCareersXLinkedInYouTubeMarketing AssetsSecuritySign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 0,
    "content": "Replit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\nOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsUse planning, memory, and sub-agents for complex, long-running tasksLangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionResources\n\nLangChain AcademyBlogCustomer StoriesCommunityEventsChangelogGuidesDocsCompany\n\nAboutCareersPricingGet a demoSign up\n\n\n\nTransforming how users build software from scratch, to code, to application with Replit Agent ¬†\n\n\nIntroduction\n\nProblem\n\nCognitive architecture\n\nPrompt engineering\n\nUX\n\nEvaluation\n\nConclusion"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 1,
    "content": "IntroductionFrom scratch, to code, to app ‚Äîin a flashBuilding a fully functioning software app is hard work. From coding the application logic to setting up environments and databases, there‚Äôs a lot that developers have to set up before anyone can interact with the app. The Replit team recently launched Replit Agent, a first-of-its-kind AI agent that helps users create applications from scratch.¬†While current tools are great for code completion and incremental development, Replit Agent can think ahead and take the right sequence of actions to help you build that e-commerce web app, financial analysis tool, or any newfangled idea you‚Äôve been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 2,
    "content": "been dreaming up. It pairs with you as a co-pilot and has all the same tools you‚Äôd have access to in Replit to help you go from idea to working code, fast.ProblemOvercoming blank page syndromeDesigning and building an app without a set rulebook can be overwhelming. It‚Äôs easy for developers to be hit with ‚Äúblank page syndrome,‚Äù causing a lot of staring at an empty code editor even if armed with the right tools.¬†Replit Agent lowers the activation barrier for new users to create software, allowing users to whip up a project with a simple prompt in plain English. Its ability to support multi-step task execution and manage infrastructure also eases the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability,"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 3,
    "content": "the build-experiment-test-deploy process.Cognitive architectureKeep reliability high and users in the loopThe Replit team focused on reliability, constraining their AI agent‚Äôs environment to the Replit web app and tools already available to Replit developers. Their agent was a ReAct style agent that could iteratively loop.¬†Over time, the Replit Agent adopted a multi-agent architecture. When there was only one agent managing tools, the chance of error increased ‚Äì so the Replit team limited their agents to each perform the smallest possible task. They assigned roles to their different agents, including:A manager agent to oversee the workflow.Editor agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 4,
    "content": "agents to handle specific coding tasks.A verifier agent to check the code and frequently interact with the user.Michele Catasta, President of Replit, notes a key difference in their building philosophy: We don‚Äôt strive for full autonomy. We want the user to stay involved and engaged.‚ÄùTheir verifier agent, for example, is unique in that it doesn't just check code and try to progress with a decision. It often falls back to talking to the user in order to enforce continuous user feedback in the development process.Prompt engineeringBuild and organize prompts for relevant insightsReplit employed a range of advanced techniques to enhance the performance of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 5,
    "content": "of their coding agents, especially for complex tasks like file edits.‚ÄçFew-shot and long instructionsReplit frequently uses few-shot examples along with long, task-specific instructions to guide the model effectively. For more difficult parts of the development process, such as file edits, Replit initially experimented with fine-tuning. But, this didn‚Äôt yield any breakthroughs. Instead, significant performance improvements came from leveraging Claude 3.5 Sonnet.Dynamic prompt construction & memory¬†Replit also developed dynamic prompt construction techniques to handle token limitations, similar to the system used by OpenAI's popular prompt orchestration libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 6,
    "content": "libraries. They condense and truncate long memory trajectories to manage ever-growing context. This involved compressing memories with LLMs to ensure only the most relevant information is retained.Structured formatting for clarityTo improve their model understanding and prompt organization, Replit incorporates structured formatting. In particular, XML tags were helpful in delineating different sections of the prompt, which guided the model in understanding tasks. For lengthy instructions, Replit relies on Markdown, as it‚Äôs often within the model‚Äôs training distribution.Tool callingNotably, Replit didn‚Äôt do tool calling in a traditional way. Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 7,
    "content": "Instead of using the function calling offered by OpenAI‚Äôs APIs, they chose to generate code to invoke tools themselves, as this approach proved more reliable. With Replit‚Äôs extensive library of 30+ tools, each tool required several arguments to function correctly, making the tool invocation process complex. Replit wrote a restricted Python-based DSL (Domain-Specific Language) to handle these invocations, improving tool execution accuracy.UXBringing the user along in the agent journeyReplit focused on enabling key human-in-the-loop workflows when designing their UX. First, the Replit team implemented a reversion feature for added control. At every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 8,
    "content": "every major step of the agent‚Äôs workflow, Replit automatically commits changes under the hood. This lets users ‚Äútravel back in time‚Äù to any previous point and make corrections.In a complex, multi-step agent trajectory, the first few steps tend to be most successful, while reliability drops off in later steps. As such, the team decided it was particularly important to empower users to revert to earlier versions when necessary. Beginner users can simply click a button to reverse changes, while power users have added flexibility to dive deeper into the Git pane and manage branches directly.¬†Because the Replit team scoped everything into tools, users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc."
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 9,
    "content": "users can see clear, concise update messages about agent actions whenever the agent installs a package, executes a shell command, creates a file etc. Instead of focusing on the raw output of the LLM, users can see their app evolving in time and decide how hands-on they want to be in the agent‚Äôs thought process (e.g. choosing to expand to view every action the agent has taken and the thought behind it, or ignore it).Unlike other agent tools, Replit also lets you deploy your application in a few clicks. The ability to publish and share applications is integrated smoothly in the agent workflow.EvaluationReal-time feedback and trace monitoringTo gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 10,
    "content": "gain confidence in their agent, Replit relied on a mix of intuition, real-world feedback, and trace visibility into their agent interactions.¬†During Replit Agent‚Äôs alpha phase, they invited a small group of ~15 AI-first developers and influencers to test their product. To gain actionable insights from the alpha feedback, Replit integrated LangSmith as their observability tool to track and action upon problematic agent interactions in their traces.The Replit team would search over long-running traces to pinpoint any issues. Because Replit Agent allowed human developers to come in and correct agent trajectories as needed, multi-turn conversations were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 11,
    "content": "were common. They were able to monitor these conversational flows in logical views within LangSmith to identify bottlenecks where users got stuck and could require human intervention.The easy integration and readability of their LangGraph code in LangSmith traces was a big bonus for using both the agent framework (LangGraph) and observability tool (LangSmith) together.ConclusionEmpowering creativity for developersReplit Agent is simplifying software development for novice and veteran developers alike.¬† By prioritizing human-agent collaboration and visibility into agent actions, the Replit team is helping users overcome initial hurdles to unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 12,
    "content": "unleash their creativity.¬†While the world of agents has offered so many powerful new use cases, debugging or predicting the agent‚Äôs actions is still often uncharted water. Alongside the developer community, Replit looks forward to pushing the boundaries and working on tricky cases like evaluating AI agent trajectory.And on the path to building useful and reliable agents, Michele Catasta puts it best: ‚ÄúWe‚Äôll just have to embrace the messiness.‚Äù¬†And that's not all...Discover more breakout AI agent stories below from the most cutting-edge companies. ¬†Breakout Agentic Apps"
  },
  {
    "url": "https://www.langchain.com/breakoutagents/replit#prompt-engineering",
    "chunk_id": 13,
    "content": "Go back to main pageRead next storyRamp\n\n\nReady to start shipping ‚Ä®reliable GenAI apps faster?LangChain, LangSmith, and LangGraph are critical parts of the reference ‚Ä®architecture to get you from prototype to production.Get a demoSign UpAll systems operationalPrivacy PolicyTerms of Service"
  }
]